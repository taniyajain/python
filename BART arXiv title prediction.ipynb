{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "04b332ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: plotly-express in c:\\users\\taniya jain\\anaconda3\\lib\\site-packages (0.4.1)\n",
      "Requirement already satisfied: statsmodels>=0.9.0 in c:\\users\\taniya jain\\anaconda3\\lib\\site-packages (from plotly-express) (0.12.2)\n",
      "Requirement already satisfied: plotly>=4.1.0 in c:\\users\\taniya jain\\anaconda3\\lib\\site-packages (from plotly-express) (5.6.0)\n",
      "Requirement already satisfied: pandas>=0.20.0 in c:\\users\\taniya jain\\anaconda3\\lib\\site-packages (from plotly-express) (1.3.4)\n",
      "Requirement already satisfied: patsy>=0.5 in c:\\users\\taniya jain\\anaconda3\\lib\\site-packages (from plotly-express) (0.5.2)\n",
      "Requirement already satisfied: scipy>=0.18 in c:\\users\\taniya jain\\anaconda3\\lib\\site-packages (from plotly-express) (1.7.1)\n",
      "Requirement already satisfied: numpy>=1.11 in c:\\users\\taniya jain\\anaconda3\\lib\\site-packages (from plotly-express) (1.20.3)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in c:\\users\\taniya jain\\anaconda3\\lib\\site-packages (from pandas>=0.20.0->plotly-express) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2017.3 in c:\\users\\taniya jain\\anaconda3\\lib\\site-packages (from pandas>=0.20.0->plotly-express) (2021.3)\n",
      "Requirement already satisfied: six in c:\\users\\taniya jain\\anaconda3\\lib\\site-packages (from patsy>=0.5->plotly-express) (1.16.0)\n",
      "Requirement already satisfied: tenacity>=6.2.0 in c:\\users\\taniya jain\\anaconda3\\lib\\site-packages (from plotly>=4.1.0->plotly-express) (8.0.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install plotly-express"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "16c16ea0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os, json, gc, re, random\n",
    "from tqdm.notebook import tqdm\n",
    "# from tqdm import tqdm\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import plotly.express as px\n",
    "import seaborn as sns\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "transformers_logger = logging.getLogger(\"transformers\")\n",
    "transformers_logger.setLevel(logging.WARNING)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "75501796",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('1.10.2+cpu', '4.17.0', '0.11.6')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch, transformers, tokenizers\n",
    "torch.__version__, transformers.__version__, tokenizers.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e1e35a61",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_file = 'C:\\\\Users\\\\TANIYA JAIN\\\\arxiv-metadata-oai-snapshot.json//arxiv-metadata-oai-snapshot.json'\n",
    "\n",
    "\"\"\" Using `yield` to load the JSON file in a loop to prevent Python memory issues if JSON is loaded directly\"\"\"\n",
    "\n",
    "def get_metadata():\n",
    "    with open(data_file, 'r') as f:\n",
    "        for line in f:\n",
    "            yield line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "27f35e1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "id: 0704.0001 \n",
      "\n",
      "submitter: Pavel Nadolsky \n",
      "\n",
      "authors: C. Bal\\'azs, E. L. Berger, P. M. Nadolsky, C.-P. Yuan \n",
      "\n",
      "title: Calculation of prompt diphoton production cross sections at Tevatron and\n",
      "  LHC energies \n",
      "\n",
      "comments: 37 pages, 15 figures; published version \n",
      "\n",
      "journal-ref: Phys.Rev.D76:013009,2007 \n",
      "\n",
      "doi: 10.1103/PhysRevD.76.013009 \n",
      "\n",
      "report-no: ANL-HEP-PR-07-12 \n",
      "\n",
      "categories: hep-ph \n",
      "\n",
      "license: None \n",
      "\n",
      "abstract:   A fully differential calculation in perturbative quantum chromodynamics is\n",
      "presented for the production of massive photon pairs at hadron colliders. All\n",
      "next-to-leading order perturbative contributions from quark-antiquark,\n",
      "gluon-(anti)quark, and gluon-gluon subprocesses are included, as well as\n",
      "all-orders resummation of initial-state gluon radiation valid at\n",
      "next-to-next-to-leading logarithmic accuracy. The region of phase space is\n",
      "specified in which the calculation is most reliable. Good agreement is\n",
      "demonstrated with data from the Fermilab Tevatron, and predictions are made for\n",
      "more detailed tests with CDF and DO data. Predictions are shown for\n",
      "distributions of diphoton pairs produced at the energy of the Large Hadron\n",
      "Collider (LHC). Distributions of the diphoton pairs from the decay of a Higgs\n",
      "boson are contrasted with those produced from QCD processes at the LHC, showing\n",
      "that enhanced sensitivity to the signal can be obtained with judicious\n",
      "selection of events.\n",
      " \n",
      "\n",
      "versions: [{'version': 'v1', 'created': 'Mon, 2 Apr 2007 19:18:42 GMT'}, {'version': 'v2', 'created': 'Tue, 24 Jul 2007 20:10:27 GMT'}] \n",
      "\n",
      "update_date: 2008-11-26 \n",
      "\n",
      "authors_parsed: [['Balázs', 'C.', ''], ['Berger', 'E. L.', ''], ['Nadolsky', 'P. M.', ''], ['Yuan', 'C. -P.', '']] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "metadata = get_metadata()\n",
    "for paper in metadata:\n",
    "    for k, v in json.loads(paper).items():\n",
    "        print(f'{k}: {v} \\n')\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ec12d702",
   "metadata": {},
   "outputs": [],
   "source": [
    "category_map = {'astro-ph': 'Astrophysics',\n",
    "                'astro-ph.CO': 'Cosmology and Nongalactic Astrophysics',\n",
    "                'astro-ph.EP': 'Earth and Planetary Astrophysics',\n",
    "                'astro-ph.GA': 'Astrophysics of Galaxies',\n",
    "                'astro-ph.HE': 'High Energy Astrophysical Phenomena',\n",
    "                'astro-ph.IM': 'Instrumentation and Methods for Astrophysics',\n",
    "                'astro-ph.SR': 'Solar and Stellar Astrophysics',\n",
    "                'cond-mat.dis-nn': 'Disordered Systems and Neural Networks',\n",
    "                'cond-mat.mes-hall': 'Mesoscale and Nanoscale Physics',\n",
    "                'cond-mat.mtrl-sci': 'Materials Science',\n",
    "                'cond-mat.other': 'Other Condensed Matter',\n",
    "                'cond-mat.quant-gas': 'Quantum Gases',\n",
    "                'cond-mat.soft': 'Soft Condensed Matter',\n",
    "                'cond-mat.stat-mech': 'Statistical Mechanics',\n",
    "                'cond-mat.str-el': 'Strongly Correlated Electrons',\n",
    "                'cond-mat.supr-con': 'Superconductivity',\n",
    "                'cs.AI': 'Artificial Intelligence',\n",
    "                'cs.AR': 'Hardware Architecture',\n",
    "                'cs.CC': 'Computational Complexity',\n",
    "                'cs.CE': 'Computational Engineering, Finance, and Science',\n",
    "                'cs.CG': 'Computational Geometry',\n",
    "                'cs.CL': 'Computation and Language',\n",
    "                'cs.CR': 'Cryptography and Security',\n",
    "                'cs.CV': 'Computer Vision and Pattern Recognition',\n",
    "                'cs.CY': 'Computers and Society',\n",
    "                'cs.DB': 'Databases',\n",
    "                'cs.DC': 'Distributed, Parallel, and Cluster Computing',\n",
    "                'cs.DL': 'Digital Libraries',\n",
    "                'cs.DM': 'Discrete Mathematics',\n",
    "                'cs.DS': 'Data Structures and Algorithms',\n",
    "                'cs.ET': 'Emerging Technologies',\n",
    "                'cs.FL': 'Formal Languages and Automata Theory',\n",
    "                'cs.GL': 'General Literature',\n",
    "                'cs.GR': 'Graphics',\n",
    "                'cs.GT': 'Computer Science and Game Theory',\n",
    "                'cs.HC': 'Human-Computer Interaction',\n",
    "                'cs.IR': 'Information Retrieval',\n",
    "                'cs.IT': 'Information Theory',\n",
    "                'cs.LG': 'Machine Learning',\n",
    "                'cs.LO': 'Logic in Computer Science',\n",
    "                'cs.MA': 'Multiagent Systems',\n",
    "                'cs.MM': 'Multimedia',\n",
    "                'cs.MS': 'Mathematical Software',\n",
    "                'cs.NA': 'Numerical Analysis',\n",
    "                'cs.NE': 'Neural and Evolutionary Computing',\n",
    "                'cs.NI': 'Networking and Internet Architecture',\n",
    "                'cs.OH': 'Other Computer Science',\n",
    "                'cs.OS': 'Operating Systems',\n",
    "                'cs.PF': 'Performance',\n",
    "                'cs.PL': 'Programming Languages',\n",
    "                'cs.RO': 'Robotics',\n",
    "                'cs.SC': 'Symbolic Computation',\n",
    "                'cs.SD': 'Sound',\n",
    "                'cs.SE': 'Software Engineering',\n",
    "                'cs.SI': 'Social and Information Networks',\n",
    "                'cs.SY': 'Systems and Control',\n",
    "                'econ.EM': 'Econometrics',\n",
    "                'eess.AS': 'Audio and Speech Processing',\n",
    "                'eess.IV': 'Image and Video Processing',\n",
    "                'eess.SP': 'Signal Processing',\n",
    "                'gr-qc': 'General Relativity and Quantum Cosmology',\n",
    "                'hep-ex': 'High Energy Physics - Experiment',\n",
    "                'hep-lat': 'High Energy Physics - Lattice',\n",
    "                'hep-ph': 'High Energy Physics - Phenomenology',\n",
    "                'hep-th': 'High Energy Physics - Theory',\n",
    "                'math.AC': 'Commutative Algebra',\n",
    "                'math.AG': 'Algebraic Geometry',\n",
    "                'math.AP': 'Analysis of PDEs',\n",
    "                'math.AT': 'Algebraic Topology',\n",
    "                'math.CA': 'Classical Analysis and ODEs',\n",
    "                'math.CO': 'Combinatorics',\n",
    "                'math.CT': 'Category Theory',\n",
    "                'math.CV': 'Complex Variables',\n",
    "                'math.DG': 'Differential Geometry',\n",
    "                'math.DS': 'Dynamical Systems',\n",
    "                'math.FA': 'Functional Analysis',\n",
    "                'math.GM': 'General Mathematics',\n",
    "                'math.GN': 'General Topology',\n",
    "                'math.GR': 'Group Theory',\n",
    "                'math.GT': 'Geometric Topology',\n",
    "                'math.HO': 'History and Overview',\n",
    "                'math.IT': 'Information Theory',\n",
    "                'math.KT': 'K-Theory and Homology',\n",
    "                'math.LO': 'Logic',\n",
    "                'math.MG': 'Metric Geometry',\n",
    "                'math.MP': 'Mathematical Physics',\n",
    "                'math.NA': 'Numerical Analysis',\n",
    "                'math.NT': 'Number Theory',\n",
    "                'math.OA': 'Operator Algebras',\n",
    "                'math.OC': 'Optimization and Control',\n",
    "                'math.PR': 'Probability',\n",
    "                'math.QA': 'Quantum Algebra',\n",
    "                'math.RA': 'Rings and Algebras',\n",
    "                'math.RT': 'Representation Theory',\n",
    "                'math.SG': 'Symplectic Geometry',\n",
    "                'math.SP': 'Spectral Theory',\n",
    "                'math.ST': 'Statistics Theory',\n",
    "                'math-ph': 'Mathematical Physics',\n",
    "                'nlin.AO': 'Adaptation and Self-Organizing Systems',\n",
    "                'nlin.CD': 'Chaotic Dynamics',\n",
    "                'nlin.CG': 'Cellular Automata and Lattice Gases',\n",
    "                'nlin.PS': 'Pattern Formation and Solitons',\n",
    "                'nlin.SI': 'Exactly Solvable and Integrable Systems',\n",
    "                'nucl-ex': 'Nuclear Experiment',\n",
    "                'nucl-th': 'Nuclear Theory',\n",
    "                'physics.acc-ph': 'Accelerator Physics',\n",
    "                'physics.ao-ph': 'Atmospheric and Oceanic Physics',\n",
    "                'physics.app-ph': 'Applied Physics',\n",
    "                'physics.atm-clus': 'Atomic and Molecular Clusters',\n",
    "                'physics.atom-ph': 'Atomic Physics',\n",
    "                'physics.bio-ph': 'Biological Physics',\n",
    "                'physics.chem-ph': 'Chemical Physics',\n",
    "                'physics.class-ph': 'Classical Physics',\n",
    "                'physics.comp-ph': 'Computational Physics',\n",
    "                'physics.data-an': 'Data Analysis, Statistics and Probability',\n",
    "                'physics.ed-ph': 'Physics Education',\n",
    "                'physics.flu-dyn': 'Fluid Dynamics',\n",
    "                'physics.gen-ph': 'General Physics',\n",
    "                'physics.geo-ph': 'Geophysics',\n",
    "                'physics.hist-ph': 'History and Philosophy of Physics',\n",
    "                'physics.ins-det': 'Instrumentation and Detectors',\n",
    "                'physics.med-ph': 'Medical Physics',\n",
    "                'physics.optics': 'Optics',\n",
    "                'physics.plasm-ph': 'Plasma Physics',\n",
    "                'physics.poph': 'Popular Physics',\n",
    "                'physics.soc-ph': 'Physics and Society',\n",
    "                'physics.space-ph': 'Space Physics',\n",
    "                'q-bio.BM': 'Biomolecules',\n",
    "                'q-bio.CB': 'Cell Behavior',\n",
    "                'q-bio.GN': 'Genomics',\n",
    "                'q-bio.MN': 'Molecular Networks',\n",
    "                'q-bio.NC': 'Neurons and Cognition',\n",
    "                'q-bio.OT': 'Other Quantitative Biology',\n",
    "                'q-bio.PE': 'Populations and Evolution',\n",
    "                'q-bio.QM': 'Quantitative Methods',\n",
    "                'q-bio.SC': 'Subcellular Processes',\n",
    "                'q-bio.TO': 'Tissues and Organs',\n",
    "                'q-fin.CP': 'Computational Finance',\n",
    "                'q-fin.EC': 'Economics',\n",
    "                'q-fin.GN': 'General Finance',\n",
    "                'q-fin.MF': 'Mathematical Finance',\n",
    "                'q-fin.PM': 'Portfolio Management',\n",
    "                'q-fin.PR': 'Pricing of Securities',\n",
    "                'q-fin.RM': 'Risk Management',\n",
    "                'q-fin.ST': 'Statistical Finance',\n",
    "                'q-fin.TR': 'Trading and Market Microstructure',\n",
    "                'quant-ph': 'Quantum Physics',\n",
    "                'stat.AP': 'Applications',\n",
    "                'stat.CO': 'Computation',\n",
    "                'stat.ME': 'Methodology',\n",
    "                'stat.ML': 'Machine Learning',\n",
    "                'stat.OT': 'Other Statistics',\n",
    "                'stat.TH': 'Statistics Theory'}\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "88580530",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e1c5d420ae6449a6be87a275e00b6cad",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "(94, 94)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "titles = []\n",
    "abstracts = []\n",
    "\n",
    "# Consider all categories in the `category_map` to be used during training and prediction\n",
    "# paper_categories = np.array(list(category_map.keys())).flatten()\n",
    "\n",
    "# # Consider specific paper categories to be used during training and prediction\n",
    "paper_categories = [#\"cs.AI\", # Artificial Intelligence\n",
    "                     #\"cs.CV\", # Computer Vision and Pattern Recognition\n",
    "\"cs.LG\" # Machine Learning\n",
    "                   ]\n",
    "metadata = get_metadata()\n",
    "for paper in tqdm(metadata):\n",
    "    paper_dict = json.loads(paper)\n",
    "    category = paper_dict.get('categories')\n",
    "    try:\n",
    "        year = int(paper_dict.get('journal-ref')[-4:])\n",
    "        if category in paper_categories and 2010<year<2021:\n",
    "            titles.append(paper_dict.get('title'))\n",
    "            abstracts.append(paper_dict.get('abstract').replace(\"\\n\",\"\"))\n",
    "    except:\n",
    "        pass \n",
    "\n",
    "len(titles), len(abstracts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4c81c40c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>input_text</th>\n",
       "      <th>target_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>In this paper we present a new algorithm for...</td>\n",
       "      <td>Geometric Decision Tree</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Margin theory provides one of the most popul...</td>\n",
       "      <td>On the Doubt about Margin Explanation of Boosting</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>We consider the problem of energy-efficient ...</td>\n",
       "      <td>Fast Reinforcement Learning for Energy-Efficie...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>To classify time series by nearest neighbors...</td>\n",
       "      <td>Time Series Classification by Class-Specific M...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Ordinal regression is commonly formulated as...</td>\n",
       "      <td>Transductive Ordinal Regression</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89</th>\n",
       "      <td>Recent breakthroughs in Deep Neural Networks...</td>\n",
       "      <td>AdaDeep: A Usage-Driven, Automated Deep Model ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90</th>\n",
       "      <td>The Symbolic Aggregate approXimation (SAX) i...</td>\n",
       "      <td>Modifying the Symbolic Aggregate Approximation...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>91</th>\n",
       "      <td>In continual learning (CL), a learner is fac...</td>\n",
       "      <td>Continual Learning in Low-rank Orthogonal Subs...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>92</th>\n",
       "      <td>The current expansion of theory and research...</td>\n",
       "      <td>Augmenting Organizational Decision-Making with...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>93</th>\n",
       "      <td>In quantum mechanics, a norm squared wave fu...</td>\n",
       "      <td>Probability-Density-Based Deep Learning Paradi...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>94 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           input_text  \\\n",
       "0     In this paper we present a new algorithm for...   \n",
       "1     Margin theory provides one of the most popul...   \n",
       "2     We consider the problem of energy-efficient ...   \n",
       "3     To classify time series by nearest neighbors...   \n",
       "4     Ordinal regression is commonly formulated as...   \n",
       "..                                                ...   \n",
       "89    Recent breakthroughs in Deep Neural Networks...   \n",
       "90    The Symbolic Aggregate approXimation (SAX) i...   \n",
       "91    In continual learning (CL), a learner is fac...   \n",
       "92    The current expansion of theory and research...   \n",
       "93    In quantum mechanics, a norm squared wave fu...   \n",
       "\n",
       "                                          target_text  \n",
       "0                             Geometric Decision Tree  \n",
       "1   On the Doubt about Margin Explanation of Boosting  \n",
       "2   Fast Reinforcement Learning for Energy-Efficie...  \n",
       "3   Time Series Classification by Class-Specific M...  \n",
       "4                     Transductive Ordinal Regression  \n",
       "..                                                ...  \n",
       "89  AdaDeep: A Usage-Driven, Automated Deep Model ...  \n",
       "90  Modifying the Symbolic Aggregate Approximation...  \n",
       "91  Continual Learning in Low-rank Orthogonal Subs...  \n",
       "92  Augmenting Organizational Decision-Making with...  \n",
       "93  Probability-Density-Based Deep Learning Paradi...  \n",
       "\n",
       "[94 rows x 2 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "papers = pd.DataFrame({\n",
    "    'title': titles,\n",
    "    'abstract': abstracts,\n",
    "})\n",
    "\n",
    "papers = papers[['abstract', 'title']]\n",
    "papers.columns = ['input_text', 'target_text']\n",
    "papers = papers.dropna()\n",
    "\n",
    "del titles, abstracts\n",
    "papers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "33073887",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: simpletransformers in c:\\users\\taniya jain\\anaconda3\\lib\\site-packages (0.63.6)\n",
      "Requirement already satisfied: wandb>=0.10.32 in c:\\users\\taniya jain\\anaconda3\\lib\\site-packages (from simpletransformers) (0.12.14)\n",
      "Requirement already satisfied: tensorboard in c:\\users\\taniya jain\\anaconda3\\lib\\site-packages (from simpletransformers) (2.8.0)\n",
      "Requirement already satisfied: tqdm>=4.47.0 in c:\\users\\taniya jain\\anaconda3\\lib\\site-packages (from simpletransformers) (4.62.3)\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\taniya jain\\anaconda3\\lib\\site-packages (from simpletransformers) (0.24.2)\n",
      "Requirement already satisfied: regex in c:\\users\\taniya jain\\anaconda3\\lib\\site-packages (from simpletransformers) (2021.8.3)\n",
      "Requirement already satisfied: seqeval in c:\\users\\taniya jain\\anaconda3\\lib\\site-packages (from simpletransformers) (1.2.2)\n",
      "Requirement already satisfied: datasets in c:\\users\\taniya jain\\anaconda3\\lib\\site-packages (from simpletransformers) (2.1.0)\n",
      "Requirement already satisfied: streamlit in c:\\users\\taniya jain\\anaconda3\\lib\\site-packages (from simpletransformers) (1.8.1)\n",
      "Requirement already satisfied: transformers>=4.6.0 in c:\\users\\taniya jain\\anaconda3\\lib\\site-packages (from simpletransformers) (4.17.0)\n",
      "Requirement already satisfied: numpy in c:\\users\\taniya jain\\anaconda3\\lib\\site-packages (from simpletransformers) (1.20.3)\n",
      "Requirement already satisfied: tokenizers in c:\\users\\taniya jain\\anaconda3\\lib\\site-packages (from simpletransformers) (0.11.6)\n",
      "Requirement already satisfied: pandas in c:\\users\\taniya jain\\anaconda3\\lib\\site-packages (from simpletransformers) (1.3.4)\n",
      "Requirement already satisfied: requests in c:\\users\\taniya jain\\anaconda3\\lib\\site-packages (from simpletransformers) (2.26.0)\n",
      "Requirement already satisfied: sentencepiece in c:\\users\\taniya jain\\anaconda3\\lib\\site-packages (from simpletransformers) (0.1.96)\n",
      "Requirement already satisfied: scipy in c:\\users\\taniya jain\\anaconda3\\lib\\site-packages (from simpletransformers) (1.7.1)\n",
      "Requirement already satisfied: colorama in c:\\users\\taniya jain\\anaconda3\\lib\\site-packages (from tqdm>=4.47.0->simpletransformers) (0.4.4)\n",
      "Requirement already satisfied: filelock in c:\\users\\taniya jain\\anaconda3\\lib\\site-packages (from transformers>=4.6.0->simpletransformers) (3.3.1)\n",
      "Requirement already satisfied: sacremoses in c:\\users\\taniya jain\\anaconda3\\lib\\site-packages (from transformers>=4.6.0->simpletransformers) (0.0.47)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\taniya jain\\anaconda3\\lib\\site-packages (from transformers>=4.6.0->simpletransformers) (21.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.1.0 in c:\\users\\taniya jain\\anaconda3\\lib\\site-packages (from transformers>=4.6.0->simpletransformers) (0.4.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\taniya jain\\anaconda3\\lib\\site-packages (from transformers>=4.6.0->simpletransformers) (6.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\taniya jain\\anaconda3\\lib\\site-packages (from huggingface-hub<1.0,>=0.1.0->transformers>=4.6.0->simpletransformers) (3.10.0.2)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in c:\\users\\taniya jain\\anaconda3\\lib\\site-packages (from packaging>=20.0->transformers>=4.6.0->simpletransformers) (3.0.4)\n",
      "Requirement already satisfied: python-dateutil>=2.6.1 in c:\\users\\taniya jain\\anaconda3\\lib\\site-packages (from wandb>=0.10.32->simpletransformers) (2.8.2)\n",
      "Requirement already satisfied: shortuuid>=0.5.0 in c:\\users\\taniya jain\\anaconda3\\lib\\site-packages (from wandb>=0.10.32->simpletransformers) (1.0.8)\n",
      "Requirement already satisfied: Click!=8.0.0,>=7.0 in c:\\users\\taniya jain\\anaconda3\\lib\\site-packages (from wandb>=0.10.32->simpletransformers) (8.0.3)\n",
      "Requirement already satisfied: docker-pycreds>=0.4.0 in c:\\users\\taniya jain\\anaconda3\\lib\\site-packages (from wandb>=0.10.32->simpletransformers) (0.4.0)\n",
      "Requirement already satisfied: pathtools in c:\\users\\taniya jain\\anaconda3\\lib\\site-packages (from wandb>=0.10.32->simpletransformers) (0.1.2)\n",
      "Requirement already satisfied: promise<3,>=2.0 in c:\\users\\taniya jain\\anaconda3\\lib\\site-packages (from wandb>=0.10.32->simpletransformers) (2.3)\n",
      "Requirement already satisfied: psutil>=5.0.0 in c:\\users\\taniya jain\\anaconda3\\lib\\site-packages (from wandb>=0.10.32->simpletransformers) (5.8.0)\n",
      "Requirement already satisfied: protobuf>=3.12.0 in c:\\users\\taniya jain\\anaconda3\\lib\\site-packages (from wandb>=0.10.32->simpletransformers) (3.20.0)\n",
      "Requirement already satisfied: GitPython>=1.0.0 in c:\\users\\taniya jain\\anaconda3\\lib\\site-packages (from wandb>=0.10.32->simpletransformers) (3.1.27)\n",
      "Requirement already satisfied: six>=1.13.0 in c:\\users\\taniya jain\\anaconda3\\lib\\site-packages (from wandb>=0.10.32->simpletransformers) (1.16.0)\n",
      "Requirement already satisfied: sentry-sdk>=1.0.0 in c:\\users\\taniya jain\\anaconda3\\lib\\site-packages (from wandb>=0.10.32->simpletransformers) (1.5.10)\n",
      "Requirement already satisfied: setproctitle in c:\\users\\taniya jain\\anaconda3\\lib\\site-packages (from wandb>=0.10.32->simpletransformers) (1.2.3)\n",
      "Requirement already satisfied: gitdb<5,>=4.0.1 in c:\\users\\taniya jain\\anaconda3\\lib\\site-packages (from GitPython>=1.0.0->wandb>=0.10.32->simpletransformers) (4.0.9)\n",
      "Requirement already satisfied: smmap<6,>=3.0.1 in c:\\users\\taniya jain\\anaconda3\\lib\\site-packages (from gitdb<5,>=4.0.1->GitPython>=1.0.0->wandb>=0.10.32->simpletransformers) (5.0.0)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\taniya jain\\anaconda3\\lib\\site-packages (from requests->simpletransformers) (1.26.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\taniya jain\\anaconda3\\lib\\site-packages (from requests->simpletransformers) (2021.10.8)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\taniya jain\\anaconda3\\lib\\site-packages (from requests->simpletransformers) (3.2)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in c:\\users\\taniya jain\\anaconda3\\lib\\site-packages (from requests->simpletransformers) (2.0.4)\n",
      "Requirement already satisfied: responses<0.19 in c:\\users\\taniya jain\\anaconda3\\lib\\site-packages (from datasets->simpletransformers) (0.18.0)\n",
      "Requirement already satisfied: xxhash in c:\\users\\taniya jain\\anaconda3\\lib\\site-packages (from datasets->simpletransformers) (3.0.0)\n",
      "Requirement already satisfied: dill in c:\\users\\taniya jain\\anaconda3\\lib\\site-packages (from datasets->simpletransformers) (0.3.4)\n",
      "Requirement already satisfied: fsspec[http]>=2021.05.0 in c:\\users\\taniya jain\\anaconda3\\lib\\site-packages (from datasets->simpletransformers) (2021.10.1)\n",
      "Requirement already satisfied: aiohttp in c:\\users\\taniya jain\\anaconda3\\lib\\site-packages (from datasets->simpletransformers) (3.8.1)\n",
      "Requirement already satisfied: multiprocess in c:\\users\\taniya jain\\anaconda3\\lib\\site-packages (from datasets->simpletransformers) (0.70.12.2)\n",
      "Requirement already satisfied: pyarrow>=5.0.0 in c:\\users\\taniya jain\\anaconda3\\lib\\site-packages (from datasets->simpletransformers) (7.0.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\taniya jain\\anaconda3\\lib\\site-packages (from aiohttp->datasets->simpletransformers) (6.0.2)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\taniya jain\\anaconda3\\lib\\site-packages (from aiohttp->datasets->simpletransformers) (21.2.0)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in c:\\users\\taniya jain\\anaconda3\\lib\\site-packages (from aiohttp->datasets->simpletransformers) (4.0.2)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\taniya jain\\anaconda3\\lib\\site-packages (from aiohttp->datasets->simpletransformers) (1.3.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in c:\\users\\taniya jain\\anaconda3\\lib\\site-packages (from aiohttp->datasets->simpletransformers) (1.2.0)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in c:\\users\\taniya jain\\anaconda3\\lib\\site-packages (from aiohttp->datasets->simpletransformers) (1.7.2)\n",
      "Requirement already satisfied: pytz>=2017.3 in c:\\users\\taniya jain\\anaconda3\\lib\\site-packages (from pandas->simpletransformers) (2021.3)\n",
      "Requirement already satisfied: joblib in c:\\users\\taniya jain\\anaconda3\\lib\\site-packages (from sacremoses->transformers>=4.6.0->simpletransformers) (1.1.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\taniya jain\\anaconda3\\lib\\site-packages (from scikit-learn->simpletransformers) (2.2.0)\n",
      "Requirement already satisfied: tornado>=5.0 in c:\\users\\taniya jain\\anaconda3\\lib\\site-packages (from streamlit->simpletransformers) (6.1)\n",
      "Requirement already satisfied: validators in c:\\users\\taniya jain\\anaconda3\\lib\\site-packages (from streamlit->simpletransformers) (0.18.2)\n",
      "Requirement already satisfied: toml in c:\\users\\taniya jain\\anaconda3\\lib\\site-packages (from streamlit->simpletransformers) (0.10.2)\n",
      "Requirement already satisfied: pympler>=0.9 in c:\\users\\taniya jain\\anaconda3\\lib\\site-packages (from streamlit->simpletransformers) (1.0.1)\n",
      "Requirement already satisfied: importlib-metadata>=1.4 in c:\\users\\taniya jain\\anaconda3\\lib\\site-packages (from streamlit->simpletransformers) (4.8.1)\n",
      "Requirement already satisfied: blinker in c:\\users\\taniya jain\\anaconda3\\lib\\site-packages (from streamlit->simpletransformers) (1.4)\n",
      "Requirement already satisfied: tzlocal in c:\\users\\taniya jain\\anaconda3\\lib\\site-packages (from streamlit->simpletransformers) (4.2)\n",
      "Requirement already satisfied: watchdog in c:\\users\\taniya jain\\anaconda3\\lib\\site-packages (from streamlit->simpletransformers) (2.1.3)\n",
      "Requirement already satisfied: altair>=3.2.0 in c:\\users\\taniya jain\\anaconda3\\lib\\site-packages (from streamlit->simpletransformers) (4.2.0)\n",
      "Requirement already satisfied: pillow>=6.2.0 in c:\\users\\taniya jain\\anaconda3\\lib\\site-packages (from streamlit->simpletransformers) (8.4.0)\n",
      "Requirement already satisfied: cachetools>=4.0 in c:\\users\\taniya jain\\anaconda3\\lib\\site-packages (from streamlit->simpletransformers) (5.0.0)\n",
      "Requirement already satisfied: pydeck>=0.1.dev5 in c:\\users\\taniya jain\\anaconda3\\lib\\site-packages (from streamlit->simpletransformers) (0.7.1)\n",
      "Requirement already satisfied: semver in c:\\users\\taniya jain\\anaconda3\\lib\\site-packages (from streamlit->simpletransformers) (2.13.0)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\taniya jain\\anaconda3\\lib\\site-packages (from altair>=3.2.0->streamlit->simpletransformers) (2.11.3)\n",
      "Requirement already satisfied: toolz in c:\\users\\taniya jain\\anaconda3\\lib\\site-packages (from altair>=3.2.0->streamlit->simpletransformers) (0.11.1)\n",
      "Requirement already satisfied: entrypoints in c:\\users\\taniya jain\\anaconda3\\lib\\site-packages (from altair>=3.2.0->streamlit->simpletransformers) (0.3)\n",
      "Requirement already satisfied: jsonschema>=3.0 in c:\\users\\taniya jain\\anaconda3\\lib\\site-packages (from altair>=3.2.0->streamlit->simpletransformers) (3.2.0)\n",
      "Requirement already satisfied: zipp>=0.5 in c:\\users\\taniya jain\\anaconda3\\lib\\site-packages (from importlib-metadata>=1.4->streamlit->simpletransformers) (3.6.0)\n",
      "Requirement already satisfied: pyrsistent>=0.14.0 in c:\\users\\taniya jain\\anaconda3\\lib\\site-packages (from jsonschema>=3.0->altair>=3.2.0->streamlit->simpletransformers) (0.18.0)\n",
      "Requirement already satisfied: setuptools in c:\\users\\taniya jain\\anaconda3\\lib\\site-packages (from jsonschema>=3.0->altair>=3.2.0->streamlit->simpletransformers) (58.0.4)\n",
      "Requirement already satisfied: ipywidgets>=7.0.0 in c:\\users\\taniya jain\\anaconda3\\lib\\site-packages (from pydeck>=0.1.dev5->streamlit->simpletransformers) (7.6.5)\n",
      "Requirement already satisfied: traitlets>=4.3.2 in c:\\users\\taniya jain\\anaconda3\\lib\\site-packages (from pydeck>=0.1.dev5->streamlit->simpletransformers) (5.1.0)\n",
      "Requirement already satisfied: ipykernel>=5.1.2 in c:\\users\\taniya jain\\anaconda3\\lib\\site-packages (from pydeck>=0.1.dev5->streamlit->simpletransformers) (6.4.1)\n",
      "Requirement already satisfied: ipython<8.0,>=7.23.1 in c:\\users\\taniya jain\\anaconda3\\lib\\site-packages (from ipykernel>=5.1.2->pydeck>=0.1.dev5->streamlit->simpletransformers) (7.29.0)\n",
      "Requirement already satisfied: debugpy<2.0,>=1.0.0 in c:\\users\\taniya jain\\anaconda3\\lib\\site-packages (from ipykernel>=5.1.2->pydeck>=0.1.dev5->streamlit->simpletransformers) (1.4.1)\n",
      "Requirement already satisfied: jupyter-client<8.0 in c:\\users\\taniya jain\\anaconda3\\lib\\site-packages (from ipykernel>=5.1.2->pydeck>=0.1.dev5->streamlit->simpletransformers) (6.1.12)\n",
      "Requirement already satisfied: matplotlib-inline<0.2.0,>=0.1.0 in c:\\users\\taniya jain\\anaconda3\\lib\\site-packages (from ipykernel>=5.1.2->pydeck>=0.1.dev5->streamlit->simpletransformers) (0.1.2)\n",
      "Requirement already satisfied: ipython-genutils in c:\\users\\taniya jain\\anaconda3\\lib\\site-packages (from ipykernel>=5.1.2->pydeck>=0.1.dev5->streamlit->simpletransformers) (0.2.0)\n",
      "Requirement already satisfied: decorator in c:\\users\\taniya jain\\anaconda3\\lib\\site-packages (from ipython<8.0,>=7.23.1->ipykernel>=5.1.2->pydeck>=0.1.dev5->streamlit->simpletransformers) (5.1.0)\n",
      "Requirement already satisfied: jedi>=0.16 in c:\\users\\taniya jain\\anaconda3\\lib\\site-packages (from ipython<8.0,>=7.23.1->ipykernel>=5.1.2->pydeck>=0.1.dev5->streamlit->simpletransformers) (0.18.0)\n",
      "Requirement already satisfied: prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0 in c:\\users\\taniya jain\\anaconda3\\lib\\site-packages (from ipython<8.0,>=7.23.1->ipykernel>=5.1.2->pydeck>=0.1.dev5->streamlit->simpletransformers) (3.0.20)\n",
      "Requirement already satisfied: pickleshare in c:\\users\\taniya jain\\anaconda3\\lib\\site-packages (from ipython<8.0,>=7.23.1->ipykernel>=5.1.2->pydeck>=0.1.dev5->streamlit->simpletransformers) (0.7.5)\n",
      "Requirement already satisfied: backcall in c:\\users\\taniya jain\\anaconda3\\lib\\site-packages (from ipython<8.0,>=7.23.1->ipykernel>=5.1.2->pydeck>=0.1.dev5->streamlit->simpletransformers) (0.2.0)\n",
      "Requirement already satisfied: pygments in c:\\users\\taniya jain\\anaconda3\\lib\\site-packages (from ipython<8.0,>=7.23.1->ipykernel>=5.1.2->pydeck>=0.1.dev5->streamlit->simpletransformers) (2.10.0)\n",
      "Requirement already satisfied: widgetsnbextension~=3.5.0 in c:\\users\\taniya jain\\anaconda3\\lib\\site-packages (from ipywidgets>=7.0.0->pydeck>=0.1.dev5->streamlit->simpletransformers) (3.5.1)\n",
      "Requirement already satisfied: nbformat>=4.2.0 in c:\\users\\taniya jain\\anaconda3\\lib\\site-packages (from ipywidgets>=7.0.0->pydeck>=0.1.dev5->streamlit->simpletransformers) (5.1.3)\n",
      "Requirement already satisfied: jupyterlab-widgets>=1.0.0 in c:\\users\\taniya jain\\anaconda3\\lib\\site-packages (from ipywidgets>=7.0.0->pydeck>=0.1.dev5->streamlit->simpletransformers) (1.0.0)\n",
      "Requirement already satisfied: parso<0.9.0,>=0.8.0 in c:\\users\\taniya jain\\anaconda3\\lib\\site-packages (from jedi>=0.16->ipython<8.0,>=7.23.1->ipykernel>=5.1.2->pydeck>=0.1.dev5->streamlit->simpletransformers) (0.8.2)\n",
      "Requirement already satisfied: MarkupSafe>=0.23 in c:\\users\\taniya jain\\anaconda3\\lib\\site-packages (from jinja2->altair>=3.2.0->streamlit->simpletransformers) (1.1.1)\n",
      "Requirement already satisfied: jupyter-core>=4.6.0 in c:\\users\\taniya jain\\anaconda3\\lib\\site-packages (from jupyter-client<8.0->ipykernel>=5.1.2->pydeck>=0.1.dev5->streamlit->simpletransformers) (4.8.1)\n",
      "Requirement already satisfied: pyzmq>=13 in c:\\users\\taniya jain\\anaconda3\\lib\\site-packages (from jupyter-client<8.0->ipykernel>=5.1.2->pydeck>=0.1.dev5->streamlit->simpletransformers) (22.2.1)\n",
      "Requirement already satisfied: pywin32>=1.0 in c:\\users\\taniya jain\\anaconda3\\lib\\site-packages (from jupyter-core>=4.6.0->jupyter-client<8.0->ipykernel>=5.1.2->pydeck>=0.1.dev5->streamlit->simpletransformers) (228)\n",
      "Requirement already satisfied: wcwidth in c:\\users\\taniya jain\\anaconda3\\lib\\site-packages (from prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0->ipython<8.0,>=7.23.1->ipykernel>=5.1.2->pydeck>=0.1.dev5->streamlit->simpletransformers) (0.2.5)\n",
      "Requirement already satisfied: notebook>=4.4.1 in c:\\users\\taniya jain\\anaconda3\\lib\\site-packages (from widgetsnbextension~=3.5.0->ipywidgets>=7.0.0->pydeck>=0.1.dev5->streamlit->simpletransformers) (6.4.5)\n",
      "Requirement already satisfied: argon2-cffi in c:\\users\\taniya jain\\anaconda3\\lib\\site-packages (from notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets>=7.0.0->pydeck>=0.1.dev5->streamlit->simpletransformers) (20.1.0)\n",
      "Requirement already satisfied: Send2Trash>=1.5.0 in c:\\users\\taniya jain\\anaconda3\\lib\\site-packages (from notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets>=7.0.0->pydeck>=0.1.dev5->streamlit->simpletransformers) (1.8.0)\n",
      "Requirement already satisfied: prometheus-client in c:\\users\\taniya jain\\anaconda3\\lib\\site-packages (from notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets>=7.0.0->pydeck>=0.1.dev5->streamlit->simpletransformers) (0.11.0)\n",
      "Requirement already satisfied: terminado>=0.8.3 in c:\\users\\taniya jain\\anaconda3\\lib\\site-packages (from notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets>=7.0.0->pydeck>=0.1.dev5->streamlit->simpletransformers) (0.9.4)\n",
      "Requirement already satisfied: nbconvert in c:\\users\\taniya jain\\anaconda3\\lib\\site-packages (from notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets>=7.0.0->pydeck>=0.1.dev5->streamlit->simpletransformers) (6.1.0)\n",
      "Requirement already satisfied: pywinpty>=0.5 in c:\\users\\taniya jain\\anaconda3\\lib\\site-packages (from terminado>=0.8.3->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets>=7.0.0->pydeck>=0.1.dev5->streamlit->simpletransformers) (0.5.7)\n",
      "Requirement already satisfied: cffi>=1.0.0 in c:\\users\\taniya jain\\anaconda3\\lib\\site-packages (from argon2-cffi->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets>=7.0.0->pydeck>=0.1.dev5->streamlit->simpletransformers) (1.14.6)\n",
      "Requirement already satisfied: pycparser in c:\\users\\taniya jain\\anaconda3\\lib\\site-packages (from cffi>=1.0.0->argon2-cffi->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets>=7.0.0->pydeck>=0.1.dev5->streamlit->simpletransformers) (2.20)\n",
      "Requirement already satisfied: testpath in c:\\users\\taniya jain\\anaconda3\\lib\\site-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets>=7.0.0->pydeck>=0.1.dev5->streamlit->simpletransformers) (0.5.0)\n",
      "Requirement already satisfied: defusedxml in c:\\users\\taniya jain\\anaconda3\\lib\\site-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets>=7.0.0->pydeck>=0.1.dev5->streamlit->simpletransformers) (0.7.1)\n",
      "Requirement already satisfied: mistune<2,>=0.8.1 in c:\\users\\taniya jain\\anaconda3\\lib\\site-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets>=7.0.0->pydeck>=0.1.dev5->streamlit->simpletransformers) (0.8.4)\n",
      "Requirement already satisfied: pandocfilters>=1.4.1 in c:\\users\\taniya jain\\anaconda3\\lib\\site-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets>=7.0.0->pydeck>=0.1.dev5->streamlit->simpletransformers) (1.4.3)\n",
      "Requirement already satisfied: bleach in c:\\users\\taniya jain\\anaconda3\\lib\\site-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets>=7.0.0->pydeck>=0.1.dev5->streamlit->simpletransformers) (4.0.0)\n",
      "Requirement already satisfied: nbclient<0.6.0,>=0.5.0 in c:\\users\\taniya jain\\anaconda3\\lib\\site-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets>=7.0.0->pydeck>=0.1.dev5->streamlit->simpletransformers) (0.5.3)\n",
      "Requirement already satisfied: jupyterlab-pygments in c:\\users\\taniya jain\\anaconda3\\lib\\site-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets>=7.0.0->pydeck>=0.1.dev5->streamlit->simpletransformers) (0.1.2)\n",
      "Requirement already satisfied: async-generator in c:\\users\\taniya jain\\anaconda3\\lib\\site-packages (from nbclient<0.6.0,>=0.5.0->nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets>=7.0.0->pydeck>=0.1.dev5->streamlit->simpletransformers) (1.10)\n",
      "Requirement already satisfied: nest-asyncio in c:\\users\\taniya jain\\anaconda3\\lib\\site-packages (from nbclient<0.6.0,>=0.5.0->nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets>=7.0.0->pydeck>=0.1.dev5->streamlit->simpletransformers) (1.5.1)\n",
      "Requirement already satisfied: webencodings in c:\\users\\taniya jain\\anaconda3\\lib\\site-packages (from bleach->nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets>=7.0.0->pydeck>=0.1.dev5->streamlit->simpletransformers) (0.5.1)\n",
      "Requirement already satisfied: werkzeug>=0.11.15 in c:\\users\\taniya jain\\anaconda3\\lib\\site-packages (from tensorboard->simpletransformers) (2.0.2)\n",
      "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in c:\\users\\taniya jain\\anaconda3\\lib\\site-packages (from tensorboard->simpletransformers) (1.8.1)\n",
      "Requirement already satisfied: google-auth<3,>=1.6.3 in c:\\users\\taniya jain\\anaconda3\\lib\\site-packages (from tensorboard->simpletransformers) (2.6.5)\n",
      "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in c:\\users\\taniya jain\\anaconda3\\lib\\site-packages (from tensorboard->simpletransformers) (0.4.6)\n",
      "Requirement already satisfied: absl-py>=0.4 in c:\\users\\taniya jain\\anaconda3\\lib\\site-packages (from tensorboard->simpletransformers) (1.0.0)\n",
      "Requirement already satisfied: grpcio>=1.24.3 in c:\\users\\taniya jain\\anaconda3\\lib\\site-packages (from tensorboard->simpletransformers) (1.44.0)\n",
      "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in c:\\users\\taniya jain\\anaconda3\\lib\\site-packages (from tensorboard->simpletransformers) (0.6.1)\n",
      "Requirement already satisfied: markdown>=2.6.8 in c:\\users\\taniya jain\\anaconda3\\lib\\site-packages (from tensorboard->simpletransformers) (3.3.6)\n",
      "Requirement already satisfied: wheel>=0.26 in c:\\users\\taniya jain\\anaconda3\\lib\\site-packages (from tensorboard->simpletransformers) (0.37.0)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in c:\\users\\taniya jain\\anaconda3\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard->simpletransformers) (4.8)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in c:\\users\\taniya jain\\anaconda3\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard->simpletransformers) (0.2.8)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in c:\\users\\taniya jain\\anaconda3\\lib\\site-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard->simpletransformers) (1.3.1)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in c:\\users\\taniya jain\\anaconda3\\lib\\site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard->simpletransformers) (0.4.8)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in c:\\users\\taniya jain\\anaconda3\\lib\\site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard->simpletransformers) (3.2.0)\n",
      "Requirement already satisfied: tzdata in c:\\users\\taniya jain\\anaconda3\\lib\\site-packages (from tzlocal->streamlit->simpletransformers) (2022.1)\n",
      "Requirement already satisfied: pytz-deprecation-shim in c:\\users\\taniya jain\\anaconda3\\lib\\site-packages (from tzlocal->streamlit->simpletransformers) (0.1.0.post0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install simpletransformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "02a9f509",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 12.3 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "\n",
    "from simpletransformers.seq2seq import Seq2SeqModel\n",
    "\n",
    "eval_df = papers.sample(frac=0.1, random_state=42)\n",
    "train_df = papers.drop(eval_df.index)\n",
    "\n",
    "model_args = {\n",
    "    \"reprocess_input_data\": True,\n",
    "    \"overwrite_output_dir\": True,\n",
    "    \"save_model_every_epoch\": False,\n",
    "    \"save_eval_checkpoints\": False,\n",
    "    \"max_seq_length\": 512,\n",
    "    \"train_batch_size\": 6,\n",
    "    \"num_train_epochs\": 3,\n",
    "    \"fp16\": False,\n",
    "\n",
    "}\n",
    "\n",
    "\n",
    "# Create a Bart-base model\n",
    "model = Seq2SeqModel(encoder_decoder_type=\"bart\",\n",
    "                    encoder_decoder_name=\"facebook/bart-base\", use_cuda=False,\n",
    "                    args=model_args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "aa2600a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:simpletransformers.seq2seq.seq2seq_utils: Creating features from dataset file at cache_dir/\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "82543b1a92544810ba23e750a998d0df",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/85 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:simpletransformers.seq2seq.seq2seq_model: Training started\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e1e0bb9060f34ed687f3c7a7bf4c0b8f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "13366d5420f84bce94d3009f7b7d4e1c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running Epoch 0 of 3:   0%|          | 0/15 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ba398f68027f4dafa653abe0892e2ff4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running Epoch 1 of 3:   0%|          | 0/15 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ce6a72d84eee46169c7a31a9bd51aa99",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running Epoch 2 of 3:   0%|          | 0/15 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:simpletransformers.seq2seq.seq2seq_model:Saving model into outputs/\n",
      "INFO:simpletransformers.seq2seq.seq2seq_model: Training of facebook/bart-base model complete. Saved to outputs/.\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'pickle' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'pickle' is not defined"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "import pandas\n",
    "from sklearn import model_selection\n",
    "# Train the model\n",
    "model.train_model(train_df)\n",
    "\n",
    "filename = 'finalized_model.sav'\n",
    "pickle.dump(model, open(filename, 'wb'))\n",
    "\n",
    "# Evaluate the model\n",
    "result = model.eval_model(eval_df)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ac32511f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "180c3c633c4044739ec689cba1276f1c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating outputs:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True Title: Preference-Based Monte Carlo Tree Search\n",
      "\n",
      "Predicted Title: A Monte Carlo Tree Search: A Unique and Unique Approach\n",
      "\n",
      "Abstract:   Monte Carlo tree search (MCTS) is a popular choice for solving sequentialanytime problems. However, it depends on a numeric feedback signal, which canbe difficult to define. Real-time MCTS is a variant which may only rarelyencounter states with an explicit, extrinsic reward. To deal with such cases,the experimenter has to supply an additional numeric feedback signal in theform of a heuristic, which intrinsically guides the agent. Recent work hasshown evidence that in different areas the underlying structure is ordinal andnot numerical. Hence erroneous and biased heuristics are inevitable, especiallyin such domains. In this paper, we propose a MCTS variant which only depends onqualitative feedback, and therefore opens up new applications for MCTS. We alsofind indications that translating absolute into ordinal feedback may bebeneficial. Using a puzzle domain, we show that our preference-based MCTSvariant, wich only receives qualitative feedback, is able to reach aperformance level comparable to a regular MCTS baseline, which obtainsquantitative feedback.\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fc5246efb8d743bca1cb719288741a2a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating outputs:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True Title: The SP theory of intelligence and the representation and processing of\n",
      "  knowledge in the brain\n",
      "\n",
      "Predicted Title: A SP theory of intelligence: An overview and partial model\n",
      "\n",
      "Abstract:   The \"SP theory of intelligence\", with its realisation in the \"SP computermodel\", aims to simplify and integrate observations and concepts acrossAI-related fields, with information compression as a unifying theme. This paperdescribes how abstract structures and processes in the theory may be realisedin terms of neurons, their interconnections, and the transmission of signalsbetween neurons. This part of the SP theory -- \"SP-neural\" -- is a tentativeand partial model for the representation and processing of knowledge in thebrain. In the SP theory (apart from SP-neural), all kinds of knowledge arerepresented with \"patterns\", where a pattern is an array of atomic symbols inone or two dimensions. In SP-neural, the concept of a \"pattern\" is realised asan array of neurons called a \"pattern assembly\", similar to Hebb's concept of a\"cell assembly\" but with important differences. Central to the processing ofinformation in the SP system is the powerful concept of \"multiple alignment\",borrowed and adapted from bioinformatics. Processes such as patternrecognition, reasoning and problem solving are achieved via the building ofmultiple alignments, while unsupervised learning -- significantly differentfrom the \"Hebbian\" kinds of learning -- is achieved by creating patterns fromsensory information and also by creating patterns from multiple alignments inwhich there is a partial match between one pattern and another. Short-livedneural structures equivalent to multiple alignments will be created via aninter-play of excitatory and inhibitory neural signals. The paper discussesseveral associated issues, with relevant empirical evidence.\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8ea52e644df349fe99fc026eb7a6fe76",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating outputs:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True Title: Configurable Markov Decision Processes\n",
      "\n",
      "Predicted Title: AConfigurableMarkov Decision Processes\n",
      "\n",
      "Abstract:   In many real-world problems, there is the possibility to configure, to alimited extent, some environmental parameters to improve the performance of alearning agent. In this paper, we propose a novel framework, ConfigurableMarkov Decision Processes (Conf-MDPs), to model this new type of interactionwith the environment. Furthermore, we provide a new learning algorithm, SafePolicy-Model Iteration (SPMI), to jointly and adaptively optimize the policyand the environment configuration. After having introduced our approach andderived some theoretical results, we present the experimental evaluation in twoexplicative problems to show the benefits of the environment configurability onthe performance of the learned policy.\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "23d6250b07974ea2a14ec80c56ccccaa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating outputs:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True Title: A Directional Feature with Energy based Offline Signature Verification\n",
      "  Network\n",
      "\n",
      "Predicted Title: Aofflinesignature verification system\n",
      "\n",
      "Abstract:   Signature used as a biometric is implemented in various systems as well asevery signature signed by each person is distinct at the same time. So, it isvery important to have a computerized signature verification system. In offlinesignature verification system dynamic features are not available obviously, butone can use a signature as an image and apply image processing techniques tomake an effective offline signature verification system. Author proposes aintelligent network used directional feature and energy density both as inputsto the same network and classifies the signature. Neural network is used as aclassifier for this system. The results are compared with both the very basicenergy density method and a simple directional feature method of offlinesignature verification system and this proposed new network is found veryeffective as compared to the above two methods, specially for less number oftraining samples, which can be implemented practically.\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1ce2e7dad5ee40b5a75cb69f33e3dbba",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating outputs:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True Title: The SP theory of intelligence and the representation and processing of\n",
      "  knowledge in the brain\n",
      "\n",
      "Predicted Title: A SP theory of intelligence: An overview and partial model\n",
      "\n",
      "Abstract:   The \"SP theory of intelligence\", with its realisation in the \"SP computermodel\", aims to simplify and integrate observations and concepts acrossAI-related fields, with information compression as a unifying theme. This paperdescribes how abstract structures and processes in the theory may be realisedin terms of neurons, their interconnections, and the transmission of signalsbetween neurons. This part of the SP theory -- \"SP-neural\" -- is a tentativeand partial model for the representation and processing of knowledge in thebrain. In the SP theory (apart from SP-neural), all kinds of knowledge arerepresented with \"patterns\", where a pattern is an array of atomic symbols inone or two dimensions. In SP-neural, the concept of a \"pattern\" is realised asan array of neurons called a \"pattern assembly\", similar to Hebb's concept of a\"cell assembly\" but with important differences. Central to the processing ofinformation in the SP system is the powerful concept of \"multiple alignment\",borrowed and adapted from bioinformatics. Processes such as patternrecognition, reasoning and problem solving are achieved via the building ofmultiple alignments, while unsupervised learning -- significantly differentfrom the \"Hebbian\" kinds of learning -- is achieved by creating patterns fromsensory information and also by creating patterns from multiple alignments inwhich there is a partial match between one pattern and another. Short-livedneural structures equivalent to multiple alignments will be created via aninter-play of excitatory and inhibitory neural signals. The paper discussesseveral associated issues, with relevant empirical evidence.\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "61f779dff66146b9b00d64f05a1443c1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating outputs:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True Title: The emergence of Explainability of Intelligent Systems: Delivering\n",
      "  Explainable and Personalised Recommendations for Energy Efficiency\n",
      "\n",
      "Predicted Title: Aation Systems for Energy Efficiency\n",
      "\n",
      "Abstract:   The recent advances in artificial intelligence namely in machine learning anddeep learning, have boosted the performance of intelligent systems in severalways. This gave rise to human expectations, but also created the need for adeeper understanding of how intelligent systems think and decide. The conceptof explainability appeared, in the extent of explaining the internal systemmechanics in human terms. Recommendation systems are intelligent systems thatsupport human decision making, and as such, they have to be explainable inorder to increase user trust and improve the acceptance of recommendations. Inthis work, we focus on a context-aware recommendation system for energyefficiency and develop a mechanism for explainable and persuasiverecommendations, which are personalized to user preferences and habits. Thepersuasive facts either emphasize on the economical saving prospects (Econ) oron a positive ecological impact (Eco) and explanations provide the reason forrecommending an energy saving action. Based on a study conducted using aTelegram bot, different scenarios have been validated with actual data andhuman feedback. Current results show a total increase of 19\\% on therecommendation acceptance ratio when both economical and ecological persuasivefacts are employed. This revolutionary approach on recommendation systems,demonstrates how intelligent recommendations can effectively encourage energysaving behavior.\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2f066e8568714c61a9679f805ec038f6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating outputs:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True Title: A Survey of Multi-Objective Sequential Decision-Making\n",
      "\n",
      "Predicted Title: A Multi-Objective Methods for Planning and Learning\n",
      "\n",
      "Abstract:   Sequential decision-making problems with multiple objectives arise naturallyin practice and pose unique challenges for research in decision-theoreticplanning and learning, which has largely focused on single-objective settings.This article surveys algorithms designed for sequential decision-makingproblems with multiple objectives. Though there is a growing body of literatureon this subject, little of it makes explicit under what circumstances specialmethods are needed to solve multi-objective problems. Therefore, we identifythree distinct scenarios in which converting such a problem to asingle-objective one is impossible, infeasible, or undesirable. Furthermore, wepropose a taxonomy that classifies multi-objective methods according to theapplicable scenario, the nature of the scalarization function (which projectsmulti-objective values to scalar ones), and the type of policies considered. Weshow how these factors determine the nature of an optimal solution, which canbe a single policy, a convex hull, or a Pareto front. Using this taxonomy, wesurvey the literature on multi-objective methods for planning and learning.Finally, we discuss key applications of such methods and outline opportunitiesfor future work.\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5ecb1de57fcd4e4c89703c28b5a8709a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating outputs:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True Title: Adding Context to Concept Trees\n",
      "\n",
      "Predicted Title: A Concept Trees: A Concept Base\n",
      "\n",
      "Abstract:   A Concept Tree is a structure for storing knowledge where the trees arestored in a database called a Concept Base. It sits between the highlydistributed neural architectures and the distributed information systems, withthe intention of bringing brain-like and computer systems closer together.Concept Trees can grow from the semi-structured sources when consistentsequences of concepts are presented. Each tree ideally represents a singlecohesive concept and the trees can link with each other for navigation andsemantic purposes. The trees are therefore also a type of semantic network andwould benefit from having a consistent level of context for each node. Aconsistent build process is managed through a 'counting rule' and some otherrules that can normalise the database structure. This restricted structure canthen be complimented and enriched by the more dynamic context. It is alsosuggested to use the linking structure of the licas system [15] as a basis forthe context links, where the mathematical model is extended further to definethis. A number of tests have demonstrated the soundness of the architecture.Building the trees from text documents shows that the tree structure could beinherent in natural language. Then, two types of query language are described.Both of these can perform consistent query processes to return knowledge to theuser and even enhance the query with new knowledge. This is supported evenfurther with direct comparisons to a cognitive model, also being developed bythe author.\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "88f092877b0f43a1889c3cf2a009621b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating outputs:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True Title: Configurable Markov Decision Processes\n",
      "\n",
      "Predicted Title: AConfigurableMarkov Decision Processes\n",
      "\n",
      "Abstract:   In many real-world problems, there is the possibility to configure, to alimited extent, some environmental parameters to improve the performance of alearning agent. In this paper, we propose a novel framework, ConfigurableMarkov Decision Processes (Conf-MDPs), to model this new type of interactionwith the environment. Furthermore, we provide a new learning algorithm, SafePolicy-Model Iteration (SPMI), to jointly and adaptively optimize the policyand the environment configuration. After having introduced our approach andderived some theoretical results, we present the experimental evaluation in twoexplicative problems to show the benefits of the environment configurability onthe performance of the learned policy.\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e8d2212d29d948788a7a7823a112b490",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating outputs:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True Title: A Hybrid LP-RPG Heuristic for Modelling Numeric Resource Flows in\n",
      "  Planning\n",
      "\n",
      "Predicted Title: A-Based Hybrid Heuristic for numeric planning problems\n",
      "\n",
      "Abstract:   Although the use of metric fluents is fundamental to many practical planningproblems, the study of heuristics to support fully automated planners workingwith these fluents remains relatively unexplored. The most widely usedheuristic is the relaxation of metric fluents into interval-valued variables--- an idea first proposed a decade ago. Other heuristics depend on domainencodings that supply additional information about fluents, such as capacityconstraints or other resource-related annotations. A particular challenge tothese approaches is in handling interactions between metric fluents thatrepresent exchange, such as the transformation of quantities of raw materialsinto quantities of processed goods, or trading of money for materials. Theusual relaxation of metric fluents is often very poor in these situations,since it does not recognise that resources, once spent, are no longer availableto be spent again. We present a heuristic for numeric planning problemsbuilding on the propositional relaxed planning graph, but using a mathematicalprogram for numeric reasoning. We define a class of producer--consumer planningproblems and demonstrate how the numeric constraints in these can be modelledin a mixed integer program (MIP). This MIP is then combined with a metricRelaxed Planning Graph (RPG) heuristic to produce an integrated hybridheuristic. The MIP tracks resource use more accurately than the usualrelaxation, but relaxes the ordering of actions, while the RPG captures thecausal propositional aspects of the problem. We discuss how these twocomponents interact to produce a single unified heuristic and go on to explorehow further numeric features of planning problems can be integrated into theMIP. We show that encoding a limited subset of the propositional problem toaugment the MIP can yield more accurate guidance, partly by exploitingstructure such as propositional landmarks and propositional resources. Ourresults show that the use of this heuristic enhances scalability on problemswhere numeric resource interaction is key in finding a solution.\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5bdeed8dd6a84becb94239287993d377",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating outputs:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True Title: A Directional Feature with Energy based Offline Signature Verification\n",
      "  Network\n",
      "\n",
      "Predicted Title: Aofflinesignature verification system\n",
      "\n",
      "Abstract:   Signature used as a biometric is implemented in various systems as well asevery signature signed by each person is distinct at the same time. So, it isvery important to have a computerized signature verification system. In offlinesignature verification system dynamic features are not available obviously, butone can use a signature as an image and apply image processing techniques tomake an effective offline signature verification system. Author proposes aintelligent network used directional feature and energy density both as inputsto the same network and classifies the signature. Neural network is used as aclassifier for this system. The results are compared with both the very basicenergy density method and a simple directional feature method of offlinesignature verification system and this proposed new network is found veryeffective as compared to the above two methods, specially for less number oftraining samples, which can be implemented practically.\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ba7ed6885c2e4ed585bdfce9c71c23be",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating outputs:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True Title: A Survey of Multi-Objective Sequential Decision-Making\n",
      "\n",
      "Predicted Title: A Multi-Objective Methods for Planning and Learning\n",
      "\n",
      "Abstract:   Sequential decision-making problems with multiple objectives arise naturallyin practice and pose unique challenges for research in decision-theoreticplanning and learning, which has largely focused on single-objective settings.This article surveys algorithms designed for sequential decision-makingproblems with multiple objectives. Though there is a growing body of literatureon this subject, little of it makes explicit under what circumstances specialmethods are needed to solve multi-objective problems. Therefore, we identifythree distinct scenarios in which converting such a problem to asingle-objective one is impossible, infeasible, or undesirable. Furthermore, wepropose a taxonomy that classifies multi-objective methods according to theapplicable scenario, the nature of the scalarization function (which projectsmulti-objective values to scalar ones), and the type of policies considered. Weshow how these factors determine the nature of an optimal solution, which canbe a single policy, a convex hull, or a Pareto front. Using this taxonomy, wesurvey the literature on multi-objective methods for planning and learning.Finally, we discuss key applications of such methods and outline opportunitiesfor future work.\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "20b38fc65735497396906355b331f093",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating outputs:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True Title: The SP theory of intelligence and the representation and processing of\n",
      "  knowledge in the brain\n",
      "\n",
      "Predicted Title: A SP theory of intelligence: An overview and partial model\n",
      "\n",
      "Abstract:   The \"SP theory of intelligence\", with its realisation in the \"SP computermodel\", aims to simplify and integrate observations and concepts acrossAI-related fields, with information compression as a unifying theme. This paperdescribes how abstract structures and processes in the theory may be realisedin terms of neurons, their interconnections, and the transmission of signalsbetween neurons. This part of the SP theory -- \"SP-neural\" -- is a tentativeand partial model for the representation and processing of knowledge in thebrain. In the SP theory (apart from SP-neural), all kinds of knowledge arerepresented with \"patterns\", where a pattern is an array of atomic symbols inone or two dimensions. In SP-neural, the concept of a \"pattern\" is realised asan array of neurons called a \"pattern assembly\", similar to Hebb's concept of a\"cell assembly\" but with important differences. Central to the processing ofinformation in the SP system is the powerful concept of \"multiple alignment\",borrowed and adapted from bioinformatics. Processes such as patternrecognition, reasoning and problem solving are achieved via the building ofmultiple alignments, while unsupervised learning -- significantly differentfrom the \"Hebbian\" kinds of learning -- is achieved by creating patterns fromsensory information and also by creating patterns from multiple alignments inwhich there is a partial match between one pattern and another. Short-livedneural structures equivalent to multiple alignments will be created via aninter-play of excitatory and inhibitory neural signals. The paper discussesseveral associated issues, with relevant empirical evidence.\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0b6e373fd654412695bd2b078ef33086",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating outputs:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True Title: A note on the complexity of the causal ordering problem\n",
      "\n",
      "Predicted Title: A causal ordering: A concise report on the complexity of the causalordering problem\n",
      "\n",
      "Abstract:   In this note we provide a concise report on the complexity of the causalordering problem, originally introduced by Simon to reason about causaldependencies implicit in systems of mathematical equations. We show thatSimon's classical algorithm to infer causal ordering is NP-Hard---anintractability previously guessed but never proven. We present then a detailedaccount based on Nayak's suggested algorithmic solution (the best available),which is dominated by computing transitive closure---bounded in time by$O(|\\mathcal V|\\cdot |\\mathcal S|)$, where $\\mathcal S(\\mathcal E, \\mathcal V)$is the input system structure composed of a set $\\mathcal E$ of equations overa set $\\mathcal V$ of variables with number of variable appearances (density)$|\\mathcal S|$. We also comment on the potential of causal ordering foremerging applications in large-scale hypothesis management and analytics.\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6cf84624a4af4620bdd71d44bd121f2c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating outputs:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True Title: The Multi-Agent Reinforcement Learning in Malm\\\"O (MARL\\\"O) Competition\n",
      "\n",
      "Predicted Title: A Multi-Agent Reinforcement Learning in Malm\\\"O\n",
      "\n",
      "Abstract:   Learning in multi-agent scenarios is a fruitful research direction, butcurrent approaches still show scalability problems in multiple games withgeneral reward settings and different opponent types. The Multi-AgentReinforcement Learning in Malm\\\"O (MARL\\\"O) competition is a new challenge thatproposes research in this domain using multiple 3D games. The goal of thiscontest is to foster research in general agents that can learn across differentgames and opponent types, proposing a challenge as a milestone in the directionof Artificial General Intelligence.\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3f7b80306f134e37a995c0c4e87e71d9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating outputs:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True Title: Preference-Based Monte Carlo Tree Search\n",
      "\n",
      "Predicted Title: A Monte Carlo Tree Search: A Unique and Unique Approach\n",
      "\n",
      "Abstract:   Monte Carlo tree search (MCTS) is a popular choice for solving sequentialanytime problems. However, it depends on a numeric feedback signal, which canbe difficult to define. Real-time MCTS is a variant which may only rarelyencounter states with an explicit, extrinsic reward. To deal with such cases,the experimenter has to supply an additional numeric feedback signal in theform of a heuristic, which intrinsically guides the agent. Recent work hasshown evidence that in different areas the underlying structure is ordinal andnot numerical. Hence erroneous and biased heuristics are inevitable, especiallyin such domains. In this paper, we propose a MCTS variant which only depends onqualitative feedback, and therefore opens up new applications for MCTS. We alsofind indications that translating absolute into ordinal feedback may bebeneficial. Using a puzzle domain, we show that our preference-based MCTSvariant, wich only receives qualitative feedback, is able to reach aperformance level comparable to a regular MCTS baseline, which obtainsquantitative feedback.\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "edad3cf05bd84744b5a60c1342c47b8e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating outputs:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True Title: Towards an Ontology based integrated Framework for Semantic Web\n",
      "\n",
      "Predicted Title: A ontologies for heterogeneous data sources\n",
      "\n",
      "Abstract:   This Ontologies are widely used as a means for solving the informationheterogeneity problems on the web because of their capability to provideexplicit meaning to the information. They become an efficient tool forknowledge representation in a structured manner. There is always more than oneontology for the same domain. Furthermore, there is no standard method forbuilding ontologies, and there are many ontology building tools using differentontology languages. Because of these reasons, interoperability between theontologies is very low. Current ontology tools mostly use functions to build,edit and inference the ontology. Methods for merging heterogeneous domainontologies are not included in most tools. This paper presents ontology mergingmethodology for building a single global ontology from heterogeneous eXtensibleMarkup Language (XML) data sources to capture and maintain all the knowledgewhich XML data sources can contain\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bc4bf164670c449693861254e61b57e7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating outputs:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True Title: A Modified Vortex Search Algorithm for Numerical Function Optimization\n",
      "\n",
      "Predicted Title: Avortex Search: A Modification of Vortex Search\n",
      "\n",
      "Abstract:   The Vortex Search (VS) algorithm is one of the recently proposedmetaheuristic algorithms which was inspired from the vortical flow of thestirred fluids. Although the VS algorithm is shown to be a good candidate forthe solution of certain optimization problems, it also has some drawbacks. Inthe VS algorithm, candidate solutions are generated around the current bestsolution by using a Gaussian distribution at each iteration pass. This providessimplicity to the algorithm but it also leads to some problems along.Especially, for the functions those have a number of local minimum points, toselect a single point to generate candidate solutions leads the algorithm tobeing trapped into a local minimum point. Due to the adaptive step-sizeadjustment scheme used in the VS algorithm, the locality of the createdcandidate solutions is increased at each iteration pass. Therefore, if thealgorithm cannot escape a local point as quickly as possible, it becomes muchmore difficult for the algorithm to escape from that point in the latteriterations. In this study, a modified Vortex Search algorithm (MVS) is proposedto overcome above mentioned drawback of the existing VS algorithm. In the MVSalgorithm, the candidate solutions are generated around a number of points ateach iteration pass. Computational results showed that with the help of thismodification the global search ability of the existing VS algorithm is improvedand the MVS algorithm outperformed the existing VS algorithm, PSO2011 and ABCalgorithms for the benchmark numerical function set.\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1eec0ee8ef3943f188a35f64203caa28",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating outputs:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True Title: A Hybrid LP-RPG Heuristic for Modelling Numeric Resource Flows in\n",
      "  Planning\n",
      "\n",
      "Predicted Title: A-Based Hybrid Heuristic for numeric planning problems\n",
      "\n",
      "Abstract:   Although the use of metric fluents is fundamental to many practical planningproblems, the study of heuristics to support fully automated planners workingwith these fluents remains relatively unexplored. The most widely usedheuristic is the relaxation of metric fluents into interval-valued variables--- an idea first proposed a decade ago. Other heuristics depend on domainencodings that supply additional information about fluents, such as capacityconstraints or other resource-related annotations. A particular challenge tothese approaches is in handling interactions between metric fluents thatrepresent exchange, such as the transformation of quantities of raw materialsinto quantities of processed goods, or trading of money for materials. Theusual relaxation of metric fluents is often very poor in these situations,since it does not recognise that resources, once spent, are no longer availableto be spent again. We present a heuristic for numeric planning problemsbuilding on the propositional relaxed planning graph, but using a mathematicalprogram for numeric reasoning. We define a class of producer--consumer planningproblems and demonstrate how the numeric constraints in these can be modelledin a mixed integer program (MIP). This MIP is then combined with a metricRelaxed Planning Graph (RPG) heuristic to produce an integrated hybridheuristic. The MIP tracks resource use more accurately than the usualrelaxation, but relaxes the ordering of actions, while the RPG captures thecausal propositional aspects of the problem. We discuss how these twocomponents interact to produce a single unified heuristic and go on to explorehow further numeric features of planning problems can be integrated into theMIP. We show that encoding a limited subset of the propositional problem toaugment the MIP can yield more accurate guidance, partly by exploitingstructure such as propositional landmarks and propositional resources. Ourresults show that the use of this heuristic enhances scalability on problemswhere numeric resource interaction is key in finding a solution.\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "52fef30c92944f74bc9f49dca31615ef",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating outputs:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True Title: A Survey of Multi-Objective Sequential Decision-Making\n",
      "\n",
      "Predicted Title: A Multi-Objective Methods for Planning and Learning\n",
      "\n",
      "Abstract:   Sequential decision-making problems with multiple objectives arise naturallyin practice and pose unique challenges for research in decision-theoreticplanning and learning, which has largely focused on single-objective settings.This article surveys algorithms designed for sequential decision-makingproblems with multiple objectives. Though there is a growing body of literatureon this subject, little of it makes explicit under what circumstances specialmethods are needed to solve multi-objective problems. Therefore, we identifythree distinct scenarios in which converting such a problem to asingle-objective one is impossible, infeasible, or undesirable. Furthermore, wepropose a taxonomy that classifies multi-objective methods according to theapplicable scenario, the nature of the scalarization function (which projectsmulti-objective values to scalar ones), and the type of policies considered. Weshow how these factors determine the nature of an optimal solution, which canbe a single policy, a convex hull, or a Pareto front. Using this taxonomy, wesurvey the literature on multi-objective methods for planning and learning.Finally, we discuss key applications of such methods and outline opportunitiesfor future work.\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "869dd338d613420daf17cff70207af75",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating outputs:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True Title: A Hybrid LP-RPG Heuristic for Modelling Numeric Resource Flows in\n",
      "  Planning\n",
      "\n",
      "Predicted Title: A-Based Hybrid Heuristic for numeric planning problems\n",
      "\n",
      "Abstract:   Although the use of metric fluents is fundamental to many practical planningproblems, the study of heuristics to support fully automated planners workingwith these fluents remains relatively unexplored. The most widely usedheuristic is the relaxation of metric fluents into interval-valued variables--- an idea first proposed a decade ago. Other heuristics depend on domainencodings that supply additional information about fluents, such as capacityconstraints or other resource-related annotations. A particular challenge tothese approaches is in handling interactions between metric fluents thatrepresent exchange, such as the transformation of quantities of raw materialsinto quantities of processed goods, or trading of money for materials. Theusual relaxation of metric fluents is often very poor in these situations,since it does not recognise that resources, once spent, are no longer availableto be spent again. We present a heuristic for numeric planning problemsbuilding on the propositional relaxed planning graph, but using a mathematicalprogram for numeric reasoning. We define a class of producer--consumer planningproblems and demonstrate how the numeric constraints in these can be modelledin a mixed integer program (MIP). This MIP is then combined with a metricRelaxed Planning Graph (RPG) heuristic to produce an integrated hybridheuristic. The MIP tracks resource use more accurately than the usualrelaxation, but relaxes the ordering of actions, while the RPG captures thecausal propositional aspects of the problem. We discuss how these twocomponents interact to produce a single unified heuristic and go on to explorehow further numeric features of planning problems can be integrated into theMIP. We show that encoding a limited subset of the propositional problem toaugment the MIP can yield more accurate guidance, partly by exploitingstructure such as propositional landmarks and propositional resources. Ourresults show that the use of this heuristic enhances scalability on problemswhere numeric resource interaction is key in finding a solution.\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "de75a4a182a84528a632e5cc25a0ad45",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating outputs:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True Title: Adding Context to Concept Trees\n",
      "\n",
      "Predicted Title: A Concept Trees: A Concept Base\n",
      "\n",
      "Abstract:   A Concept Tree is a structure for storing knowledge where the trees arestored in a database called a Concept Base. It sits between the highlydistributed neural architectures and the distributed information systems, withthe intention of bringing brain-like and computer systems closer together.Concept Trees can grow from the semi-structured sources when consistentsequences of concepts are presented. Each tree ideally represents a singlecohesive concept and the trees can link with each other for navigation andsemantic purposes. The trees are therefore also a type of semantic network andwould benefit from having a consistent level of context for each node. Aconsistent build process is managed through a 'counting rule' and some otherrules that can normalise the database structure. This restricted structure canthen be complimented and enriched by the more dynamic context. It is alsosuggested to use the linking structure of the licas system [15] as a basis forthe context links, where the mathematical model is extended further to definethis. A number of tests have demonstrated the soundness of the architecture.Building the trees from text documents shows that the tree structure could beinherent in natural language. Then, two types of query language are described.Both of these can perform consistent query processes to return knowledge to theuser and even enhance the query with new knowledge. This is supported evenfurther with direct comparisons to a cognitive model, also being developed bythe author.\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5c691f0940be4487ad63d93b893f65fb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating outputs:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True Title: Shiva++: An Enhanced Graph based Ontology Matcher\n",
      "\n",
      "Predicted Title: Anology based systems for knowledge management\n",
      "\n",
      "Abstract:   With the web getting bigger and assimilating knowledge about differentconcepts and domains, it is becoming very difficult for simple database drivenapplications to capture the data for a domain. Thus developers have come outwith ontology based systems which can store large amount of information and canapply reasoning and produce timely information. Thus facilitating effectiveknowledge management. Though this approach has made our lives easier, but atthe same time has given rise to another problem. Two different ontologiesassimilating same knowledge tend to use different terms for the same concepts.This creates confusion among knowledge engineers and workers, as they do notknow which is a better term then the other. Thus we need to merge ontologiesworking on same domain so that the engineers can develop a better applicationover it. This paper shows the development of one such matcher which merges theconcepts available in two ontologies at two levels; 1) at string level and 2)at semantic level; thus producing better merged ontologies. We have used agraph matching technique which works at the core of the system. We have alsoevaluated the system and have tested its performance with its predecessor whichworks only on string matching. Thus current approach produces better results.\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "33622740088e415d93e3bcd9fe9f4570",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating outputs:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True Title: Online Guest Detection in a Smart Home using Pervasive Sensors and\n",
      "  Probabilistic Reasoning\n",
      "\n",
      "Predicted Title: Aing the Number of People in a Smart Home at Each Time Step\n",
      "\n",
      "Abstract:   Smart home environments equipped with distributed sensor networks are capableof helping people by providing services related to health, emergency detectionor daily routine management. A backbone to these systems relies often on thesystem's ability to track and detect activities performed by the users in theirhome. Despite the continuous progress in the area of activity recognition insmart homes, many systems make a strong underlying assumption that the numberof occupants in the home at any given moment of time is always known.Estimating the number of persons in a Smart Home at each time step remains achallenge nowadays. Indeed, unlike most (crowd) counting solution which arebased on computer vision techniques, the sensors considered in a Smart Home areoften very simple and do not offer individually a good overview of thesituation. The data gathered needs therefore to be fused in order to inferuseful information. This paper aims at addressing this challenge and presents aprobabilistic approach able to estimate the number of persons in theenvironment at each time step. This approach works in two steps: first, anestimate of the number of persons present in the environment is done using aConstraint Satisfaction Problem solver, based on the topology of the sensornetwork and the sensor activation pattern at this time point. Then, a HiddenMarkov Model refines this estimate by considering the uncertainty related tothe sensors. Using both simulated and real data, our method has been tested andvalidated on two smart homes of different sizes and configuration anddemonstrates the ability to accurately estimate the number of inhabitants.\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6d638d791d2f4518bb63ed29e29ad6ed",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating outputs:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True Title: The emergence of Explainability of Intelligent Systems: Delivering\n",
      "  Explainable and Personalised Recommendations for Energy Efficiency\n",
      "\n",
      "Predicted Title: Aation Systems for Energy Efficiency\n",
      "\n",
      "Abstract:   The recent advances in artificial intelligence namely in machine learning anddeep learning, have boosted the performance of intelligent systems in severalways. This gave rise to human expectations, but also created the need for adeeper understanding of how intelligent systems think and decide. The conceptof explainability appeared, in the extent of explaining the internal systemmechanics in human terms. Recommendation systems are intelligent systems thatsupport human decision making, and as such, they have to be explainable inorder to increase user trust and improve the acceptance of recommendations. Inthis work, we focus on a context-aware recommendation system for energyefficiency and develop a mechanism for explainable and persuasiverecommendations, which are personalized to user preferences and habits. Thepersuasive facts either emphasize on the economical saving prospects (Econ) oron a positive ecological impact (Eco) and explanations provide the reason forrecommending an energy saving action. Based on a study conducted using aTelegram bot, different scenarios have been validated with actual data andhuman feedback. Current results show a total increase of 19\\% on therecommendation acceptance ratio when both economical and ecological persuasivefacts are employed. This revolutionary approach on recommendation systems,demonstrates how intelligent recommendations can effectively encourage energysaving behavior.\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f425309bca564f39845e0206b7fec260",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating outputs:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True Title: Representing and Reasoning with Qualitative Preferences for\n",
      "  Compositional Systems\n",
      "\n",
      "Predicted Title: A dominance relation for the selection of most preferred collections\n",
      "\n",
      "Abstract:   Many applications, e.g., Web service composition, complex system design, teamformation, etc., rely on methods for identifying collections of objects orentities satisfying some functional requirement. Among the collections thatsatisfy the functional requirement, it is often necessary to identify one ormore collections that are optimal with respect to user preferences over a setof attributes that describe the non-functional properties of the collection.  We develop a formalism that lets users express the relative importance amongattributes and qualitative preferences over the valuations of each attribute.We define a dominance relation that allows us to compare collections of objectsin terms of preferences over attributes of the objects that make up thecollection. We establish some key properties of the dominance relation. Inparticular, we show that the dominance relation is a strict partial order whenthe intra-attribute preference relations are strict partial orders and therelative importance preference relation is an interval order.  We provide algorithms that use this dominance relation to identify the set ofmost preferred collections. We show that under certain conditions, thealgorithms are guaranteed to return only (sound), all (complete), or at leastone (weakly complete) of the most preferred collections. We present results ofsimulation experiments comparing the proposed algorithms with respect to (a)the quality of solutions (number of most preferred solutions) produced by thealgorithms, and (b) their performance and efficiency. We also explore someinteresting conjectures suggested by the results of our experiments that relatethe properties of the user preferences, the dominance relation, and thealgorithms.\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "248865fe499646eba8fa52f528cc4292",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating outputs:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True Title: A Study of Student Learning Skills Using Fuzzy Relation Equations\n",
      "\n",
      "Predicted Title: A Fuzzy relation equations (FRE)\n",
      "\n",
      "Abstract:   Fuzzy relation equations (FRE)are associated with the composition of binaryfuzzy relations. In the present work FRE are used as a tool for studying theprocess of learning a new subject matter by a student class. A classroomapplication and other csuitable examples connected to the student learning ofthe derivative are also presented illustrating our results and usefulconclusions are obtained.\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "34ded23c984348f6806d9f25ab0e68db",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating outputs:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True Title: Preference-Based Monte Carlo Tree Search\n",
      "\n",
      "Predicted Title: A Monte Carlo Tree Search: A Unique and Unique Approach\n",
      "\n",
      "Abstract:   Monte Carlo tree search (MCTS) is a popular choice for solving sequentialanytime problems. However, it depends on a numeric feedback signal, which canbe difficult to define. Real-time MCTS is a variant which may only rarelyencounter states with an explicit, extrinsic reward. To deal with such cases,the experimenter has to supply an additional numeric feedback signal in theform of a heuristic, which intrinsically guides the agent. Recent work hasshown evidence that in different areas the underlying structure is ordinal andnot numerical. Hence erroneous and biased heuristics are inevitable, especiallyin such domains. In this paper, we propose a MCTS variant which only depends onqualitative feedback, and therefore opens up new applications for MCTS. We alsofind indications that translating absolute into ordinal feedback may bebeneficial. Using a puzzle domain, we show that our preference-based MCTSvariant, wich only receives qualitative feedback, is able to reach aperformance level comparable to a regular MCTS baseline, which obtainsquantitative feedback.\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "486a756bfa7b46febfaf11f31cc11305",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating outputs:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True Title: Modeling Variations of First-Order Horn Abduction in Answer Set\n",
      "  Programming\n",
      "\n",
      "Predicted Title: A Answer Set Programming: An Approach to Abduction in First Order Horn Logic\n",
      "\n",
      "Abstract:   We study abduction in First Order Horn logic theories where all atoms can beabduced and we are looking for preferred solutions with respect to threeobjective functions: cardinality minimality, coherence, and weighted abduction.We represent this reasoning problem in Answer Set Programming (ASP), in orderto obtain a flexible framework for experimenting with global constraints andobjective functions, and to test the boundaries of what is possible with ASP.Realizing this problem in ASP is challenging as it requires value invention andequivalence between certain constants, because the Unique Names Assumption doesnot hold in general. To permit reasoning in cyclic theories, we formallydescribe fine-grained variations of limiting Skolemization. We identify termequivalence as a main instantiation bottleneck, and improve the efficiency ofour approach with on-demand constraints that were used to eliminate the samebottleneck in state-of-the-art solvers. We evaluate our approach experimentallyon the ACCEL benchmark for plan recognition in Natural Language Understanding.Our encodings are publicly available, modular, and our approach is moreefficient than state-of-the-art solvers on the ACCEL benchmark.\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "53ce94842a7a4226bda601e1bd2e6ca3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating outputs:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True Title: A Modified Vortex Search Algorithm for Numerical Function Optimization\n",
      "\n",
      "Predicted Title: Avortex Search: A Modification of Vortex Search\n",
      "\n",
      "Abstract:   The Vortex Search (VS) algorithm is one of the recently proposedmetaheuristic algorithms which was inspired from the vortical flow of thestirred fluids. Although the VS algorithm is shown to be a good candidate forthe solution of certain optimization problems, it also has some drawbacks. Inthe VS algorithm, candidate solutions are generated around the current bestsolution by using a Gaussian distribution at each iteration pass. This providessimplicity to the algorithm but it also leads to some problems along.Especially, for the functions those have a number of local minimum points, toselect a single point to generate candidate solutions leads the algorithm tobeing trapped into a local minimum point. Due to the adaptive step-sizeadjustment scheme used in the VS algorithm, the locality of the createdcandidate solutions is increased at each iteration pass. Therefore, if thealgorithm cannot escape a local point as quickly as possible, it becomes muchmore difficult for the algorithm to escape from that point in the latteriterations. In this study, a modified Vortex Search algorithm (MVS) is proposedto overcome above mentioned drawback of the existing VS algorithm. In the MVSalgorithm, the candidate solutions are generated around a number of points ateach iteration pass. Computational results showed that with the help of thismodification the global search ability of the existing VS algorithm is improvedand the MVS algorithm outperformed the existing VS algorithm, PSO2011 and ABCalgorithms for the benchmark numerical function set.\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d60200f88d3a44d39820de65c280e241",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating outputs:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True Title: A Study of Student Learning Skills Using Fuzzy Relation Equations\n",
      "\n",
      "Predicted Title: A Fuzzy relation equations (FRE)\n",
      "\n",
      "Abstract:   Fuzzy relation equations (FRE)are associated with the composition of binaryfuzzy relations. In the present work FRE are used as a tool for studying theprocess of learning a new subject matter by a student class. A classroomapplication and other csuitable examples connected to the student learning ofthe derivative are also presented illustrating our results and usefulconclusions are obtained.\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "394706c4934c4b9aa9617334c3de7c2d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating outputs:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True Title: Learning STRIPS Action Models with Classical Planning\n",
      "\n",
      "Predicted Title: Acompiling STRIPS action models from a classical planning task\n",
      "\n",
      "Abstract:   This paper presents a novel approach for learning STRIPS action models fromexamples that compiles this inductive learning task into a classical planningtask. Interestingly, the compilation approach is flexible to different amountsof available input knowledge; the learning examples can range from a set ofplans (with their corresponding initial and final states) to just a pair ofinitial and final states (no intermediate action or state is given). Moreover,the compilation accepts partially specified action models and it can be used tovalidate whether the observation of a plan execution follows a given STRIPSaction model, even if this model is not fully specified.\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "843a2c5c89db46d2a738013e0d2e06c7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating outputs:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True Title: A Survey of Multi-Objective Sequential Decision-Making\n",
      "\n",
      "Predicted Title: A Multi-Objective Methods for Planning and Learning\n",
      "\n",
      "Abstract:   Sequential decision-making problems with multiple objectives arise naturallyin practice and pose unique challenges for research in decision-theoreticplanning and learning, which has largely focused on single-objective settings.This article surveys algorithms designed for sequential decision-makingproblems with multiple objectives. Though there is a growing body of literatureon this subject, little of it makes explicit under what circumstances specialmethods are needed to solve multi-objective problems. Therefore, we identifythree distinct scenarios in which converting such a problem to asingle-objective one is impossible, infeasible, or undesirable. Furthermore, wepropose a taxonomy that classifies multi-objective methods according to theapplicable scenario, the nature of the scalarization function (which projectsmulti-objective values to scalar ones), and the type of policies considered. Weshow how these factors determine the nature of an optimal solution, which canbe a single policy, a convex hull, or a Pareto front. Using this taxonomy, wesurvey the literature on multi-objective methods for planning and learning.Finally, we discuss key applications of such methods and outline opportunitiesfor future work.\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e861e998b7f14141852c4c83f30ef67e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating outputs:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True Title: A Comparative Study on Parameter Estimation in Software Reliability\n",
      "  Modeling using Swarm Intelligence\n",
      "\n",
      "Predicted Title: Anfficient Swarm Optimization and Firefly Algorithm\n",
      "\n",
      "Abstract:   This work focuses on a comparison between the performances of two well-knownSwarm algorithms: Cuckoo Search (CS) and Firefly Algorithm (FA), in estimatingthe parameters of Software Reliability Growth Models. This study is furtherreinforced using Particle Swarm Optimization (PSO) and Ant Colony Optimization(ACO). All algorithms are evaluated according to real software failure data,the tests are performed and the obtained results are compared to show theperformance of each of the used algorithms. Furthermore, CS and FA are alsocompared with each other on bases of execution time and iteration number.Experimental results show that CS is more efficient in estimating theparameters of SRGMs, and it has outperformed FA in addition to PSO and ACO forthe selected Data sets and employed models.\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "add512b23f204c3490c3e52dd531a573",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating outputs:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True Title: Configurable Markov Decision Processes\n",
      "\n",
      "Predicted Title: AConfigurableMarkov Decision Processes\n",
      "\n",
      "Abstract:   In many real-world problems, there is the possibility to configure, to alimited extent, some environmental parameters to improve the performance of alearning agent. In this paper, we propose a novel framework, ConfigurableMarkov Decision Processes (Conf-MDPs), to model this new type of interactionwith the environment. Furthermore, we provide a new learning algorithm, SafePolicy-Model Iteration (SPMI), to jointly and adaptively optimize the policyand the environment configuration. After having introduced our approach andderived some theoretical results, we present the experimental evaluation in twoexplicative problems to show the benefits of the environment configurability onthe performance of the learned policy.\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9d5e54a2710f491c927a26de9d526863",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating outputs:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True Title: The emergence of Explainability of Intelligent Systems: Delivering\n",
      "  Explainable and Personalised Recommendations for Energy Efficiency\n",
      "\n",
      "Predicted Title: Aation Systems for Energy Efficiency\n",
      "\n",
      "Abstract:   The recent advances in artificial intelligence namely in machine learning anddeep learning, have boosted the performance of intelligent systems in severalways. This gave rise to human expectations, but also created the need for adeeper understanding of how intelligent systems think and decide. The conceptof explainability appeared, in the extent of explaining the internal systemmechanics in human terms. Recommendation systems are intelligent systems thatsupport human decision making, and as such, they have to be explainable inorder to increase user trust and improve the acceptance of recommendations. Inthis work, we focus on a context-aware recommendation system for energyefficiency and develop a mechanism for explainable and persuasiverecommendations, which are personalized to user preferences and habits. Thepersuasive facts either emphasize on the economical saving prospects (Econ) oron a positive ecological impact (Eco) and explanations provide the reason forrecommending an energy saving action. Based on a study conducted using aTelegram bot, different scenarios have been validated with actual data andhuman feedback. Current results show a total increase of 19\\% on therecommendation acceptance ratio when both economical and ecological persuasivefacts are employed. This revolutionary approach on recommendation systems,demonstrates how intelligent recommendations can effectively encourage energysaving behavior.\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b0ce517173fe4da8b170e32eebedfa36",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating outputs:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True Title: Contextual Symmetries in Probabilistic Graphical Models\n",
      "\n",
      "Predicted Title: A Contextual symmetries\n",
      "\n",
      "Abstract:   An important approach for efficient inference in probabilistic graphicalmodels exploits symmetries among objects in the domain. Symmetric variables(states) are collapsed into meta-variables (meta-states) and inferencealgorithms are run over the lifted graphical model instead of the flat one. Ourpaper extends existing definitions of symmetry by introducing the novel notionof contextual symmetry. Two states that are not globally symmetric, can becontextually symmetric under some specific assignment to a subset of variables,referred to as the context variables. Contextual symmetry subsumes previoussymmetry definitions and can rep resent a large class of symmetries notrepresentable earlier. We show how to compute contextual symmetries by reducingit to the problem of graph isomorphism. We extend previous work on exploitingsymmetries in the MCMC framework to the case of contextual symmetries. Ourexperiments on several domains of interest demonstrate that exploitingcontextual symmetries can result in significant computational gains.\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e1f8c3cd10374f3db2fab3a30632ab9d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating outputs:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True Title: Autonomous Self-Explanation of Behavior for Interactive Reinforcement\n",
      "  Learning Agents\n",
      "\n",
      "Predicted Title: A Instruction-Based Behavior Explanation\n",
      "\n",
      "Abstract:   In cooperation, the workers must know how co-workers behave. However, anagent's policy, which is embedded in a statistical machine learning model, ishard to understand, and requires much time and knowledge to comprehend.Therefore, it is difficult for people to predict the behavior of machinelearning robots, which makes Human Robot Cooperation challenging. In thispaper, we propose Instruction-based Behavior Explanation (IBE), a method toexplain an autonomous agent's future behavior. In IBE, an agent canautonomously acquire the expressions to explain its own behavior by reusing theinstructions given by a human expert to accelerate the learning of the agent'spolicy. IBE also enables a developmental agent, whose policy may change duringthe cooperation, to explain its own behavior with sufficient time granularity.\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8e502db51c2146c290ef35f1b6e0eb6a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating outputs:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True Title: The SP theory of intelligence and the representation and processing of\n",
      "  knowledge in the brain\n",
      "\n",
      "Predicted Title: A SP theory of intelligence: An overview and partial model\n",
      "\n",
      "Abstract:   The \"SP theory of intelligence\", with its realisation in the \"SP computermodel\", aims to simplify and integrate observations and concepts acrossAI-related fields, with information compression as a unifying theme. This paperdescribes how abstract structures and processes in the theory may be realisedin terms of neurons, their interconnections, and the transmission of signalsbetween neurons. This part of the SP theory -- \"SP-neural\" -- is a tentativeand partial model for the representation and processing of knowledge in thebrain. In the SP theory (apart from SP-neural), all kinds of knowledge arerepresented with \"patterns\", where a pattern is an array of atomic symbols inone or two dimensions. In SP-neural, the concept of a \"pattern\" is realised asan array of neurons called a \"pattern assembly\", similar to Hebb's concept of a\"cell assembly\" but with important differences. Central to the processing ofinformation in the SP system is the powerful concept of \"multiple alignment\",borrowed and adapted from bioinformatics. Processes such as patternrecognition, reasoning and problem solving are achieved via the building ofmultiple alignments, while unsupervised learning -- significantly differentfrom the \"Hebbian\" kinds of learning -- is achieved by creating patterns fromsensory information and also by creating patterns from multiple alignments inwhich there is a partial match between one pattern and another. Short-livedneural structures equivalent to multiple alignments will be created via aninter-play of excitatory and inhibitory neural signals. The paper discussesseveral associated issues, with relevant empirical evidence.\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4f8235c095f1493b90f5dc9d2e4f8c84",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating outputs:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True Title: A Hybrid LP-RPG Heuristic for Modelling Numeric Resource Flows in\n",
      "  Planning\n",
      "\n",
      "Predicted Title: A-Based Hybrid Heuristic for numeric planning problems\n",
      "\n",
      "Abstract:   Although the use of metric fluents is fundamental to many practical planningproblems, the study of heuristics to support fully automated planners workingwith these fluents remains relatively unexplored. The most widely usedheuristic is the relaxation of metric fluents into interval-valued variables--- an idea first proposed a decade ago. Other heuristics depend on domainencodings that supply additional information about fluents, such as capacityconstraints or other resource-related annotations. A particular challenge tothese approaches is in handling interactions between metric fluents thatrepresent exchange, such as the transformation of quantities of raw materialsinto quantities of processed goods, or trading of money for materials. Theusual relaxation of metric fluents is often very poor in these situations,since it does not recognise that resources, once spent, are no longer availableto be spent again. We present a heuristic for numeric planning problemsbuilding on the propositional relaxed planning graph, but using a mathematicalprogram for numeric reasoning. We define a class of producer--consumer planningproblems and demonstrate how the numeric constraints in these can be modelledin a mixed integer program (MIP). This MIP is then combined with a metricRelaxed Planning Graph (RPG) heuristic to produce an integrated hybridheuristic. The MIP tracks resource use more accurately than the usualrelaxation, but relaxes the ordering of actions, while the RPG captures thecausal propositional aspects of the problem. We discuss how these twocomponents interact to produce a single unified heuristic and go on to explorehow further numeric features of planning problems can be integrated into theMIP. We show that encoding a limited subset of the propositional problem toaugment the MIP can yield more accurate guidance, partly by exploitingstructure such as propositional landmarks and propositional resources. Ourresults show that the use of this heuristic enhances scalability on problemswhere numeric resource interaction is key in finding a solution.\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "263c62d877a445349645942a901d8f30",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating outputs:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True Title: Adding Context to Concept Trees\n",
      "\n",
      "Predicted Title: A Concept Trees: A Concept Base\n",
      "\n",
      "Abstract:   A Concept Tree is a structure for storing knowledge where the trees arestored in a database called a Concept Base. It sits between the highlydistributed neural architectures and the distributed information systems, withthe intention of bringing brain-like and computer systems closer together.Concept Trees can grow from the semi-structured sources when consistentsequences of concepts are presented. Each tree ideally represents a singlecohesive concept and the trees can link with each other for navigation andsemantic purposes. The trees are therefore also a type of semantic network andwould benefit from having a consistent level of context for each node. Aconsistent build process is managed through a 'counting rule' and some otherrules that can normalise the database structure. This restricted structure canthen be complimented and enriched by the more dynamic context. It is alsosuggested to use the linking structure of the licas system [15] as a basis forthe context links, where the mathematical model is extended further to definethis. A number of tests have demonstrated the soundness of the architecture.Building the trees from text documents shows that the tree structure could beinherent in natural language. Then, two types of query language are described.Both of these can perform consistent query processes to return knowledge to theuser and even enhance the query with new knowledge. This is supported evenfurther with direct comparisons to a cognitive model, also being developed bythe author.\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "63cd6bc66e1c44e49eed91cbc4fcdf02",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating outputs:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True Title: Representing and Reasoning with Qualitative Preferences for\n",
      "  Compositional Systems\n",
      "\n",
      "Predicted Title: A dominance relation for the selection of most preferred collections\n",
      "\n",
      "Abstract:   Many applications, e.g., Web service composition, complex system design, teamformation, etc., rely on methods for identifying collections of objects orentities satisfying some functional requirement. Among the collections thatsatisfy the functional requirement, it is often necessary to identify one ormore collections that are optimal with respect to user preferences over a setof attributes that describe the non-functional properties of the collection.  We develop a formalism that lets users express the relative importance amongattributes and qualitative preferences over the valuations of each attribute.We define a dominance relation that allows us to compare collections of objectsin terms of preferences over attributes of the objects that make up thecollection. We establish some key properties of the dominance relation. Inparticular, we show that the dominance relation is a strict partial order whenthe intra-attribute preference relations are strict partial orders and therelative importance preference relation is an interval order.  We provide algorithms that use this dominance relation to identify the set ofmost preferred collections. We show that under certain conditions, thealgorithms are guaranteed to return only (sound), all (complete), or at leastone (weakly complete) of the most preferred collections. We present results ofsimulation experiments comparing the proposed algorithms with respect to (a)the quality of solutions (number of most preferred solutions) produced by thealgorithms, and (b) their performance and efficiency. We also explore someinteresting conjectures suggested by the results of our experiments that relatethe properties of the user preferences, the dominance relation, and thealgorithms.\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aaec0ba3cb75491fa009aea08292d3d3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating outputs:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True Title: A Hybrid LP-RPG Heuristic for Modelling Numeric Resource Flows in\n",
      "  Planning\n",
      "\n",
      "Predicted Title: A-Based Hybrid Heuristic for numeric planning problems\n",
      "\n",
      "Abstract:   Although the use of metric fluents is fundamental to many practical planningproblems, the study of heuristics to support fully automated planners workingwith these fluents remains relatively unexplored. The most widely usedheuristic is the relaxation of metric fluents into interval-valued variables--- an idea first proposed a decade ago. Other heuristics depend on domainencodings that supply additional information about fluents, such as capacityconstraints or other resource-related annotations. A particular challenge tothese approaches is in handling interactions between metric fluents thatrepresent exchange, such as the transformation of quantities of raw materialsinto quantities of processed goods, or trading of money for materials. Theusual relaxation of metric fluents is often very poor in these situations,since it does not recognise that resources, once spent, are no longer availableto be spent again. We present a heuristic for numeric planning problemsbuilding on the propositional relaxed planning graph, but using a mathematicalprogram for numeric reasoning. We define a class of producer--consumer planningproblems and demonstrate how the numeric constraints in these can be modelledin a mixed integer program (MIP). This MIP is then combined with a metricRelaxed Planning Graph (RPG) heuristic to produce an integrated hybridheuristic. The MIP tracks resource use more accurately than the usualrelaxation, but relaxes the ordering of actions, while the RPG captures thecausal propositional aspects of the problem. We discuss how these twocomponents interact to produce a single unified heuristic and go on to explorehow further numeric features of planning problems can be integrated into theMIP. We show that encoding a limited subset of the propositional problem toaugment the MIP can yield more accurate guidance, partly by exploitingstructure such as propositional landmarks and propositional resources. Ourresults show that the use of this heuristic enhances scalability on problemswhere numeric resource interaction is key in finding a solution.\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "734a916924e2443d8a36658deda113e4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating outputs:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True Title: SAPFOCS: a metaheuristic based approach to part family formation\n",
      "  problems in group technology\n",
      "\n",
      "Predicted Title: A Part Family Formation in GroupTechnology\n",
      "\n",
      "Abstract:   This article deals with Part family formation problem which is believed to bemoderately complicated to be solved in polynomial time in the vicinity of GroupTechnology (GT). In the past literature researchers investigated that the partfamily formation techniques are principally based on production flow analysis(PFA) which usually considers operational requirements, sequences and time.Part Coding Analysis (PCA) is merely considered in GT which is believed to bethe proficient method to identify the part families. PCA classifies parts byallotting them to different families based on their resemblances in: (1) designcharacteristics such as shape and size, and/or (2) manufacturingcharacteristics (machining requirements). A novel approach based on simulatedannealing namely SAPFOCS is adopted in this study to develop effective partfamilies exploiting the PCA technique. Thereafter Taguchi's orthogonal designmethod is employed to solve the critical issues on the subject of parametersselection for the proposed metaheuristic algorithm. The adopted technique istherefore tested on 5 different datasets of size 5 {\\times} 9 to 27 {\\times} 9and the obtained results are compared with C-Linkage clustering technique. Theexperimental results reported that the proposed metaheuristic algorithm isextremely effective in terms of the quality of the solution obtained and hasoutperformed C-Linkage algorithm in most instances.\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "37f5639c498a4666ae364e145dec102d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating outputs:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True Title: Modeling Variations of First-Order Horn Abduction in Answer Set\n",
      "  Programming\n",
      "\n",
      "Predicted Title: A Answer Set Programming: An Approach to Abduction in First Order Horn Logic\n",
      "\n",
      "Abstract:   We study abduction in First Order Horn logic theories where all atoms can beabduced and we are looking for preferred solutions with respect to threeobjective functions: cardinality minimality, coherence, and weighted abduction.We represent this reasoning problem in Answer Set Programming (ASP), in orderto obtain a flexible framework for experimenting with global constraints andobjective functions, and to test the boundaries of what is possible with ASP.Realizing this problem in ASP is challenging as it requires value invention andequivalence between certain constants, because the Unique Names Assumption doesnot hold in general. To permit reasoning in cyclic theories, we formallydescribe fine-grained variations of limiting Skolemization. We identify termequivalence as a main instantiation bottleneck, and improve the efficiency ofour approach with on-demand constraints that were used to eliminate the samebottleneck in state-of-the-art solvers. We evaluate our approach experimentallyon the ACCEL benchmark for plan recognition in Natural Language Understanding.Our encodings are publicly available, modular, and our approach is moreefficient than state-of-the-art solvers on the ACCEL benchmark.\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9448f40f24cd4838baf670d42ad18b10",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating outputs:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True Title: A Modified Vortex Search Algorithm for Numerical Function Optimization\n",
      "\n",
      "Predicted Title: Avortex Search: A Modification of Vortex Search\n",
      "\n",
      "Abstract:   The Vortex Search (VS) algorithm is one of the recently proposedmetaheuristic algorithms which was inspired from the vortical flow of thestirred fluids. Although the VS algorithm is shown to be a good candidate forthe solution of certain optimization problems, it also has some drawbacks. Inthe VS algorithm, candidate solutions are generated around the current bestsolution by using a Gaussian distribution at each iteration pass. This providessimplicity to the algorithm but it also leads to some problems along.Especially, for the functions those have a number of local minimum points, toselect a single point to generate candidate solutions leads the algorithm tobeing trapped into a local minimum point. Due to the adaptive step-sizeadjustment scheme used in the VS algorithm, the locality of the createdcandidate solutions is increased at each iteration pass. Therefore, if thealgorithm cannot escape a local point as quickly as possible, it becomes muchmore difficult for the algorithm to escape from that point in the latteriterations. In this study, a modified Vortex Search algorithm (MVS) is proposedto overcome above mentioned drawback of the existing VS algorithm. In the MVSalgorithm, the candidate solutions are generated around a number of points ateach iteration pass. Computational results showed that with the help of thismodification the global search ability of the existing VS algorithm is improvedand the MVS algorithm outperformed the existing VS algorithm, PSO2011 and ABCalgorithms for the benchmark numerical function set.\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e90fafb9c0064207bbfa7aefc73e4560",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating outputs:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True Title: A Survey of Multi-Objective Sequential Decision-Making\n",
      "\n",
      "Predicted Title: A Multi-Objective Methods for Planning and Learning\n",
      "\n",
      "Abstract:   Sequential decision-making problems with multiple objectives arise naturallyin practice and pose unique challenges for research in decision-theoreticplanning and learning, which has largely focused on single-objective settings.This article surveys algorithms designed for sequential decision-makingproblems with multiple objectives. Though there is a growing body of literatureon this subject, little of it makes explicit under what circumstances specialmethods are needed to solve multi-objective problems. Therefore, we identifythree distinct scenarios in which converting such a problem to asingle-objective one is impossible, infeasible, or undesirable. Furthermore, wepropose a taxonomy that classifies multi-objective methods according to theapplicable scenario, the nature of the scalarization function (which projectsmulti-objective values to scalar ones), and the type of policies considered. Weshow how these factors determine the nature of an optimal solution, which canbe a single policy, a convex hull, or a Pareto front. Using this taxonomy, wesurvey the literature on multi-objective methods for planning and learning.Finally, we discuss key applications of such methods and outline opportunitiesfor future work.\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1099e0e643314154a21587cd70340361",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating outputs:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True Title: A Survey of Multi-Objective Sequential Decision-Making\n",
      "\n",
      "Predicted Title: A Multi-Objective Methods for Planning and Learning\n",
      "\n",
      "Abstract:   Sequential decision-making problems with multiple objectives arise naturallyin practice and pose unique challenges for research in decision-theoreticplanning and learning, which has largely focused on single-objective settings.This article surveys algorithms designed for sequential decision-makingproblems with multiple objectives. Though there is a growing body of literatureon this subject, little of it makes explicit under what circumstances specialmethods are needed to solve multi-objective problems. Therefore, we identifythree distinct scenarios in which converting such a problem to asingle-objective one is impossible, infeasible, or undesirable. Furthermore, wepropose a taxonomy that classifies multi-objective methods according to theapplicable scenario, the nature of the scalarization function (which projectsmulti-objective values to scalar ones), and the type of policies considered. Weshow how these factors determine the nature of an optimal solution, which canbe a single policy, a convex hull, or a Pareto front. Using this taxonomy, wesurvey the literature on multi-objective methods for planning and learning.Finally, we discuss key applications of such methods and outline opportunitiesfor future work.\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e322848b41df4536ba271cf3cf985816",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating outputs:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True Title: Cortex simulation system proposal using distributed computer network\n",
      "  environments\n",
      "\n",
      "Predicted Title: A neuroscience: A New Approach\n",
      "\n",
      "Abstract:   In the dawn of computer science and the eve of neuroscience we participate inrebirth of neuroscience due to new technology that allows us to deeply andprecisely explore whole new world that dwells in our brains.\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a7857c188325459f9cbb4aa15a78fcc5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating outputs:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True Title: Representing and Reasoning with Qualitative Preferences for\n",
      "  Compositional Systems\n",
      "\n",
      "Predicted Title: A dominance relation for the selection of most preferred collections\n",
      "\n",
      "Abstract:   Many applications, e.g., Web service composition, complex system design, teamformation, etc., rely on methods for identifying collections of objects orentities satisfying some functional requirement. Among the collections thatsatisfy the functional requirement, it is often necessary to identify one ormore collections that are optimal with respect to user preferences over a setof attributes that describe the non-functional properties of the collection.  We develop a formalism that lets users express the relative importance amongattributes and qualitative preferences over the valuations of each attribute.We define a dominance relation that allows us to compare collections of objectsin terms of preferences over attributes of the objects that make up thecollection. We establish some key properties of the dominance relation. Inparticular, we show that the dominance relation is a strict partial order whenthe intra-attribute preference relations are strict partial orders and therelative importance preference relation is an interval order.  We provide algorithms that use this dominance relation to identify the set ofmost preferred collections. We show that under certain conditions, thealgorithms are guaranteed to return only (sound), all (complete), or at leastone (weakly complete) of the most preferred collections. We present results ofsimulation experiments comparing the proposed algorithms with respect to (a)the quality of solutions (number of most preferred solutions) produced by thealgorithms, and (b) their performance and efficiency. We also explore someinteresting conjectures suggested by the results of our experiments that relatethe properties of the user preferences, the dominance relation, and thealgorithms.\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eaa78bd5d8e34edda030cb68e977dc54",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating outputs:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True Title: A Modified Vortex Search Algorithm for Numerical Function Optimization\n",
      "\n",
      "Predicted Title: Avortex Search: A Modification of Vortex Search\n",
      "\n",
      "Abstract:   The Vortex Search (VS) algorithm is one of the recently proposedmetaheuristic algorithms which was inspired from the vortical flow of thestirred fluids. Although the VS algorithm is shown to be a good candidate forthe solution of certain optimization problems, it also has some drawbacks. Inthe VS algorithm, candidate solutions are generated around the current bestsolution by using a Gaussian distribution at each iteration pass. This providessimplicity to the algorithm but it also leads to some problems along.Especially, for the functions those have a number of local minimum points, toselect a single point to generate candidate solutions leads the algorithm tobeing trapped into a local minimum point. Due to the adaptive step-sizeadjustment scheme used in the VS algorithm, the locality of the createdcandidate solutions is increased at each iteration pass. Therefore, if thealgorithm cannot escape a local point as quickly as possible, it becomes muchmore difficult for the algorithm to escape from that point in the latteriterations. In this study, a modified Vortex Search algorithm (MVS) is proposedto overcome above mentioned drawback of the existing VS algorithm. In the MVSalgorithm, the candidate solutions are generated around a number of points ateach iteration pass. Computational results showed that with the help of thismodification the global search ability of the existing VS algorithm is improvedand the MVS algorithm outperformed the existing VS algorithm, PSO2011 and ABCalgorithms for the benchmark numerical function set.\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "61a4b03c7eed4fb39cc6025f96e7b5a8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating outputs:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True Title: A Comparative Study on Parameter Estimation in Software Reliability\n",
      "  Modeling using Swarm Intelligence\n",
      "\n",
      "Predicted Title: Anfficient Swarm Optimization and Firefly Algorithm\n",
      "\n",
      "Abstract:   This work focuses on a comparison between the performances of two well-knownSwarm algorithms: Cuckoo Search (CS) and Firefly Algorithm (FA), in estimatingthe parameters of Software Reliability Growth Models. This study is furtherreinforced using Particle Swarm Optimization (PSO) and Ant Colony Optimization(ACO). All algorithms are evaluated according to real software failure data,the tests are performed and the obtained results are compared to show theperformance of each of the used algorithms. Furthermore, CS and FA are alsocompared with each other on bases of execution time and iteration number.Experimental results show that CS is more efficient in estimating theparameters of SRGMs, and it has outperformed FA in addition to PSO and ACO forthe selected Data sets and employed models.\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bcac1cdd44a5411fad22d9c3b067a831",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating outputs:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True Title: Autonomous Self-Explanation of Behavior for Interactive Reinforcement\n",
      "  Learning Agents\n",
      "\n",
      "Predicted Title: A Instruction-Based Behavior Explanation\n",
      "\n",
      "Abstract:   In cooperation, the workers must know how co-workers behave. However, anagent's policy, which is embedded in a statistical machine learning model, ishard to understand, and requires much time and knowledge to comprehend.Therefore, it is difficult for people to predict the behavior of machinelearning robots, which makes Human Robot Cooperation challenging. In thispaper, we propose Instruction-based Behavior Explanation (IBE), a method toexplain an autonomous agent's future behavior. In IBE, an agent canautonomously acquire the expressions to explain its own behavior by reusing theinstructions given by a human expert to accelerate the learning of the agent'spolicy. IBE also enables a developmental agent, whose policy may change duringthe cooperation, to explain its own behavior with sufficient time granularity.\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "443152b6913c474c8363789e48ffbcc5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating outputs:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True Title: A Study of Student Learning Skills Using Fuzzy Relation Equations\n",
      "\n",
      "Predicted Title: A Fuzzy relation equations (FRE)\n",
      "\n",
      "Abstract:   Fuzzy relation equations (FRE)are associated with the composition of binaryfuzzy relations. In the present work FRE are used as a tool for studying theprocess of learning a new subject matter by a student class. A classroomapplication and other csuitable examples connected to the student learning ofthe derivative are also presented illustrating our results and usefulconclusions are obtained.\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e4e8cb5296aa49c8bda8b6ce42623f04",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating outputs:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True Title: A Hybrid LP-RPG Heuristic for Modelling Numeric Resource Flows in\n",
      "  Planning\n",
      "\n",
      "Predicted Title: A-Based Hybrid Heuristic for numeric planning problems\n",
      "\n",
      "Abstract:   Although the use of metric fluents is fundamental to many practical planningproblems, the study of heuristics to support fully automated planners workingwith these fluents remains relatively unexplored. The most widely usedheuristic is the relaxation of metric fluents into interval-valued variables--- an idea first proposed a decade ago. Other heuristics depend on domainencodings that supply additional information about fluents, such as capacityconstraints or other resource-related annotations. A particular challenge tothese approaches is in handling interactions between metric fluents thatrepresent exchange, such as the transformation of quantities of raw materialsinto quantities of processed goods, or trading of money for materials. Theusual relaxation of metric fluents is often very poor in these situations,since it does not recognise that resources, once spent, are no longer availableto be spent again. We present a heuristic for numeric planning problemsbuilding on the propositional relaxed planning graph, but using a mathematicalprogram for numeric reasoning. We define a class of producer--consumer planningproblems and demonstrate how the numeric constraints in these can be modelledin a mixed integer program (MIP). This MIP is then combined with a metricRelaxed Planning Graph (RPG) heuristic to produce an integrated hybridheuristic. The MIP tracks resource use more accurately than the usualrelaxation, but relaxes the ordering of actions, while the RPG captures thecausal propositional aspects of the problem. We discuss how these twocomponents interact to produce a single unified heuristic and go on to explorehow further numeric features of planning problems can be integrated into theMIP. We show that encoding a limited subset of the propositional problem toaugment the MIP can yield more accurate guidance, partly by exploitingstructure such as propositional landmarks and propositional resources. Ourresults show that the use of this heuristic enhances scalability on problemswhere numeric resource interaction is key in finding a solution.\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "906e5884ef3244fda5a281131f5ddc9b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating outputs:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True Title: Preference-Based Monte Carlo Tree Search\n",
      "\n",
      "Predicted Title: A Monte Carlo Tree Search: A Unique and Unique Approach\n",
      "\n",
      "Abstract:   Monte Carlo tree search (MCTS) is a popular choice for solving sequentialanytime problems. However, it depends on a numeric feedback signal, which canbe difficult to define. Real-time MCTS is a variant which may only rarelyencounter states with an explicit, extrinsic reward. To deal with such cases,the experimenter has to supply an additional numeric feedback signal in theform of a heuristic, which intrinsically guides the agent. Recent work hasshown evidence that in different areas the underlying structure is ordinal andnot numerical. Hence erroneous and biased heuristics are inevitable, especiallyin such domains. In this paper, we propose a MCTS variant which only depends onqualitative feedback, and therefore opens up new applications for MCTS. We alsofind indications that translating absolute into ordinal feedback may bebeneficial. Using a puzzle domain, we show that our preference-based MCTSvariant, wich only receives qualitative feedback, is able to reach aperformance level comparable to a regular MCTS baseline, which obtainsquantitative feedback.\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0adf4c82a7bf443692883af3902e8f07",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating outputs:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True Title: Self-Organizing Maps for Storage and Transfer of Knowledge in\n",
      "  Reinforcement Learning\n",
      "\n",
      "Predicted Title: A-based Map for Reusing and Transfer Knowledge from Previously Learned Task Knowledge\n",
      "\n",
      "Abstract:   The idea of reusing or transferring information from previously learned tasks(source tasks) for the learning of new tasks (target tasks) has the potentialto significantly improve the sample efficiency of a reinforcement learningagent. In this work, we describe a novel approach for reusing previouslyacquired knowledge by using it to guide the exploration of an agent while itlearns new tasks. In order to do so, we employ a variant of the growingself-organizing map algorithm, which is trained using a measure of similaritythat is defined directly in the space of the vectorized representations of thevalue functions. In addition to enabling transfer across tasks, the resultingmap is simultaneously used to enable the efficient storage of previouslyacquired task knowledge in an adaptive and scalable manner. We empiricallyvalidate our approach in a simulated navigation environment, and alsodemonstrate its utility through simple experiments using a mobilemicro-robotics platform. In addition, we demonstrate the scalability of thisapproach, and analytically examine its relation to the proposed network growthmechanism. Further, we briefly discuss some of the possible improvements andextensions to this approach, as well as its relevance to real world scenariosin the context of continual learning.\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "672d20ff824c4d2e8d1d14c9e40d9720",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating outputs:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True Title: Evaluating Go Game Records for Prediction of Player Attributes\n",
      "\n",
      "Predicted Title: A Evaluation of Go Game Data\n",
      "\n",
      "Abstract:   We propose a way of extracting and aggregating per-move evaluations from setsof Go game records. The evaluations capture different aspects of the games suchas played patterns or statistic of sente/gote sequences. Using machine learningalgorithms, the evaluations can be utilized to predict different relevanttarget variables. We apply this methodology to predict the strength and playingstyle of the player (e.g. territoriality or aggressivity) with good accuracy.We propose a number of possible applications including aiding in Go study,seeding real-work ranks of internet players or tuning of Go-playing programs.\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "87d34d22c2574b8787346adc79a0ac03",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating outputs:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True Title: SAPFOCS: a metaheuristic based approach to part family formation\n",
      "  problems in group technology\n",
      "\n",
      "Predicted Title: A Part Family Formation in GroupTechnology\n",
      "\n",
      "Abstract:   This article deals with Part family formation problem which is believed to bemoderately complicated to be solved in polynomial time in the vicinity of GroupTechnology (GT). In the past literature researchers investigated that the partfamily formation techniques are principally based on production flow analysis(PFA) which usually considers operational requirements, sequences and time.Part Coding Analysis (PCA) is merely considered in GT which is believed to bethe proficient method to identify the part families. PCA classifies parts byallotting them to different families based on their resemblances in: (1) designcharacteristics such as shape and size, and/or (2) manufacturingcharacteristics (machining requirements). A novel approach based on simulatedannealing namely SAPFOCS is adopted in this study to develop effective partfamilies exploiting the PCA technique. Thereafter Taguchi's orthogonal designmethod is employed to solve the critical issues on the subject of parametersselection for the proposed metaheuristic algorithm. The adopted technique istherefore tested on 5 different datasets of size 5 {\\times} 9 to 27 {\\times} 9and the obtained results are compared with C-Linkage clustering technique. Theexperimental results reported that the proposed metaheuristic algorithm isextremely effective in terms of the quality of the solution obtained and hasoutperformed C-Linkage algorithm in most instances.\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "66e1fc46381c43fd84301216798d4979",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating outputs:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True Title: Contextual Symmetries in Probabilistic Graphical Models\n",
      "\n",
      "Predicted Title: A Contextual symmetries\n",
      "\n",
      "Abstract:   An important approach for efficient inference in probabilistic graphicalmodels exploits symmetries among objects in the domain. Symmetric variables(states) are collapsed into meta-variables (meta-states) and inferencealgorithms are run over the lifted graphical model instead of the flat one. Ourpaper extends existing definitions of symmetry by introducing the novel notionof contextual symmetry. Two states that are not globally symmetric, can becontextually symmetric under some specific assignment to a subset of variables,referred to as the context variables. Contextual symmetry subsumes previoussymmetry definitions and can rep resent a large class of symmetries notrepresentable earlier. We show how to compute contextual symmetries by reducingit to the problem of graph isomorphism. We extend previous work on exploitingsymmetries in the MCMC framework to the case of contextual symmetries. Ourexperiments on several domains of interest demonstrate that exploitingcontextual symmetries can result in significant computational gains.\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "47be59d47c8e41c9afbc0dd24de19399",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating outputs:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True Title: Representing and Reasoning with Qualitative Preferences for\n",
      "  Compositional Systems\n",
      "\n",
      "Predicted Title: A dominance relation for the selection of most preferred collections\n",
      "\n",
      "Abstract:   Many applications, e.g., Web service composition, complex system design, teamformation, etc., rely on methods for identifying collections of objects orentities satisfying some functional requirement. Among the collections thatsatisfy the functional requirement, it is often necessary to identify one ormore collections that are optimal with respect to user preferences over a setof attributes that describe the non-functional properties of the collection.  We develop a formalism that lets users express the relative importance amongattributes and qualitative preferences over the valuations of each attribute.We define a dominance relation that allows us to compare collections of objectsin terms of preferences over attributes of the objects that make up thecollection. We establish some key properties of the dominance relation. Inparticular, we show that the dominance relation is a strict partial order whenthe intra-attribute preference relations are strict partial orders and therelative importance preference relation is an interval order.  We provide algorithms that use this dominance relation to identify the set ofmost preferred collections. We show that under certain conditions, thealgorithms are guaranteed to return only (sound), all (complete), or at leastone (weakly complete) of the most preferred collections. We present results ofsimulation experiments comparing the proposed algorithms with respect to (a)the quality of solutions (number of most preferred solutions) produced by thealgorithms, and (b) their performance and efficiency. We also explore someinteresting conjectures suggested by the results of our experiments that relatethe properties of the user preferences, the dominance relation, and thealgorithms.\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "670479be615e4732aeb72a48b892222d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating outputs:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True Title: Contextual Symmetries in Probabilistic Graphical Models\n",
      "\n",
      "Predicted Title: A Contextual symmetries\n",
      "\n",
      "Abstract:   An important approach for efficient inference in probabilistic graphicalmodels exploits symmetries among objects in the domain. Symmetric variables(states) are collapsed into meta-variables (meta-states) and inferencealgorithms are run over the lifted graphical model instead of the flat one. Ourpaper extends existing definitions of symmetry by introducing the novel notionof contextual symmetry. Two states that are not globally symmetric, can becontextually symmetric under some specific assignment to a subset of variables,referred to as the context variables. Contextual symmetry subsumes previoussymmetry definitions and can rep resent a large class of symmetries notrepresentable earlier. We show how to compute contextual symmetries by reducingit to the problem of graph isomorphism. We extend previous work on exploitingsymmetries in the MCMC framework to the case of contextual symmetries. Ourexperiments on several domains of interest demonstrate that exploitingcontextual symmetries can result in significant computational gains.\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a6b3c1f7abb249e3a42a103e92160e70",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating outputs:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True Title: Shiva++: An Enhanced Graph based Ontology Matcher\n",
      "\n",
      "Predicted Title: Anology based systems for knowledge management\n",
      "\n",
      "Abstract:   With the web getting bigger and assimilating knowledge about differentconcepts and domains, it is becoming very difficult for simple database drivenapplications to capture the data for a domain. Thus developers have come outwith ontology based systems which can store large amount of information and canapply reasoning and produce timely information. Thus facilitating effectiveknowledge management. Though this approach has made our lives easier, but atthe same time has given rise to another problem. Two different ontologiesassimilating same knowledge tend to use different terms for the same concepts.This creates confusion among knowledge engineers and workers, as they do notknow which is a better term then the other. Thus we need to merge ontologiesworking on same domain so that the engineers can develop a better applicationover it. This paper shows the development of one such matcher which merges theconcepts available in two ontologies at two levels; 1) at string level and 2)at semantic level; thus producing better merged ontologies. We have used agraph matching technique which works at the core of the system. We have alsoevaluated the system and have tested its performance with its predecessor whichworks only on string matching. Thus current approach produces better results.\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "320f4db75a1949a6986484b5e29e482f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating outputs:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True Title: A Study of Student Learning Skills Using Fuzzy Relation Equations\n",
      "\n",
      "Predicted Title: A Fuzzy relation equations (FRE)\n",
      "\n",
      "Abstract:   Fuzzy relation equations (FRE)are associated with the composition of binaryfuzzy relations. In the present work FRE are used as a tool for studying theprocess of learning a new subject matter by a student class. A classroomapplication and other csuitable examples connected to the student learning ofthe derivative are also presented illustrating our results and usefulconclusions are obtained.\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1d4e7e809ce347ae9197cd29f7ca089e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating outputs:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True Title: A Survey of Multi-Objective Sequential Decision-Making\n",
      "\n",
      "Predicted Title: A Multi-Objective Methods for Planning and Learning\n",
      "\n",
      "Abstract:   Sequential decision-making problems with multiple objectives arise naturallyin practice and pose unique challenges for research in decision-theoreticplanning and learning, which has largely focused on single-objective settings.This article surveys algorithms designed for sequential decision-makingproblems with multiple objectives. Though there is a growing body of literatureon this subject, little of it makes explicit under what circumstances specialmethods are needed to solve multi-objective problems. Therefore, we identifythree distinct scenarios in which converting such a problem to asingle-objective one is impossible, infeasible, or undesirable. Furthermore, wepropose a taxonomy that classifies multi-objective methods according to theapplicable scenario, the nature of the scalarization function (which projectsmulti-objective values to scalar ones), and the type of policies considered. Weshow how these factors determine the nature of an optimal solution, which canbe a single policy, a convex hull, or a Pareto front. Using this taxonomy, wesurvey the literature on multi-objective methods for planning and learning.Finally, we discuss key applications of such methods and outline opportunitiesfor future work.\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "acd29e000c8041bbb7c2091f24e7df93",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating outputs:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True Title: The Multi-Agent Reinforcement Learning in Malm\\\"O (MARL\\\"O) Competition\n",
      "\n",
      "Predicted Title: A Multi-Agent Reinforcement Learning in Malm\\\"O\n",
      "\n",
      "Abstract:   Learning in multi-agent scenarios is a fruitful research direction, butcurrent approaches still show scalability problems in multiple games withgeneral reward settings and different opponent types. The Multi-AgentReinforcement Learning in Malm\\\"O (MARL\\\"O) competition is a new challenge thatproposes research in this domain using multiple 3D games. The goal of thiscontest is to foster research in general agents that can learn across differentgames and opponent types, proposing a challenge as a milestone in the directionof Artificial General Intelligence.\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b4e79c2a23bc4483a635f04a99da3ee1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating outputs:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True Title: A Study of Student Learning Skills Using Fuzzy Relation Equations\n",
      "\n",
      "Predicted Title: A Fuzzy relation equations (FRE)\n",
      "\n",
      "Abstract:   Fuzzy relation equations (FRE)are associated with the composition of binaryfuzzy relations. In the present work FRE are used as a tool for studying theprocess of learning a new subject matter by a student class. A classroomapplication and other csuitable examples connected to the student learning ofthe derivative are also presented illustrating our results and usefulconclusions are obtained.\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "abce3faddeff416980d2c8b81425bf5a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating outputs:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True Title: Modeling Variations of First-Order Horn Abduction in Answer Set\n",
      "  Programming\n",
      "\n",
      "Predicted Title: A Answer Set Programming: An Approach to Abduction in First Order Horn Logic\n",
      "\n",
      "Abstract:   We study abduction in First Order Horn logic theories where all atoms can beabduced and we are looking for preferred solutions with respect to threeobjective functions: cardinality minimality, coherence, and weighted abduction.We represent this reasoning problem in Answer Set Programming (ASP), in orderto obtain a flexible framework for experimenting with global constraints andobjective functions, and to test the boundaries of what is possible with ASP.Realizing this problem in ASP is challenging as it requires value invention andequivalence between certain constants, because the Unique Names Assumption doesnot hold in general. To permit reasoning in cyclic theories, we formallydescribe fine-grained variations of limiting Skolemization. We identify termequivalence as a main instantiation bottleneck, and improve the efficiency ofour approach with on-demand constraints that were used to eliminate the samebottleneck in state-of-the-art solvers. We evaluate our approach experimentallyon the ACCEL benchmark for plan recognition in Natural Language Understanding.Our encodings are publicly available, modular, and our approach is moreefficient than state-of-the-art solvers on the ACCEL benchmark.\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8bef007640624efc8f8ae84a82d7ef47",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating outputs:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True Title: Learning STRIPS Action Models with Classical Planning\n",
      "\n",
      "Predicted Title: Acompiling STRIPS action models from a classical planning task\n",
      "\n",
      "Abstract:   This paper presents a novel approach for learning STRIPS action models fromexamples that compiles this inductive learning task into a classical planningtask. Interestingly, the compilation approach is flexible to different amountsof available input knowledge; the learning examples can range from a set ofplans (with their corresponding initial and final states) to just a pair ofinitial and final states (no intermediate action or state is given). Moreover,the compilation accepts partially specified action models and it can be used tovalidate whether the observation of a plan execution follows a given STRIPSaction model, even if this model is not fully specified.\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0f3343a577b54169a838f601cf121b4d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating outputs:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True Title: Self-Organizing Maps for Storage and Transfer of Knowledge in\n",
      "  Reinforcement Learning\n",
      "\n",
      "Predicted Title: A-based Map for Reusing and Transfer Knowledge from Previously Learned Task Knowledge\n",
      "\n",
      "Abstract:   The idea of reusing or transferring information from previously learned tasks(source tasks) for the learning of new tasks (target tasks) has the potentialto significantly improve the sample efficiency of a reinforcement learningagent. In this work, we describe a novel approach for reusing previouslyacquired knowledge by using it to guide the exploration of an agent while itlearns new tasks. In order to do so, we employ a variant of the growingself-organizing map algorithm, which is trained using a measure of similaritythat is defined directly in the space of the vectorized representations of thevalue functions. In addition to enabling transfer across tasks, the resultingmap is simultaneously used to enable the efficient storage of previouslyacquired task knowledge in an adaptive and scalable manner. We empiricallyvalidate our approach in a simulated navigation environment, and alsodemonstrate its utility through simple experiments using a mobilemicro-robotics platform. In addition, we demonstrate the scalability of thisapproach, and analytically examine its relation to the proposed network growthmechanism. Further, we briefly discuss some of the possible improvements andextensions to this approach, as well as its relevance to real world scenariosin the context of continual learning.\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bfdc2932045942f4b866fce04ee172ec",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating outputs:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True Title: Adding Context to Concept Trees\n",
      "\n",
      "Predicted Title: A Concept Trees: A Concept Base\n",
      "\n",
      "Abstract:   A Concept Tree is a structure for storing knowledge where the trees arestored in a database called a Concept Base. It sits between the highlydistributed neural architectures and the distributed information systems, withthe intention of bringing brain-like and computer systems closer together.Concept Trees can grow from the semi-structured sources when consistentsequences of concepts are presented. Each tree ideally represents a singlecohesive concept and the trees can link with each other for navigation andsemantic purposes. The trees are therefore also a type of semantic network andwould benefit from having a consistent level of context for each node. Aconsistent build process is managed through a 'counting rule' and some otherrules that can normalise the database structure. This restricted structure canthen be complimented and enriched by the more dynamic context. It is alsosuggested to use the linking structure of the licas system [15] as a basis forthe context links, where the mathematical model is extended further to definethis. A number of tests have demonstrated the soundness of the architecture.Building the trees from text documents shows that the tree structure could beinherent in natural language. Then, two types of query language are described.Both of these can perform consistent query processes to return knowledge to theuser and even enhance the query with new knowledge. This is supported evenfurther with direct comparisons to a cognitive model, also being developed bythe author.\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d5a0b1aec1e34e4488fbf1d955fd7e58",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating outputs:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True Title: Configurable Markov Decision Processes\n",
      "\n",
      "Predicted Title: AConfigurableMarkov Decision Processes\n",
      "\n",
      "Abstract:   In many real-world problems, there is the possibility to configure, to alimited extent, some environmental parameters to improve the performance of alearning agent. In this paper, we propose a novel framework, ConfigurableMarkov Decision Processes (Conf-MDPs), to model this new type of interactionwith the environment. Furthermore, we provide a new learning algorithm, SafePolicy-Model Iteration (SPMI), to jointly and adaptively optimize the policyand the environment configuration. After having introduced our approach andderived some theoretical results, we present the experimental evaluation in twoexplicative problems to show the benefits of the environment configurability onthe performance of the learned policy.\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "358209f7d31b4ba48967d5119059eb63",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating outputs:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True Title: A Study of Student Learning Skills Using Fuzzy Relation Equations\n",
      "\n",
      "Predicted Title: A Fuzzy relation equations (FRE)\n",
      "\n",
      "Abstract:   Fuzzy relation equations (FRE)are associated with the composition of binaryfuzzy relations. In the present work FRE are used as a tool for studying theprocess of learning a new subject matter by a student class. A classroomapplication and other csuitable examples connected to the student learning ofthe derivative are also presented illustrating our results and usefulconclusions are obtained.\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3160606f55984f3488cdb7faae336adc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating outputs:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True Title: Towards an Ontology based integrated Framework for Semantic Web\n",
      "\n",
      "Predicted Title: A ontologies for heterogeneous data sources\n",
      "\n",
      "Abstract:   This Ontologies are widely used as a means for solving the informationheterogeneity problems on the web because of their capability to provideexplicit meaning to the information. They become an efficient tool forknowledge representation in a structured manner. There is always more than oneontology for the same domain. Furthermore, there is no standard method forbuilding ontologies, and there are many ontology building tools using differentontology languages. Because of these reasons, interoperability between theontologies is very low. Current ontology tools mostly use functions to build,edit and inference the ontology. Methods for merging heterogeneous domainontologies are not included in most tools. This paper presents ontology mergingmethodology for building a single global ontology from heterogeneous eXtensibleMarkup Language (XML) data sources to capture and maintain all the knowledgewhich XML data sources can contain\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0b76fb9df7e747f898ce11ee5ea03aa6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating outputs:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True Title: A Study of Student Learning Skills Using Fuzzy Relation Equations\n",
      "\n",
      "Predicted Title: A Fuzzy relation equations (FRE)\n",
      "\n",
      "Abstract:   Fuzzy relation equations (FRE)are associated with the composition of binaryfuzzy relations. In the present work FRE are used as a tool for studying theprocess of learning a new subject matter by a student class. A classroomapplication and other csuitable examples connected to the student learning ofthe derivative are also presented illustrating our results and usefulconclusions are obtained.\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0074578925d7488a9221ba0be3f766cd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating outputs:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True Title: The SP theory of intelligence and the representation and processing of\n",
      "  knowledge in the brain\n",
      "\n",
      "Predicted Title: A SP theory of intelligence: An overview and partial model\n",
      "\n",
      "Abstract:   The \"SP theory of intelligence\", with its realisation in the \"SP computermodel\", aims to simplify and integrate observations and concepts acrossAI-related fields, with information compression as a unifying theme. This paperdescribes how abstract structures and processes in the theory may be realisedin terms of neurons, their interconnections, and the transmission of signalsbetween neurons. This part of the SP theory -- \"SP-neural\" -- is a tentativeand partial model for the representation and processing of knowledge in thebrain. In the SP theory (apart from SP-neural), all kinds of knowledge arerepresented with \"patterns\", where a pattern is an array of atomic symbols inone or two dimensions. In SP-neural, the concept of a \"pattern\" is realised asan array of neurons called a \"pattern assembly\", similar to Hebb's concept of a\"cell assembly\" but with important differences. Central to the processing ofinformation in the SP system is the powerful concept of \"multiple alignment\",borrowed and adapted from bioinformatics. Processes such as patternrecognition, reasoning and problem solving are achieved via the building ofmultiple alignments, while unsupervised learning -- significantly differentfrom the \"Hebbian\" kinds of learning -- is achieved by creating patterns fromsensory information and also by creating patterns from multiple alignments inwhich there is a partial match between one pattern and another. Short-livedneural structures equivalent to multiple alignments will be created via aninter-play of excitatory and inhibitory neural signals. The paper discussesseveral associated issues, with relevant empirical evidence.\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3c6fe4d436aa49f9aefc8c20483fcffe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating outputs:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True Title: Configurable Markov Decision Processes\n",
      "\n",
      "Predicted Title: AConfigurableMarkov Decision Processes\n",
      "\n",
      "Abstract:   In many real-world problems, there is the possibility to configure, to alimited extent, some environmental parameters to improve the performance of alearning agent. In this paper, we propose a novel framework, ConfigurableMarkov Decision Processes (Conf-MDPs), to model this new type of interactionwith the environment. Furthermore, we provide a new learning algorithm, SafePolicy-Model Iteration (SPMI), to jointly and adaptively optimize the policyand the environment configuration. After having introduced our approach andderived some theoretical results, we present the experimental evaluation in twoexplicative problems to show the benefits of the environment configurability onthe performance of the learned policy.\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "864850e3330946bc9bfc5fa2b74a88ac",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating outputs:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True Title: Shiva++: An Enhanced Graph based Ontology Matcher\n",
      "\n",
      "Predicted Title: Anology based systems for knowledge management\n",
      "\n",
      "Abstract:   With the web getting bigger and assimilating knowledge about differentconcepts and domains, it is becoming very difficult for simple database drivenapplications to capture the data for a domain. Thus developers have come outwith ontology based systems which can store large amount of information and canapply reasoning and produce timely information. Thus facilitating effectiveknowledge management. Though this approach has made our lives easier, but atthe same time has given rise to another problem. Two different ontologiesassimilating same knowledge tend to use different terms for the same concepts.This creates confusion among knowledge engineers and workers, as they do notknow which is a better term then the other. Thus we need to merge ontologiesworking on same domain so that the engineers can develop a better applicationover it. This paper shows the development of one such matcher which merges theconcepts available in two ontologies at two levels; 1) at string level and 2)at semantic level; thus producing better merged ontologies. We have used agraph matching technique which works at the core of the system. We have alsoevaluated the system and have tested its performance with its predecessor whichworks only on string matching. Thus current approach produces better results.\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1c23518686fc41ee942528bce5956dfc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating outputs:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True Title: Towards an Ontology based integrated Framework for Semantic Web\n",
      "\n",
      "Predicted Title: A ontologies for heterogeneous data sources\n",
      "\n",
      "Abstract:   This Ontologies are widely used as a means for solving the informationheterogeneity problems on the web because of their capability to provideexplicit meaning to the information. They become an efficient tool forknowledge representation in a structured manner. There is always more than oneontology for the same domain. Furthermore, there is no standard method forbuilding ontologies, and there are many ontology building tools using differentontology languages. Because of these reasons, interoperability between theontologies is very low. Current ontology tools mostly use functions to build,edit and inference the ontology. Methods for merging heterogeneous domainontologies are not included in most tools. This paper presents ontology mergingmethodology for building a single global ontology from heterogeneous eXtensibleMarkup Language (XML) data sources to capture and maintain all the knowledgewhich XML data sources can contain\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6fb259509c3644b1a49e2045dbbe2a47",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating outputs:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True Title: The SP theory of intelligence and the representation and processing of\n",
      "  knowledge in the brain\n",
      "\n",
      "Predicted Title: A SP theory of intelligence: An overview and partial model\n",
      "\n",
      "Abstract:   The \"SP theory of intelligence\", with its realisation in the \"SP computermodel\", aims to simplify and integrate observations and concepts acrossAI-related fields, with information compression as a unifying theme. This paperdescribes how abstract structures and processes in the theory may be realisedin terms of neurons, their interconnections, and the transmission of signalsbetween neurons. This part of the SP theory -- \"SP-neural\" -- is a tentativeand partial model for the representation and processing of knowledge in thebrain. In the SP theory (apart from SP-neural), all kinds of knowledge arerepresented with \"patterns\", where a pattern is an array of atomic symbols inone or two dimensions. In SP-neural, the concept of a \"pattern\" is realised asan array of neurons called a \"pattern assembly\", similar to Hebb's concept of a\"cell assembly\" but with important differences. Central to the processing ofinformation in the SP system is the powerful concept of \"multiple alignment\",borrowed and adapted from bioinformatics. Processes such as patternrecognition, reasoning and problem solving are achieved via the building ofmultiple alignments, while unsupervised learning -- significantly differentfrom the \"Hebbian\" kinds of learning -- is achieved by creating patterns fromsensory information and also by creating patterns from multiple alignments inwhich there is a partial match between one pattern and another. Short-livedneural structures equivalent to multiple alignments will be created via aninter-play of excitatory and inhibitory neural signals. The paper discussesseveral associated issues, with relevant empirical evidence.\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9277aa5a4d9a499ca9572bf5e2932dc1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating outputs:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True Title: Shiva++: An Enhanced Graph based Ontology Matcher\n",
      "\n",
      "Predicted Title: Anology based systems for knowledge management\n",
      "\n",
      "Abstract:   With the web getting bigger and assimilating knowledge about differentconcepts and domains, it is becoming very difficult for simple database drivenapplications to capture the data for a domain. Thus developers have come outwith ontology based systems which can store large amount of information and canapply reasoning and produce timely information. Thus facilitating effectiveknowledge management. Though this approach has made our lives easier, but atthe same time has given rise to another problem. Two different ontologiesassimilating same knowledge tend to use different terms for the same concepts.This creates confusion among knowledge engineers and workers, as they do notknow which is a better term then the other. Thus we need to merge ontologiesworking on same domain so that the engineers can develop a better applicationover it. This paper shows the development of one such matcher which merges theconcepts available in two ontologies at two levels; 1) at string level and 2)at semantic level; thus producing better merged ontologies. We have used agraph matching technique which works at the core of the system. We have alsoevaluated the system and have tested its performance with its predecessor whichworks only on string matching. Thus current approach produces better results.\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6e703cc653024ee2b97252d7dea4999e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating outputs:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True Title: A Comparative Study on Parameter Estimation in Software Reliability\n",
      "  Modeling using Swarm Intelligence\n",
      "\n",
      "Predicted Title: Anfficient Swarm Optimization and Firefly Algorithm\n",
      "\n",
      "Abstract:   This work focuses on a comparison between the performances of two well-knownSwarm algorithms: Cuckoo Search (CS) and Firefly Algorithm (FA), in estimatingthe parameters of Software Reliability Growth Models. This study is furtherreinforced using Particle Swarm Optimization (PSO) and Ant Colony Optimization(ACO). All algorithms are evaluated according to real software failure data,the tests are performed and the obtained results are compared to show theperformance of each of the used algorithms. Furthermore, CS and FA are alsocompared with each other on bases of execution time and iteration number.Experimental results show that CS is more efficient in estimating theparameters of SRGMs, and it has outperformed FA in addition to PSO and ACO forthe selected Data sets and employed models.\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9e7420f4e02d4efab8a9bb574e87f3e5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating outputs:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True Title: A Fuzzy AHP Approach for Supplier Selection Problem: A Case Study in a\n",
      "  Gear Motor Company\n",
      "\n",
      "Predicted Title: A Fuzzy AHP for Suuplier Selection\n",
      "\n",
      "Abstract:   Suuplier selection is one of the most important functions of a purchasingdepartment. Since by deciding the best supplier, companies can save materialcosts and increase competitive advantage.However this decision becomescompilcated in case of multiple suppliers, multiple conflicting criteria, andimprecise parameters. In addition the uncertainty and vagueness of the experts'opinion is the prominent characteristic of the problem. therefore anextensively used multi criteria decision making tool Fuzzy AHP can be utilizedas an approach for supplier selection problem. This paper reveals theapplication of Fuzzy AHP in a gear motor company determining the best supplierwith respect to selected criteria. the contribution of this study is not onlythe application of the Fuzzy AHP methodology for supplier selection problem,but also releasing a comprehensive literature review of multi criteria decisionmaking problems. In addition by stating the steps of Fuzzy AHP clearly andnumerically, this study can be a guide of the methodology to be implemented toother multiple criteria decision making problems.\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "12980ecbd7174fc8b381be395625002b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating outputs:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True Title: SAPFOCS: a metaheuristic based approach to part family formation\n",
      "  problems in group technology\n",
      "\n",
      "Predicted Title: A Part Family Formation in GroupTechnology\n",
      "\n",
      "Abstract:   This article deals with Part family formation problem which is believed to bemoderately complicated to be solved in polynomial time in the vicinity of GroupTechnology (GT). In the past literature researchers investigated that the partfamily formation techniques are principally based on production flow analysis(PFA) which usually considers operational requirements, sequences and time.Part Coding Analysis (PCA) is merely considered in GT which is believed to bethe proficient method to identify the part families. PCA classifies parts byallotting them to different families based on their resemblances in: (1) designcharacteristics such as shape and size, and/or (2) manufacturingcharacteristics (machining requirements). A novel approach based on simulatedannealing namely SAPFOCS is adopted in this study to develop effective partfamilies exploiting the PCA technique. Thereafter Taguchi's orthogonal designmethod is employed to solve the critical issues on the subject of parametersselection for the proposed metaheuristic algorithm. The adopted technique istherefore tested on 5 different datasets of size 5 {\\times} 9 to 27 {\\times} 9and the obtained results are compared with C-Linkage clustering technique. Theexperimental results reported that the proposed metaheuristic algorithm isextremely effective in terms of the quality of the solution obtained and hasoutperformed C-Linkage algorithm in most instances.\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3a805c3fa9774e23a181e8de3cc96155",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating outputs:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True Title: Representing and Reasoning with Qualitative Preferences for\n",
      "  Compositional Systems\n",
      "\n",
      "Predicted Title: A dominance relation for the selection of most preferred collections\n",
      "\n",
      "Abstract:   Many applications, e.g., Web service composition, complex system design, teamformation, etc., rely on methods for identifying collections of objects orentities satisfying some functional requirement. Among the collections thatsatisfy the functional requirement, it is often necessary to identify one ormore collections that are optimal with respect to user preferences over a setof attributes that describe the non-functional properties of the collection.  We develop a formalism that lets users express the relative importance amongattributes and qualitative preferences over the valuations of each attribute.We define a dominance relation that allows us to compare collections of objectsin terms of preferences over attributes of the objects that make up thecollection. We establish some key properties of the dominance relation. Inparticular, we show that the dominance relation is a strict partial order whenthe intra-attribute preference relations are strict partial orders and therelative importance preference relation is an interval order.  We provide algorithms that use this dominance relation to identify the set ofmost preferred collections. We show that under certain conditions, thealgorithms are guaranteed to return only (sound), all (complete), or at leastone (weakly complete) of the most preferred collections. We present results ofsimulation experiments comparing the proposed algorithms with respect to (a)the quality of solutions (number of most preferred solutions) produced by thealgorithms, and (b) their performance and efficiency. We also explore someinteresting conjectures suggested by the results of our experiments that relatethe properties of the user preferences, the dominance relation, and thealgorithms.\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d33ea10ff2624f6793ca04ca59d67b32",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating outputs:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True Title: A Modified Vortex Search Algorithm for Numerical Function Optimization\n",
      "\n",
      "Predicted Title: Avortex Search: A Modification of Vortex Search\n",
      "\n",
      "Abstract:   The Vortex Search (VS) algorithm is one of the recently proposedmetaheuristic algorithms which was inspired from the vortical flow of thestirred fluids. Although the VS algorithm is shown to be a good candidate forthe solution of certain optimization problems, it also has some drawbacks. Inthe VS algorithm, candidate solutions are generated around the current bestsolution by using a Gaussian distribution at each iteration pass. This providessimplicity to the algorithm but it also leads to some problems along.Especially, for the functions those have a number of local minimum points, toselect a single point to generate candidate solutions leads the algorithm tobeing trapped into a local minimum point. Due to the adaptive step-sizeadjustment scheme used in the VS algorithm, the locality of the createdcandidate solutions is increased at each iteration pass. Therefore, if thealgorithm cannot escape a local point as quickly as possible, it becomes muchmore difficult for the algorithm to escape from that point in the latteriterations. In this study, a modified Vortex Search algorithm (MVS) is proposedto overcome above mentioned drawback of the existing VS algorithm. In the MVSalgorithm, the candidate solutions are generated around a number of points ateach iteration pass. Computational results showed that with the help of thismodification the global search ability of the existing VS algorithm is improvedand the MVS algorithm outperformed the existing VS algorithm, PSO2011 and ABCalgorithms for the benchmark numerical function set.\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "35fd152146a6480381d26d233af77ceb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating outputs:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True Title: SAPFOCS: a metaheuristic based approach to part family formation\n",
      "  problems in group technology\n",
      "\n",
      "Predicted Title: A Part Family Formation in GroupTechnology\n",
      "\n",
      "Abstract:   This article deals with Part family formation problem which is believed to bemoderately complicated to be solved in polynomial time in the vicinity of GroupTechnology (GT). In the past literature researchers investigated that the partfamily formation techniques are principally based on production flow analysis(PFA) which usually considers operational requirements, sequences and time.Part Coding Analysis (PCA) is merely considered in GT which is believed to bethe proficient method to identify the part families. PCA classifies parts byallotting them to different families based on their resemblances in: (1) designcharacteristics such as shape and size, and/or (2) manufacturingcharacteristics (machining requirements). A novel approach based on simulatedannealing namely SAPFOCS is adopted in this study to develop effective partfamilies exploiting the PCA technique. Thereafter Taguchi's orthogonal designmethod is employed to solve the critical issues on the subject of parametersselection for the proposed metaheuristic algorithm. The adopted technique istherefore tested on 5 different datasets of size 5 {\\times} 9 to 27 {\\times} 9and the obtained results are compared with C-Linkage clustering technique. Theexperimental results reported that the proposed metaheuristic algorithm isextremely effective in terms of the quality of the solution obtained and hasoutperformed C-Linkage algorithm in most instances.\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f114b22b5ea94e2190795dc05b1f3360",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating outputs:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True Title: Towards an Ontology based integrated Framework for Semantic Web\n",
      "\n",
      "Predicted Title: A ontologies for heterogeneous data sources\n",
      "\n",
      "Abstract:   This Ontologies are widely used as a means for solving the informationheterogeneity problems on the web because of their capability to provideexplicit meaning to the information. They become an efficient tool forknowledge representation in a structured manner. There is always more than oneontology for the same domain. Furthermore, there is no standard method forbuilding ontologies, and there are many ontology building tools using differentontology languages. Because of these reasons, interoperability between theontologies is very low. Current ontology tools mostly use functions to build,edit and inference the ontology. Methods for merging heterogeneous domainontologies are not included in most tools. This paper presents ontology mergingmethodology for building a single global ontology from heterogeneous eXtensibleMarkup Language (XML) data sources to capture and maintain all the knowledgewhich XML data sources can contain\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7dd07f7b959b41a49f9fc588a632b7d3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating outputs:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True Title: A Study of Student Learning Skills Using Fuzzy Relation Equations\n",
      "\n",
      "Predicted Title: A Fuzzy relation equations (FRE)\n",
      "\n",
      "Abstract:   Fuzzy relation equations (FRE)are associated with the composition of binaryfuzzy relations. In the present work FRE are used as a tool for studying theprocess of learning a new subject matter by a student class. A classroomapplication and other csuitable examples connected to the student learning ofthe derivative are also presented illustrating our results and usefulconclusions are obtained.\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c70b3f52d0174d60900261353b265fe4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating outputs:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True Title: A Modified Vortex Search Algorithm for Numerical Function Optimization\n",
      "\n",
      "Predicted Title: Avortex Search: A Modification of Vortex Search\n",
      "\n",
      "Abstract:   The Vortex Search (VS) algorithm is one of the recently proposedmetaheuristic algorithms which was inspired from the vortical flow of thestirred fluids. Although the VS algorithm is shown to be a good candidate forthe solution of certain optimization problems, it also has some drawbacks. Inthe VS algorithm, candidate solutions are generated around the current bestsolution by using a Gaussian distribution at each iteration pass. This providessimplicity to the algorithm but it also leads to some problems along.Especially, for the functions those have a number of local minimum points, toselect a single point to generate candidate solutions leads the algorithm tobeing trapped into a local minimum point. Due to the adaptive step-sizeadjustment scheme used in the VS algorithm, the locality of the createdcandidate solutions is increased at each iteration pass. Therefore, if thealgorithm cannot escape a local point as quickly as possible, it becomes muchmore difficult for the algorithm to escape from that point in the latteriterations. In this study, a modified Vortex Search algorithm (MVS) is proposedto overcome above mentioned drawback of the existing VS algorithm. In the MVSalgorithm, the candidate solutions are generated around a number of points ateach iteration pass. Computational results showed that with the help of thismodification the global search ability of the existing VS algorithm is improvedand the MVS algorithm outperformed the existing VS algorithm, PSO2011 and ABCalgorithms for the benchmark numerical function set.\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4460b3af7ccf4643b1681174660e1d0f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating outputs:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True Title: Shiva: A Framework for Graph Based Ontology Matching\n",
      "\n",
      "Predicted Title: An ontologies for data matching\n",
      "\n",
      "Abstract:   Since long, corporations are looking for knowledge sources which can providestructured description of data and can focus on meaning and sharedunderstanding. Structures which can facilitate open world assumptions and canbe flexible enough to incorporate and recognize more than one name for anentity. A source whose major purpose is to facilitate human communication andinteroperability. Clearly, databases fail to provide these features andontologies have emerged as an alternative choice, but corporations working onsame domain tend to make different ontologies. The problem occurs when theywant to share their data/knowledge. Thus we need tools to merge ontologies intoone. This task is termed as ontology matching. This is an emerging area andstill we have to go a long way in having an ideal matcher which can producegood results. In this paper we have shown a framework to matching ontologiesusing graphs.\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9da82e20ccef4a29a68ec428bc59def4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating outputs:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True Title: Autonomous Self-Explanation of Behavior for Interactive Reinforcement\n",
      "  Learning Agents\n",
      "\n",
      "Predicted Title: A Instruction-Based Behavior Explanation\n",
      "\n",
      "Abstract:   In cooperation, the workers must know how co-workers behave. However, anagent's policy, which is embedded in a statistical machine learning model, ishard to understand, and requires much time and knowledge to comprehend.Therefore, it is difficult for people to predict the behavior of machinelearning robots, which makes Human Robot Cooperation challenging. In thispaper, we propose Instruction-based Behavior Explanation (IBE), a method toexplain an autonomous agent's future behavior. In IBE, an agent canautonomously acquire the expressions to explain its own behavior by reusing theinstructions given by a human expert to accelerate the learning of the agent'spolicy. IBE also enables a developmental agent, whose policy may change duringthe cooperation, to explain its own behavior with sufficient time granularity.\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e6411a1f69bc41109f795371c535c412",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating outputs:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True Title: A Directional Feature with Energy based Offline Signature Verification\n",
      "  Network\n",
      "\n",
      "Predicted Title: Aofflinesignature verification system\n",
      "\n",
      "Abstract:   Signature used as a biometric is implemented in various systems as well asevery signature signed by each person is distinct at the same time. So, it isvery important to have a computerized signature verification system. In offlinesignature verification system dynamic features are not available obviously, butone can use a signature as an image and apply image processing techniques tomake an effective offline signature verification system. Author proposes aintelligent network used directional feature and energy density both as inputsto the same network and classifies the signature. Neural network is used as aclassifier for this system. The results are compared with both the very basicenergy density method and a simple directional feature method of offlinesignature verification system and this proposed new network is found veryeffective as compared to the above two methods, specially for less number oftraining samples, which can be implemented practically.\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eef8c2fbd77b4eefa3f6b63510281a63",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating outputs:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True Title: A note on the complexity of the causal ordering problem\n",
      "\n",
      "Predicted Title: A causal ordering: A concise report on the complexity of the causalordering problem\n",
      "\n",
      "Abstract:   In this note we provide a concise report on the complexity of the causalordering problem, originally introduced by Simon to reason about causaldependencies implicit in systems of mathematical equations. We show thatSimon's classical algorithm to infer causal ordering is NP-Hard---anintractability previously guessed but never proven. We present then a detailedaccount based on Nayak's suggested algorithmic solution (the best available),which is dominated by computing transitive closure---bounded in time by$O(|\\mathcal V|\\cdot |\\mathcal S|)$, where $\\mathcal S(\\mathcal E, \\mathcal V)$is the input system structure composed of a set $\\mathcal E$ of equations overa set $\\mathcal V$ of variables with number of variable appearances (density)$|\\mathcal S|$. We also comment on the potential of causal ordering foremerging applications in large-scale hypothesis management and analytics.\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2e7303cf18a24488bab54aaa1ec49669",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating outputs:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True Title: A Modified Vortex Search Algorithm for Numerical Function Optimization\n",
      "\n",
      "Predicted Title: Avortex Search: A Modification of Vortex Search\n",
      "\n",
      "Abstract:   The Vortex Search (VS) algorithm is one of the recently proposedmetaheuristic algorithms which was inspired from the vortical flow of thestirred fluids. Although the VS algorithm is shown to be a good candidate forthe solution of certain optimization problems, it also has some drawbacks. Inthe VS algorithm, candidate solutions are generated around the current bestsolution by using a Gaussian distribution at each iteration pass. This providessimplicity to the algorithm but it also leads to some problems along.Especially, for the functions those have a number of local minimum points, toselect a single point to generate candidate solutions leads the algorithm tobeing trapped into a local minimum point. Due to the adaptive step-sizeadjustment scheme used in the VS algorithm, the locality of the createdcandidate solutions is increased at each iteration pass. Therefore, if thealgorithm cannot escape a local point as quickly as possible, it becomes muchmore difficult for the algorithm to escape from that point in the latteriterations. In this study, a modified Vortex Search algorithm (MVS) is proposedto overcome above mentioned drawback of the existing VS algorithm. In the MVSalgorithm, the candidate solutions are generated around a number of points ateach iteration pass. Computational results showed that with the help of thismodification the global search ability of the existing VS algorithm is improvedand the MVS algorithm outperformed the existing VS algorithm, PSO2011 and ABCalgorithms for the benchmark numerical function set.\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "67485100e6af42a89a65017066fe14c1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating outputs:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True Title: A Comparative Study on Parameter Estimation in Software Reliability\n",
      "  Modeling using Swarm Intelligence\n",
      "\n",
      "Predicted Title: Anfficient Swarm Optimization and Firefly Algorithm\n",
      "\n",
      "Abstract:   This work focuses on a comparison between the performances of two well-knownSwarm algorithms: Cuckoo Search (CS) and Firefly Algorithm (FA), in estimatingthe parameters of Software Reliability Growth Models. This study is furtherreinforced using Particle Swarm Optimization (PSO) and Ant Colony Optimization(ACO). All algorithms are evaluated according to real software failure data,the tests are performed and the obtained results are compared to show theperformance of each of the used algorithms. Furthermore, CS and FA are alsocompared with each other on bases of execution time and iteration number.Experimental results show that CS is more efficient in estimating theparameters of SRGMs, and it has outperformed FA in addition to PSO and ACO forthe selected Data sets and employed models.\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9af33f65b5944655ad5c2a20fd0c5a4a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating outputs:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True Title: Qualitative Approximate Behavior Composition\n",
      "\n",
      "Predicted Title: Aimating behavior composition without simulation\n",
      "\n",
      "Abstract:   The behavior composition problem involves automatically building a controllerthat is able to realize a desired, but unavailable, target system (e.g., ahouse surveillance) by suitably coordinating a set of available components(e.g., video cameras, blinds, lamps, a vacuum cleaner, phones, etc.) Previouswork has almost exclusively aimed at bringing about the desired component inits totality, which is highly unsatisfactory for unsolvable problems. In thiswork, we develop an approach for approximate behavior composition withoutdeparting from the classical setting, thus making the problem applicable to amuch wider range of cases. Based on the notion of simulation, we characterizewhat a maximal controller and the \"closest\" implementable target module(optimal approximation) are, and show how these can be computed using ATL modelchecking technology for a special case. We show the uniqueness of optimalapproximations, and prove their soundness and completeness with respect totheir imported controllers.\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "71c52d8a0c3547579a0925bef8c3fc82",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating outputs:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True Title: Evaluating Go Game Records for Prediction of Player Attributes\n",
      "\n",
      "Predicted Title: A Evaluation of Go Game Data\n",
      "\n",
      "Abstract:   We propose a way of extracting and aggregating per-move evaluations from setsof Go game records. The evaluations capture different aspects of the games suchas played patterns or statistic of sente/gote sequences. Using machine learningalgorithms, the evaluations can be utilized to predict different relevanttarget variables. We apply this methodology to predict the strength and playingstyle of the player (e.g. territoriality or aggressivity) with good accuracy.We propose a number of possible applications including aiding in Go study,seeding real-work ranks of internet players or tuning of Go-playing programs.\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5974967c4bf54dd29dc2d848c1bf1d07",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating outputs:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True Title: Preference-Based Monte Carlo Tree Search\n",
      "\n",
      "Predicted Title: A Monte Carlo Tree Search: A Unique and Unique Approach\n",
      "\n",
      "Abstract:   Monte Carlo tree search (MCTS) is a popular choice for solving sequentialanytime problems. However, it depends on a numeric feedback signal, which canbe difficult to define. Real-time MCTS is a variant which may only rarelyencounter states with an explicit, extrinsic reward. To deal with such cases,the experimenter has to supply an additional numeric feedback signal in theform of a heuristic, which intrinsically guides the agent. Recent work hasshown evidence that in different areas the underlying structure is ordinal andnot numerical. Hence erroneous and biased heuristics are inevitable, especiallyin such domains. In this paper, we propose a MCTS variant which only depends onqualitative feedback, and therefore opens up new applications for MCTS. We alsofind indications that translating absolute into ordinal feedback may bebeneficial. Using a puzzle domain, we show that our preference-based MCTSvariant, wich only receives qualitative feedback, is able to reach aperformance level comparable to a regular MCTS baseline, which obtainsquantitative feedback.\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a037ffddd8414339b8b3ee8e68ac80dc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating outputs:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True Title: Modeling Variations of First-Order Horn Abduction in Answer Set\n",
      "  Programming\n",
      "\n",
      "Predicted Title: A Answer Set Programming: An Approach to Abduction in First Order Horn Logic\n",
      "\n",
      "Abstract:   We study abduction in First Order Horn logic theories where all atoms can beabduced and we are looking for preferred solutions with respect to threeobjective functions: cardinality minimality, coherence, and weighted abduction.We represent this reasoning problem in Answer Set Programming (ASP), in orderto obtain a flexible framework for experimenting with global constraints andobjective functions, and to test the boundaries of what is possible with ASP.Realizing this problem in ASP is challenging as it requires value invention andequivalence between certain constants, because the Unique Names Assumption doesnot hold in general. To permit reasoning in cyclic theories, we formallydescribe fine-grained variations of limiting Skolemization. We identify termequivalence as a main instantiation bottleneck, and improve the efficiency ofour approach with on-demand constraints that were used to eliminate the samebottleneck in state-of-the-art solvers. We evaluate our approach experimentallyon the ACCEL benchmark for plan recognition in Natural Language Understanding.Our encodings are publicly available, modular, and our approach is moreefficient than state-of-the-art solvers on the ACCEL benchmark.\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d9e2f79a90bd4be283be89aaaf10f9eb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating outputs:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True Title: A Fuzzy AHP Approach for Supplier Selection Problem: A Case Study in a\n",
      "  Gear Motor Company\n",
      "\n",
      "Predicted Title: A Fuzzy AHP for Suuplier Selection\n",
      "\n",
      "Abstract:   Suuplier selection is one of the most important functions of a purchasingdepartment. Since by deciding the best supplier, companies can save materialcosts and increase competitive advantage.However this decision becomescompilcated in case of multiple suppliers, multiple conflicting criteria, andimprecise parameters. In addition the uncertainty and vagueness of the experts'opinion is the prominent characteristic of the problem. therefore anextensively used multi criteria decision making tool Fuzzy AHP can be utilizedas an approach for supplier selection problem. This paper reveals theapplication of Fuzzy AHP in a gear motor company determining the best supplierwith respect to selected criteria. the contribution of this study is not onlythe application of the Fuzzy AHP methodology for supplier selection problem,but also releasing a comprehensive literature review of multi criteria decisionmaking problems. In addition by stating the steps of Fuzzy AHP clearly andnumerically, this study can be a guide of the methodology to be implemented toother multiple criteria decision making problems.\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "095d94b36b50459fbce2a9ca8697a136",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating outputs:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True Title: Shiva++: An Enhanced Graph based Ontology Matcher\n",
      "\n",
      "Predicted Title: Anology based systems for knowledge management\n",
      "\n",
      "Abstract:   With the web getting bigger and assimilating knowledge about differentconcepts and domains, it is becoming very difficult for simple database drivenapplications to capture the data for a domain. Thus developers have come outwith ontology based systems which can store large amount of information and canapply reasoning and produce timely information. Thus facilitating effectiveknowledge management. Though this approach has made our lives easier, but atthe same time has given rise to another problem. Two different ontologiesassimilating same knowledge tend to use different terms for the same concepts.This creates confusion among knowledge engineers and workers, as they do notknow which is a better term then the other. Thus we need to merge ontologiesworking on same domain so that the engineers can develop a better applicationover it. This paper shows the development of one such matcher which merges theconcepts available in two ontologies at two levels; 1) at string level and 2)at semantic level; thus producing better merged ontologies. We have used agraph matching technique which works at the core of the system. We have alsoevaluated the system and have tested its performance with its predecessor whichworks only on string matching. Thus current approach produces better results.\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7f1f929c2db44bf581c71303fe7e40b7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating outputs:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True Title: Self-Organizing Maps for Storage and Transfer of Knowledge in\n",
      "  Reinforcement Learning\n",
      "\n",
      "Predicted Title: A-based Map for Reusing and Transfer Knowledge from Previously Learned Task Knowledge\n",
      "\n",
      "Abstract:   The idea of reusing or transferring information from previously learned tasks(source tasks) for the learning of new tasks (target tasks) has the potentialto significantly improve the sample efficiency of a reinforcement learningagent. In this work, we describe a novel approach for reusing previouslyacquired knowledge by using it to guide the exploration of an agent while itlearns new tasks. In order to do so, we employ a variant of the growingself-organizing map algorithm, which is trained using a measure of similaritythat is defined directly in the space of the vectorized representations of thevalue functions. In addition to enabling transfer across tasks, the resultingmap is simultaneously used to enable the efficient storage of previouslyacquired task knowledge in an adaptive and scalable manner. We empiricallyvalidate our approach in a simulated navigation environment, and alsodemonstrate its utility through simple experiments using a mobilemicro-robotics platform. In addition, we demonstrate the scalability of thisapproach, and analytically examine its relation to the proposed network growthmechanism. Further, we briefly discuss some of the possible improvements andextensions to this approach, as well as its relevance to real world scenariosin the context of continual learning.\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "337835a1618f4780a70f8968887b7991",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating outputs:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True Title: Self-Organizing Maps for Storage and Transfer of Knowledge in\n",
      "  Reinforcement Learning\n",
      "\n",
      "Predicted Title: A-based Map for Reusing and Transfer Knowledge from Previously Learned Task Knowledge\n",
      "\n",
      "Abstract:   The idea of reusing or transferring information from previously learned tasks(source tasks) for the learning of new tasks (target tasks) has the potentialto significantly improve the sample efficiency of a reinforcement learningagent. In this work, we describe a novel approach for reusing previouslyacquired knowledge by using it to guide the exploration of an agent while itlearns new tasks. In order to do so, we employ a variant of the growingself-organizing map algorithm, which is trained using a measure of similaritythat is defined directly in the space of the vectorized representations of thevalue functions. In addition to enabling transfer across tasks, the resultingmap is simultaneously used to enable the efficient storage of previouslyacquired task knowledge in an adaptive and scalable manner. We empiricallyvalidate our approach in a simulated navigation environment, and alsodemonstrate its utility through simple experiments using a mobilemicro-robotics platform. In addition, we demonstrate the scalability of thisapproach, and analytically examine its relation to the proposed network growthmechanism. Further, we briefly discuss some of the possible improvements andextensions to this approach, as well as its relevance to real world scenariosin the context of continual learning.\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4144dd95ca114bcf94610e223ca92864",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating outputs:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True Title: Shiva++: An Enhanced Graph based Ontology Matcher\n",
      "\n",
      "Predicted Title: Anology based systems for knowledge management\n",
      "\n",
      "Abstract:   With the web getting bigger and assimilating knowledge about differentconcepts and domains, it is becoming very difficult for simple database drivenapplications to capture the data for a domain. Thus developers have come outwith ontology based systems which can store large amount of information and canapply reasoning and produce timely information. Thus facilitating effectiveknowledge management. Though this approach has made our lives easier, but atthe same time has given rise to another problem. Two different ontologiesassimilating same knowledge tend to use different terms for the same concepts.This creates confusion among knowledge engineers and workers, as they do notknow which is a better term then the other. Thus we need to merge ontologiesworking on same domain so that the engineers can develop a better applicationover it. This paper shows the development of one such matcher which merges theconcepts available in two ontologies at two levels; 1) at string level and 2)at semantic level; thus producing better merged ontologies. We have used agraph matching technique which works at the core of the system. We have alsoevaluated the system and have tested its performance with its predecessor whichworks only on string matching. Thus current approach produces better results.\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c86383eeb037471f9a97cca747f66158",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating outputs:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True Title: A Fuzzy AHP Approach for Supplier Selection Problem: A Case Study in a\n",
      "  Gear Motor Company\n",
      "\n",
      "Predicted Title: A Fuzzy AHP for Suuplier Selection\n",
      "\n",
      "Abstract:   Suuplier selection is one of the most important functions of a purchasingdepartment. Since by deciding the best supplier, companies can save materialcosts and increase competitive advantage.However this decision becomescompilcated in case of multiple suppliers, multiple conflicting criteria, andimprecise parameters. In addition the uncertainty and vagueness of the experts'opinion is the prominent characteristic of the problem. therefore anextensively used multi criteria decision making tool Fuzzy AHP can be utilizedas an approach for supplier selection problem. This paper reveals theapplication of Fuzzy AHP in a gear motor company determining the best supplierwith respect to selected criteria. the contribution of this study is not onlythe application of the Fuzzy AHP methodology for supplier selection problem,but also releasing a comprehensive literature review of multi criteria decisionmaking problems. In addition by stating the steps of Fuzzy AHP clearly andnumerically, this study can be a guide of the methodology to be implemented toother multiple criteria decision making problems.\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "04c1592c31e2459d8ca58f2ab0679848",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating outputs:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True Title: Representing and Reasoning with Qualitative Preferences for\n",
      "  Compositional Systems\n",
      "\n",
      "Predicted Title: A dominance relation for the selection of most preferred collections\n",
      "\n",
      "Abstract:   Many applications, e.g., Web service composition, complex system design, teamformation, etc., rely on methods for identifying collections of objects orentities satisfying some functional requirement. Among the collections thatsatisfy the functional requirement, it is often necessary to identify one ormore collections that are optimal with respect to user preferences over a setof attributes that describe the non-functional properties of the collection.  We develop a formalism that lets users express the relative importance amongattributes and qualitative preferences over the valuations of each attribute.We define a dominance relation that allows us to compare collections of objectsin terms of preferences over attributes of the objects that make up thecollection. We establish some key properties of the dominance relation. Inparticular, we show that the dominance relation is a strict partial order whenthe intra-attribute preference relations are strict partial orders and therelative importance preference relation is an interval order.  We provide algorithms that use this dominance relation to identify the set ofmost preferred collections. We show that under certain conditions, thealgorithms are guaranteed to return only (sound), all (complete), or at leastone (weakly complete) of the most preferred collections. We present results ofsimulation experiments comparing the proposed algorithms with respect to (a)the quality of solutions (number of most preferred solutions) produced by thealgorithms, and (b) their performance and efficiency. We also explore someinteresting conjectures suggested by the results of our experiments that relatethe properties of the user preferences, the dominance relation, and thealgorithms.\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4af82d29e5834ca9923178af2fcdb230",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating outputs:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True Title: Modeling Variations of First-Order Horn Abduction in Answer Set\n",
      "  Programming\n",
      "\n",
      "Predicted Title: A Answer Set Programming: An Approach to Abduction in First Order Horn Logic\n",
      "\n",
      "Abstract:   We study abduction in First Order Horn logic theories where all atoms can beabduced and we are looking for preferred solutions with respect to threeobjective functions: cardinality minimality, coherence, and weighted abduction.We represent this reasoning problem in Answer Set Programming (ASP), in orderto obtain a flexible framework for experimenting with global constraints andobjective functions, and to test the boundaries of what is possible with ASP.Realizing this problem in ASP is challenging as it requires value invention andequivalence between certain constants, because the Unique Names Assumption doesnot hold in general. To permit reasoning in cyclic theories, we formallydescribe fine-grained variations of limiting Skolemization. We identify termequivalence as a main instantiation bottleneck, and improve the efficiency ofour approach with on-demand constraints that were used to eliminate the samebottleneck in state-of-the-art solvers. We evaluate our approach experimentallyon the ACCEL benchmark for plan recognition in Natural Language Understanding.Our encodings are publicly available, modular, and our approach is moreefficient than state-of-the-art solvers on the ACCEL benchmark.\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "10e1842ce8fd45ed91e11e796b73751f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating outputs:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True Title: A Fuzzy AHP Approach for Supplier Selection Problem: A Case Study in a\n",
      "  Gear Motor Company\n",
      "\n",
      "Predicted Title: A Fuzzy AHP for Suuplier Selection\n",
      "\n",
      "Abstract:   Suuplier selection is one of the most important functions of a purchasingdepartment. Since by deciding the best supplier, companies can save materialcosts and increase competitive advantage.However this decision becomescompilcated in case of multiple suppliers, multiple conflicting criteria, andimprecise parameters. In addition the uncertainty and vagueness of the experts'opinion is the prominent characteristic of the problem. therefore anextensively used multi criteria decision making tool Fuzzy AHP can be utilizedas an approach for supplier selection problem. This paper reveals theapplication of Fuzzy AHP in a gear motor company determining the best supplierwith respect to selected criteria. the contribution of this study is not onlythe application of the Fuzzy AHP methodology for supplier selection problem,but also releasing a comprehensive literature review of multi criteria decisionmaking problems. In addition by stating the steps of Fuzzy AHP clearly andnumerically, this study can be a guide of the methodology to be implemented toother multiple criteria decision making problems.\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5e68850d0f644f4facfbf78474193e44",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating outputs:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True Title: A Survey of Multi-Objective Sequential Decision-Making\n",
      "\n",
      "Predicted Title: A Multi-Objective Methods for Planning and Learning\n",
      "\n",
      "Abstract:   Sequential decision-making problems with multiple objectives arise naturallyin practice and pose unique challenges for research in decision-theoreticplanning and learning, which has largely focused on single-objective settings.This article surveys algorithms designed for sequential decision-makingproblems with multiple objectives. Though there is a growing body of literatureon this subject, little of it makes explicit under what circumstances specialmethods are needed to solve multi-objective problems. Therefore, we identifythree distinct scenarios in which converting such a problem to asingle-objective one is impossible, infeasible, or undesirable. Furthermore, wepropose a taxonomy that classifies multi-objective methods according to theapplicable scenario, the nature of the scalarization function (which projectsmulti-objective values to scalar ones), and the type of policies considered. Weshow how these factors determine the nature of an optimal solution, which canbe a single policy, a convex hull, or a Pareto front. Using this taxonomy, wesurvey the literature on multi-objective methods for planning and learning.Finally, we discuss key applications of such methods and outline opportunitiesfor future work.\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d3d6015873274a4fb5c257a01799a642",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating outputs:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True Title: Learning STRIPS Action Models with Classical Planning\n",
      "\n",
      "Predicted Title: Acompiling STRIPS action models from a classical planning task\n",
      "\n",
      "Abstract:   This paper presents a novel approach for learning STRIPS action models fromexamples that compiles this inductive learning task into a classical planningtask. Interestingly, the compilation approach is flexible to different amountsof available input knowledge; the learning examples can range from a set ofplans (with their corresponding initial and final states) to just a pair ofinitial and final states (no intermediate action or state is given). Moreover,the compilation accepts partially specified action models and it can be used tovalidate whether the observation of a plan execution follows a given STRIPSaction model, even if this model is not fully specified.\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b92845469cb5414a85198de7b62e6107",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating outputs:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True Title: Representing and Reasoning with Qualitative Preferences for\n",
      "  Compositional Systems\n",
      "\n",
      "Predicted Title: A dominance relation for the selection of most preferred collections\n",
      "\n",
      "Abstract:   Many applications, e.g., Web service composition, complex system design, teamformation, etc., rely on methods for identifying collections of objects orentities satisfying some functional requirement. Among the collections thatsatisfy the functional requirement, it is often necessary to identify one ormore collections that are optimal with respect to user preferences over a setof attributes that describe the non-functional properties of the collection.  We develop a formalism that lets users express the relative importance amongattributes and qualitative preferences over the valuations of each attribute.We define a dominance relation that allows us to compare collections of objectsin terms of preferences over attributes of the objects that make up thecollection. We establish some key properties of the dominance relation. Inparticular, we show that the dominance relation is a strict partial order whenthe intra-attribute preference relations are strict partial orders and therelative importance preference relation is an interval order.  We provide algorithms that use this dominance relation to identify the set ofmost preferred collections. We show that under certain conditions, thealgorithms are guaranteed to return only (sound), all (complete), or at leastone (weakly complete) of the most preferred collections. We present results ofsimulation experiments comparing the proposed algorithms with respect to (a)the quality of solutions (number of most preferred solutions) produced by thealgorithms, and (b) their performance and efficiency. We also explore someinteresting conjectures suggested by the results of our experiments that relatethe properties of the user preferences, the dominance relation, and thealgorithms.\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "88c21fd59c2c4a3e81bf7c9e34086531",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating outputs:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True Title: Evaluating Go Game Records for Prediction of Player Attributes\n",
      "\n",
      "Predicted Title: A Evaluation of Go Game Data\n",
      "\n",
      "Abstract:   We propose a way of extracting and aggregating per-move evaluations from setsof Go game records. The evaluations capture different aspects of the games suchas played patterns or statistic of sente/gote sequences. Using machine learningalgorithms, the evaluations can be utilized to predict different relevanttarget variables. We apply this methodology to predict the strength and playingstyle of the player (e.g. territoriality or aggressivity) with good accuracy.We propose a number of possible applications including aiding in Go study,seeding real-work ranks of internet players or tuning of Go-playing programs.\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2ecfc0dba6f74bb1861b2dd6a2d685c4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating outputs:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True Title: Shiva++: An Enhanced Graph based Ontology Matcher\n",
      "\n",
      "Predicted Title: Anology based systems for knowledge management\n",
      "\n",
      "Abstract:   With the web getting bigger and assimilating knowledge about differentconcepts and domains, it is becoming very difficult for simple database drivenapplications to capture the data for a domain. Thus developers have come outwith ontology based systems which can store large amount of information and canapply reasoning and produce timely information. Thus facilitating effectiveknowledge management. Though this approach has made our lives easier, but atthe same time has given rise to another problem. Two different ontologiesassimilating same knowledge tend to use different terms for the same concepts.This creates confusion among knowledge engineers and workers, as they do notknow which is a better term then the other. Thus we need to merge ontologiesworking on same domain so that the engineers can develop a better applicationover it. This paper shows the development of one such matcher which merges theconcepts available in two ontologies at two levels; 1) at string level and 2)at semantic level; thus producing better merged ontologies. We have used agraph matching technique which works at the core of the system. We have alsoevaluated the system and have tested its performance with its predecessor whichworks only on string matching. Thus current approach produces better results.\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "87205c0499ec4df8b2bdf72783da4bba",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating outputs:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True Title: Self-Organizing Maps for Storage and Transfer of Knowledge in\n",
      "  Reinforcement Learning\n",
      "\n",
      "Predicted Title: A-based Map for Reusing and Transfer Knowledge from Previously Learned Task Knowledge\n",
      "\n",
      "Abstract:   The idea of reusing or transferring information from previously learned tasks(source tasks) for the learning of new tasks (target tasks) has the potentialto significantly improve the sample efficiency of a reinforcement learningagent. In this work, we describe a novel approach for reusing previouslyacquired knowledge by using it to guide the exploration of an agent while itlearns new tasks. In order to do so, we employ a variant of the growingself-organizing map algorithm, which is trained using a measure of similaritythat is defined directly in the space of the vectorized representations of thevalue functions. In addition to enabling transfer across tasks, the resultingmap is simultaneously used to enable the efficient storage of previouslyacquired task knowledge in an adaptive and scalable manner. We empiricallyvalidate our approach in a simulated navigation environment, and alsodemonstrate its utility through simple experiments using a mobilemicro-robotics platform. In addition, we demonstrate the scalability of thisapproach, and analytically examine its relation to the proposed network growthmechanism. Further, we briefly discuss some of the possible improvements andextensions to this approach, as well as its relevance to real world scenariosin the context of continual learning.\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2520218bf7b1462ebf7dafc493e691d5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating outputs:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True Title: The SP theory of intelligence and the representation and processing of\n",
      "  knowledge in the brain\n",
      "\n",
      "Predicted Title: A SP theory of intelligence: An overview and partial model\n",
      "\n",
      "Abstract:   The \"SP theory of intelligence\", with its realisation in the \"SP computermodel\", aims to simplify and integrate observations and concepts acrossAI-related fields, with information compression as a unifying theme. This paperdescribes how abstract structures and processes in the theory may be realisedin terms of neurons, their interconnections, and the transmission of signalsbetween neurons. This part of the SP theory -- \"SP-neural\" -- is a tentativeand partial model for the representation and processing of knowledge in thebrain. In the SP theory (apart from SP-neural), all kinds of knowledge arerepresented with \"patterns\", where a pattern is an array of atomic symbols inone or two dimensions. In SP-neural, the concept of a \"pattern\" is realised asan array of neurons called a \"pattern assembly\", similar to Hebb's concept of a\"cell assembly\" but with important differences. Central to the processing ofinformation in the SP system is the powerful concept of \"multiple alignment\",borrowed and adapted from bioinformatics. Processes such as patternrecognition, reasoning and problem solving are achieved via the building ofmultiple alignments, while unsupervised learning -- significantly differentfrom the \"Hebbian\" kinds of learning -- is achieved by creating patterns fromsensory information and also by creating patterns from multiple alignments inwhich there is a partial match between one pattern and another. Short-livedneural structures equivalent to multiple alignments will be created via aninter-play of excitatory and inhibitory neural signals. The paper discussesseveral associated issues, with relevant empirical evidence.\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "26568c43d7674e4396fd397497ee51d0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating outputs:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True Title: The emergence of Explainability of Intelligent Systems: Delivering\n",
      "  Explainable and Personalised Recommendations for Energy Efficiency\n",
      "\n",
      "Predicted Title: Aation Systems for Energy Efficiency\n",
      "\n",
      "Abstract:   The recent advances in artificial intelligence namely in machine learning anddeep learning, have boosted the performance of intelligent systems in severalways. This gave rise to human expectations, but also created the need for adeeper understanding of how intelligent systems think and decide. The conceptof explainability appeared, in the extent of explaining the internal systemmechanics in human terms. Recommendation systems are intelligent systems thatsupport human decision making, and as such, they have to be explainable inorder to increase user trust and improve the acceptance of recommendations. Inthis work, we focus on a context-aware recommendation system for energyefficiency and develop a mechanism for explainable and persuasiverecommendations, which are personalized to user preferences and habits. Thepersuasive facts either emphasize on the economical saving prospects (Econ) oron a positive ecological impact (Eco) and explanations provide the reason forrecommending an energy saving action. Based on a study conducted using aTelegram bot, different scenarios have been validated with actual data andhuman feedback. Current results show a total increase of 19\\% on therecommendation acceptance ratio when both economical and ecological persuasivefacts are employed. This revolutionary approach on recommendation systems,demonstrates how intelligent recommendations can effectively encourage energysaving behavior.\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a2313f80b17b468087d98931404c9e97",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating outputs:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True Title: SAPFOCS: a metaheuristic based approach to part family formation\n",
      "  problems in group technology\n",
      "\n",
      "Predicted Title: A Part Family Formation in GroupTechnology\n",
      "\n",
      "Abstract:   This article deals with Part family formation problem which is believed to bemoderately complicated to be solved in polynomial time in the vicinity of GroupTechnology (GT). In the past literature researchers investigated that the partfamily formation techniques are principally based on production flow analysis(PFA) which usually considers operational requirements, sequences and time.Part Coding Analysis (PCA) is merely considered in GT which is believed to bethe proficient method to identify the part families. PCA classifies parts byallotting them to different families based on their resemblances in: (1) designcharacteristics such as shape and size, and/or (2) manufacturingcharacteristics (machining requirements). A novel approach based on simulatedannealing namely SAPFOCS is adopted in this study to develop effective partfamilies exploiting the PCA technique. Thereafter Taguchi's orthogonal designmethod is employed to solve the critical issues on the subject of parametersselection for the proposed metaheuristic algorithm. The adopted technique istherefore tested on 5 different datasets of size 5 {\\times} 9 to 27 {\\times} 9and the obtained results are compared with C-Linkage clustering technique. Theexperimental results reported that the proposed metaheuristic algorithm isextremely effective in terms of the quality of the solution obtained and hasoutperformed C-Linkage algorithm in most instances.\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3a7e8e5b57864f88b88d180199046926",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating outputs:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True Title: Online Guest Detection in a Smart Home using Pervasive Sensors and\n",
      "  Probabilistic Reasoning\n",
      "\n",
      "Predicted Title: Aing the Number of People in a Smart Home at Each Time Step\n",
      "\n",
      "Abstract:   Smart home environments equipped with distributed sensor networks are capableof helping people by providing services related to health, emergency detectionor daily routine management. A backbone to these systems relies often on thesystem's ability to track and detect activities performed by the users in theirhome. Despite the continuous progress in the area of activity recognition insmart homes, many systems make a strong underlying assumption that the numberof occupants in the home at any given moment of time is always known.Estimating the number of persons in a Smart Home at each time step remains achallenge nowadays. Indeed, unlike most (crowd) counting solution which arebased on computer vision techniques, the sensors considered in a Smart Home areoften very simple and do not offer individually a good overview of thesituation. The data gathered needs therefore to be fused in order to inferuseful information. This paper aims at addressing this challenge and presents aprobabilistic approach able to estimate the number of persons in theenvironment at each time step. This approach works in two steps: first, anestimate of the number of persons present in the environment is done using aConstraint Satisfaction Problem solver, based on the topology of the sensornetwork and the sensor activation pattern at this time point. Then, a HiddenMarkov Model refines this estimate by considering the uncertainty related tothe sensors. Using both simulated and real data, our method has been tested andvalidated on two smart homes of different sizes and configuration anddemonstrates the ability to accurately estimate the number of inhabitants.\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e9f02bc2bc874f708e4b38dc4b784a3b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating outputs:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True Title: Contextual Symmetries in Probabilistic Graphical Models\n",
      "\n",
      "Predicted Title: A Contextual symmetries\n",
      "\n",
      "Abstract:   An important approach for efficient inference in probabilistic graphicalmodels exploits symmetries among objects in the domain. Symmetric variables(states) are collapsed into meta-variables (meta-states) and inferencealgorithms are run over the lifted graphical model instead of the flat one. Ourpaper extends existing definitions of symmetry by introducing the novel notionof contextual symmetry. Two states that are not globally symmetric, can becontextually symmetric under some specific assignment to a subset of variables,referred to as the context variables. Contextual symmetry subsumes previoussymmetry definitions and can rep resent a large class of symmetries notrepresentable earlier. We show how to compute contextual symmetries by reducingit to the problem of graph isomorphism. We extend previous work on exploitingsymmetries in the MCMC framework to the case of contextual symmetries. Ourexperiments on several domains of interest demonstrate that exploitingcontextual symmetries can result in significant computational gains.\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "654e8c7ae45a4303ace88a81e724f983",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating outputs:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True Title: Online Guest Detection in a Smart Home using Pervasive Sensors and\n",
      "  Probabilistic Reasoning\n",
      "\n",
      "Predicted Title: Aing the Number of People in a Smart Home at Each Time Step\n",
      "\n",
      "Abstract:   Smart home environments equipped with distributed sensor networks are capableof helping people by providing services related to health, emergency detectionor daily routine management. A backbone to these systems relies often on thesystem's ability to track and detect activities performed by the users in theirhome. Despite the continuous progress in the area of activity recognition insmart homes, many systems make a strong underlying assumption that the numberof occupants in the home at any given moment of time is always known.Estimating the number of persons in a Smart Home at each time step remains achallenge nowadays. Indeed, unlike most (crowd) counting solution which arebased on computer vision techniques, the sensors considered in a Smart Home areoften very simple and do not offer individually a good overview of thesituation. The data gathered needs therefore to be fused in order to inferuseful information. This paper aims at addressing this challenge and presents aprobabilistic approach able to estimate the number of persons in theenvironment at each time step. This approach works in two steps: first, anestimate of the number of persons present in the environment is done using aConstraint Satisfaction Problem solver, based on the topology of the sensornetwork and the sensor activation pattern at this time point. Then, a HiddenMarkov Model refines this estimate by considering the uncertainty related tothe sensors. Using both simulated and real data, our method has been tested andvalidated on two smart homes of different sizes and configuration anddemonstrates the ability to accurately estimate the number of inhabitants.\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "48d3ff50d2494cbe8b8be85b33146cfc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating outputs:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True Title: Cortex simulation system proposal using distributed computer network\n",
      "  environments\n",
      "\n",
      "Predicted Title: A neuroscience: A New Approach\n",
      "\n",
      "Abstract:   In the dawn of computer science and the eve of neuroscience we participate inrebirth of neuroscience due to new technology that allows us to deeply andprecisely explore whole new world that dwells in our brains.\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6b89c4259d85413cb9932eafed712fd5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating outputs:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True Title: The emergence of Explainability of Intelligent Systems: Delivering\n",
      "  Explainable and Personalised Recommendations for Energy Efficiency\n",
      "\n",
      "Predicted Title: Aation Systems for Energy Efficiency\n",
      "\n",
      "Abstract:   The recent advances in artificial intelligence namely in machine learning anddeep learning, have boosted the performance of intelligent systems in severalways. This gave rise to human expectations, but also created the need for adeeper understanding of how intelligent systems think and decide. The conceptof explainability appeared, in the extent of explaining the internal systemmechanics in human terms. Recommendation systems are intelligent systems thatsupport human decision making, and as such, they have to be explainable inorder to increase user trust and improve the acceptance of recommendations. Inthis work, we focus on a context-aware recommendation system for energyefficiency and develop a mechanism for explainable and persuasiverecommendations, which are personalized to user preferences and habits. Thepersuasive facts either emphasize on the economical saving prospects (Econ) oron a positive ecological impact (Eco) and explanations provide the reason forrecommending an energy saving action. Based on a study conducted using aTelegram bot, different scenarios have been validated with actual data andhuman feedback. Current results show a total increase of 19\\% on therecommendation acceptance ratio when both economical and ecological persuasivefacts are employed. This revolutionary approach on recommendation systems,demonstrates how intelligent recommendations can effectively encourage energysaving behavior.\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "03ca1c553f9d45a08a92aa4e0b75c6db",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating outputs:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True Title: A Study of Student Learning Skills Using Fuzzy Relation Equations\n",
      "\n",
      "Predicted Title: A Fuzzy relation equations (FRE)\n",
      "\n",
      "Abstract:   Fuzzy relation equations (FRE)are associated with the composition of binaryfuzzy relations. In the present work FRE are used as a tool for studying theprocess of learning a new subject matter by a student class. A classroomapplication and other csuitable examples connected to the student learning ofthe derivative are also presented illustrating our results and usefulconclusions are obtained.\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "49a2da9e652b4c06bdbf0450722683b5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating outputs:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True Title: Shiva++: An Enhanced Graph based Ontology Matcher\n",
      "\n",
      "Predicted Title: Anology based systems for knowledge management\n",
      "\n",
      "Abstract:   With the web getting bigger and assimilating knowledge about differentconcepts and domains, it is becoming very difficult for simple database drivenapplications to capture the data for a domain. Thus developers have come outwith ontology based systems which can store large amount of information and canapply reasoning and produce timely information. Thus facilitating effectiveknowledge management. Though this approach has made our lives easier, but atthe same time has given rise to another problem. Two different ontologiesassimilating same knowledge tend to use different terms for the same concepts.This creates confusion among knowledge engineers and workers, as they do notknow which is a better term then the other. Thus we need to merge ontologiesworking on same domain so that the engineers can develop a better applicationover it. This paper shows the development of one such matcher which merges theconcepts available in two ontologies at two levels; 1) at string level and 2)at semantic level; thus producing better merged ontologies. We have used agraph matching technique which works at the core of the system. We have alsoevaluated the system and have tested its performance with its predecessor whichworks only on string matching. Thus current approach produces better results.\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e22364dcab3c475e9e087d9606a8e917",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating outputs:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True Title: Towards an Ontology based integrated Framework for Semantic Web\n",
      "\n",
      "Predicted Title: A ontologies for heterogeneous data sources\n",
      "\n",
      "Abstract:   This Ontologies are widely used as a means for solving the informationheterogeneity problems on the web because of their capability to provideexplicit meaning to the information. They become an efficient tool forknowledge representation in a structured manner. There is always more than oneontology for the same domain. Furthermore, there is no standard method forbuilding ontologies, and there are many ontology building tools using differentontology languages. Because of these reasons, interoperability between theontologies is very low. Current ontology tools mostly use functions to build,edit and inference the ontology. Methods for merging heterogeneous domainontologies are not included in most tools. This paper presents ontology mergingmethodology for building a single global ontology from heterogeneous eXtensibleMarkup Language (XML) data sources to capture and maintain all the knowledgewhich XML data sources can contain\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3d1dae241e114b79950908885cd74c4f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating outputs:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True Title: Representing and Reasoning with Qualitative Preferences for\n",
      "  Compositional Systems\n",
      "\n",
      "Predicted Title: A dominance relation for the selection of most preferred collections\n",
      "\n",
      "Abstract:   Many applications, e.g., Web service composition, complex system design, teamformation, etc., rely on methods for identifying collections of objects orentities satisfying some functional requirement. Among the collections thatsatisfy the functional requirement, it is often necessary to identify one ormore collections that are optimal with respect to user preferences over a setof attributes that describe the non-functional properties of the collection.  We develop a formalism that lets users express the relative importance amongattributes and qualitative preferences over the valuations of each attribute.We define a dominance relation that allows us to compare collections of objectsin terms of preferences over attributes of the objects that make up thecollection. We establish some key properties of the dominance relation. Inparticular, we show that the dominance relation is a strict partial order whenthe intra-attribute preference relations are strict partial orders and therelative importance preference relation is an interval order.  We provide algorithms that use this dominance relation to identify the set ofmost preferred collections. We show that under certain conditions, thealgorithms are guaranteed to return only (sound), all (complete), or at leastone (weakly complete) of the most preferred collections. We present results ofsimulation experiments comparing the proposed algorithms with respect to (a)the quality of solutions (number of most preferred solutions) produced by thealgorithms, and (b) their performance and efficiency. We also explore someinteresting conjectures suggested by the results of our experiments that relatethe properties of the user preferences, the dominance relation, and thealgorithms.\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6c221584bf1e4baca2475e3b081a6734",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating outputs:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True Title: The SP theory of intelligence and the representation and processing of\n",
      "  knowledge in the brain\n",
      "\n",
      "Predicted Title: A SP theory of intelligence: An overview and partial model\n",
      "\n",
      "Abstract:   The \"SP theory of intelligence\", with its realisation in the \"SP computermodel\", aims to simplify and integrate observations and concepts acrossAI-related fields, with information compression as a unifying theme. This paperdescribes how abstract structures and processes in the theory may be realisedin terms of neurons, their interconnections, and the transmission of signalsbetween neurons. This part of the SP theory -- \"SP-neural\" -- is a tentativeand partial model for the representation and processing of knowledge in thebrain. In the SP theory (apart from SP-neural), all kinds of knowledge arerepresented with \"patterns\", where a pattern is an array of atomic symbols inone or two dimensions. In SP-neural, the concept of a \"pattern\" is realised asan array of neurons called a \"pattern assembly\", similar to Hebb's concept of a\"cell assembly\" but with important differences. Central to the processing ofinformation in the SP system is the powerful concept of \"multiple alignment\",borrowed and adapted from bioinformatics. Processes such as patternrecognition, reasoning and problem solving are achieved via the building ofmultiple alignments, while unsupervised learning -- significantly differentfrom the \"Hebbian\" kinds of learning -- is achieved by creating patterns fromsensory information and also by creating patterns from multiple alignments inwhich there is a partial match between one pattern and another. Short-livedneural structures equivalent to multiple alignments will be created via aninter-play of excitatory and inhibitory neural signals. The paper discussesseveral associated issues, with relevant empirical evidence.\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4a06a35ccc19411d881590868cbfc740",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating outputs:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True Title: Modeling Variations of First-Order Horn Abduction in Answer Set\n",
      "  Programming\n",
      "\n",
      "Predicted Title: A Answer Set Programming: An Approach to Abduction in First Order Horn Logic\n",
      "\n",
      "Abstract:   We study abduction in First Order Horn logic theories where all atoms can beabduced and we are looking for preferred solutions with respect to threeobjective functions: cardinality minimality, coherence, and weighted abduction.We represent this reasoning problem in Answer Set Programming (ASP), in orderto obtain a flexible framework for experimenting with global constraints andobjective functions, and to test the boundaries of what is possible with ASP.Realizing this problem in ASP is challenging as it requires value invention andequivalence between certain constants, because the Unique Names Assumption doesnot hold in general. To permit reasoning in cyclic theories, we formallydescribe fine-grained variations of limiting Skolemization. We identify termequivalence as a main instantiation bottleneck, and improve the efficiency ofour approach with on-demand constraints that were used to eliminate the samebottleneck in state-of-the-art solvers. We evaluate our approach experimentallyon the ACCEL benchmark for plan recognition in Natural Language Understanding.Our encodings are publicly available, modular, and our approach is moreefficient than state-of-the-art solvers on the ACCEL benchmark.\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "540626cb9b0e4ed88584228d30cb78e5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating outputs:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True Title: Towards an Ontology based integrated Framework for Semantic Web\n",
      "\n",
      "Predicted Title: A ontologies for heterogeneous data sources\n",
      "\n",
      "Abstract:   This Ontologies are widely used as a means for solving the informationheterogeneity problems on the web because of their capability to provideexplicit meaning to the information. They become an efficient tool forknowledge representation in a structured manner. There is always more than oneontology for the same domain. Furthermore, there is no standard method forbuilding ontologies, and there are many ontology building tools using differentontology languages. Because of these reasons, interoperability between theontologies is very low. Current ontology tools mostly use functions to build,edit and inference the ontology. Methods for merging heterogeneous domainontologies are not included in most tools. This paper presents ontology mergingmethodology for building a single global ontology from heterogeneous eXtensibleMarkup Language (XML) data sources to capture and maintain all the knowledgewhich XML data sources can contain\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "636386d9c247442ba1e17da149c8af91",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating outputs:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True Title: Shiva: A Framework for Graph Based Ontology Matching\n",
      "\n",
      "Predicted Title: An ontologies for data matching\n",
      "\n",
      "Abstract:   Since long, corporations are looking for knowledge sources which can providestructured description of data and can focus on meaning and sharedunderstanding. Structures which can facilitate open world assumptions and canbe flexible enough to incorporate and recognize more than one name for anentity. A source whose major purpose is to facilitate human communication andinteroperability. Clearly, databases fail to provide these features andontologies have emerged as an alternative choice, but corporations working onsame domain tend to make different ontologies. The problem occurs when theywant to share their data/knowledge. Thus we need tools to merge ontologies intoone. This task is termed as ontology matching. This is an emerging area andstill we have to go a long way in having an ideal matcher which can producegood results. In this paper we have shown a framework to matching ontologiesusing graphs.\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a8e4cae01f2344d0892698b8ffc57782",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating outputs:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True Title: A Fuzzy AHP Approach for Supplier Selection Problem: A Case Study in a\n",
      "  Gear Motor Company\n",
      "\n",
      "Predicted Title: A Fuzzy AHP for Suuplier Selection\n",
      "\n",
      "Abstract:   Suuplier selection is one of the most important functions of a purchasingdepartment. Since by deciding the best supplier, companies can save materialcosts and increase competitive advantage.However this decision becomescompilcated in case of multiple suppliers, multiple conflicting criteria, andimprecise parameters. In addition the uncertainty and vagueness of the experts'opinion is the prominent characteristic of the problem. therefore anextensively used multi criteria decision making tool Fuzzy AHP can be utilizedas an approach for supplier selection problem. This paper reveals theapplication of Fuzzy AHP in a gear motor company determining the best supplierwith respect to selected criteria. the contribution of this study is not onlythe application of the Fuzzy AHP methodology for supplier selection problem,but also releasing a comprehensive literature review of multi criteria decisionmaking problems. In addition by stating the steps of Fuzzy AHP clearly andnumerically, this study can be a guide of the methodology to be implemented toother multiple criteria decision making problems.\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "79532f1e7ab845d6b5373c3185f4a2d2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating outputs:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True Title: Online Guest Detection in a Smart Home using Pervasive Sensors and\n",
      "  Probabilistic Reasoning\n",
      "\n",
      "Predicted Title: Aing the Number of People in a Smart Home at Each Time Step\n",
      "\n",
      "Abstract:   Smart home environments equipped with distributed sensor networks are capableof helping people by providing services related to health, emergency detectionor daily routine management. A backbone to these systems relies often on thesystem's ability to track and detect activities performed by the users in theirhome. Despite the continuous progress in the area of activity recognition insmart homes, many systems make a strong underlying assumption that the numberof occupants in the home at any given moment of time is always known.Estimating the number of persons in a Smart Home at each time step remains achallenge nowadays. Indeed, unlike most (crowd) counting solution which arebased on computer vision techniques, the sensors considered in a Smart Home areoften very simple and do not offer individually a good overview of thesituation. The data gathered needs therefore to be fused in order to inferuseful information. This paper aims at addressing this challenge and presents aprobabilistic approach able to estimate the number of persons in theenvironment at each time step. This approach works in two steps: first, anestimate of the number of persons present in the environment is done using aConstraint Satisfaction Problem solver, based on the topology of the sensornetwork and the sensor activation pattern at this time point. Then, a HiddenMarkov Model refines this estimate by considering the uncertainty related tothe sensors. Using both simulated and real data, our method has been tested andvalidated on two smart homes of different sizes and configuration anddemonstrates the ability to accurately estimate the number of inhabitants.\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b5a68b27476e4de784867bacdffbdfb4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating outputs:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True Title: Contextual Symmetries in Probabilistic Graphical Models\n",
      "\n",
      "Predicted Title: A Contextual symmetries\n",
      "\n",
      "Abstract:   An important approach for efficient inference in probabilistic graphicalmodels exploits symmetries among objects in the domain. Symmetric variables(states) are collapsed into meta-variables (meta-states) and inferencealgorithms are run over the lifted graphical model instead of the flat one. Ourpaper extends existing definitions of symmetry by introducing the novel notionof contextual symmetry. Two states that are not globally symmetric, can becontextually symmetric under some specific assignment to a subset of variables,referred to as the context variables. Contextual symmetry subsumes previoussymmetry definitions and can rep resent a large class of symmetries notrepresentable earlier. We show how to compute contextual symmetries by reducingit to the problem of graph isomorphism. We extend previous work on exploitingsymmetries in the MCMC framework to the case of contextual symmetries. Ourexperiments on several domains of interest demonstrate that exploitingcontextual symmetries can result in significant computational gains.\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "86b1239bb87046ecafe08880b5ff8230",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating outputs:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True Title: The Multi-Agent Reinforcement Learning in Malm\\\"O (MARL\\\"O) Competition\n",
      "\n",
      "Predicted Title: A Multi-Agent Reinforcement Learning in Malm\\\"O\n",
      "\n",
      "Abstract:   Learning in multi-agent scenarios is a fruitful research direction, butcurrent approaches still show scalability problems in multiple games withgeneral reward settings and different opponent types. The Multi-AgentReinforcement Learning in Malm\\\"O (MARL\\\"O) competition is a new challenge thatproposes research in this domain using multiple 3D games. The goal of thiscontest is to foster research in general agents that can learn across differentgames and opponent types, proposing a challenge as a milestone in the directionof Artificial General Intelligence.\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "718059491dc64f64a158fa515355580e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating outputs:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True Title: Self-Organizing Maps for Storage and Transfer of Knowledge in\n",
      "  Reinforcement Learning\n",
      "\n",
      "Predicted Title: A-based Map for Reusing and Transfer Knowledge from Previously Learned Task Knowledge\n",
      "\n",
      "Abstract:   The idea of reusing or transferring information from previously learned tasks(source tasks) for the learning of new tasks (target tasks) has the potentialto significantly improve the sample efficiency of a reinforcement learningagent. In this work, we describe a novel approach for reusing previouslyacquired knowledge by using it to guide the exploration of an agent while itlearns new tasks. In order to do so, we employ a variant of the growingself-organizing map algorithm, which is trained using a measure of similaritythat is defined directly in the space of the vectorized representations of thevalue functions. In addition to enabling transfer across tasks, the resultingmap is simultaneously used to enable the efficient storage of previouslyacquired task knowledge in an adaptive and scalable manner. We empiricallyvalidate our approach in a simulated navigation environment, and alsodemonstrate its utility through simple experiments using a mobilemicro-robotics platform. In addition, we demonstrate the scalability of thisapproach, and analytically examine its relation to the proposed network growthmechanism. Further, we briefly discuss some of the possible improvements andextensions to this approach, as well as its relevance to real world scenariosin the context of continual learning.\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3b2e71d8f80a412fb5e7cc2c233bafdd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating outputs:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True Title: A Hybrid LP-RPG Heuristic for Modelling Numeric Resource Flows in\n",
      "  Planning\n",
      "\n",
      "Predicted Title: A-Based Hybrid Heuristic for numeric planning problems\n",
      "\n",
      "Abstract:   Although the use of metric fluents is fundamental to many practical planningproblems, the study of heuristics to support fully automated planners workingwith these fluents remains relatively unexplored. The most widely usedheuristic is the relaxation of metric fluents into interval-valued variables--- an idea first proposed a decade ago. Other heuristics depend on domainencodings that supply additional information about fluents, such as capacityconstraints or other resource-related annotations. A particular challenge tothese approaches is in handling interactions between metric fluents thatrepresent exchange, such as the transformation of quantities of raw materialsinto quantities of processed goods, or trading of money for materials. Theusual relaxation of metric fluents is often very poor in these situations,since it does not recognise that resources, once spent, are no longer availableto be spent again. We present a heuristic for numeric planning problemsbuilding on the propositional relaxed planning graph, but using a mathematicalprogram for numeric reasoning. We define a class of producer--consumer planningproblems and demonstrate how the numeric constraints in these can be modelledin a mixed integer program (MIP). This MIP is then combined with a metricRelaxed Planning Graph (RPG) heuristic to produce an integrated hybridheuristic. The MIP tracks resource use more accurately than the usualrelaxation, but relaxes the ordering of actions, while the RPG captures thecausal propositional aspects of the problem. We discuss how these twocomponents interact to produce a single unified heuristic and go on to explorehow further numeric features of planning problems can be integrated into theMIP. We show that encoding a limited subset of the propositional problem toaugment the MIP can yield more accurate guidance, partly by exploitingstructure such as propositional landmarks and propositional resources. Ourresults show that the use of this heuristic enhances scalability on problemswhere numeric resource interaction is key in finding a solution.\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9f79f44b81ed42078a1e9b0c9f4a7da9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating outputs:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True Title: Qualitative Approximate Behavior Composition\n",
      "\n",
      "Predicted Title: Aimating behavior composition without simulation\n",
      "\n",
      "Abstract:   The behavior composition problem involves automatically building a controllerthat is able to realize a desired, but unavailable, target system (e.g., ahouse surveillance) by suitably coordinating a set of available components(e.g., video cameras, blinds, lamps, a vacuum cleaner, phones, etc.) Previouswork has almost exclusively aimed at bringing about the desired component inits totality, which is highly unsatisfactory for unsolvable problems. In thiswork, we develop an approach for approximate behavior composition withoutdeparting from the classical setting, thus making the problem applicable to amuch wider range of cases. Based on the notion of simulation, we characterizewhat a maximal controller and the \"closest\" implementable target module(optimal approximation) are, and show how these can be computed using ATL modelchecking technology for a special case. We show the uniqueness of optimalapproximations, and prove their soundness and completeness with respect totheir imported controllers.\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "78b8688ed3694d0fae6cff9a1f217b26",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating outputs:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True Title: A Comparative Study on Parameter Estimation in Software Reliability\n",
      "  Modeling using Swarm Intelligence\n",
      "\n",
      "Predicted Title: Anfficient Swarm Optimization and Firefly Algorithm\n",
      "\n",
      "Abstract:   This work focuses on a comparison between the performances of two well-knownSwarm algorithms: Cuckoo Search (CS) and Firefly Algorithm (FA), in estimatingthe parameters of Software Reliability Growth Models. This study is furtherreinforced using Particle Swarm Optimization (PSO) and Ant Colony Optimization(ACO). All algorithms are evaluated according to real software failure data,the tests are performed and the obtained results are compared to show theperformance of each of the used algorithms. Furthermore, CS and FA are alsocompared with each other on bases of execution time and iteration number.Experimental results show that CS is more efficient in estimating theparameters of SRGMs, and it has outperformed FA in addition to PSO and ACO forthe selected Data sets and employed models.\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5128b9bbbdd04fdbb5ff03b01c2b1444",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating outputs:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True Title: A note on the complexity of the causal ordering problem\n",
      "\n",
      "Predicted Title: A causal ordering: A concise report on the complexity of the causalordering problem\n",
      "\n",
      "Abstract:   In this note we provide a concise report on the complexity of the causalordering problem, originally introduced by Simon to reason about causaldependencies implicit in systems of mathematical equations. We show thatSimon's classical algorithm to infer causal ordering is NP-Hard---anintractability previously guessed but never proven. We present then a detailedaccount based on Nayak's suggested algorithmic solution (the best available),which is dominated by computing transitive closure---bounded in time by$O(|\\mathcal V|\\cdot |\\mathcal S|)$, where $\\mathcal S(\\mathcal E, \\mathcal V)$is the input system structure composed of a set $\\mathcal E$ of equations overa set $\\mathcal V$ of variables with number of variable appearances (density)$|\\mathcal S|$. We also comment on the potential of causal ordering foremerging applications in large-scale hypothesis management and analytics.\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "59b332eb9e34413aa0977c67081ca243",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating outputs:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True Title: The Multi-Agent Reinforcement Learning in Malm\\\"O (MARL\\\"O) Competition\n",
      "\n",
      "Predicted Title: A Multi-Agent Reinforcement Learning in Malm\\\"O\n",
      "\n",
      "Abstract:   Learning in multi-agent scenarios is a fruitful research direction, butcurrent approaches still show scalability problems in multiple games withgeneral reward settings and different opponent types. The Multi-AgentReinforcement Learning in Malm\\\"O (MARL\\\"O) competition is a new challenge thatproposes research in this domain using multiple 3D games. The goal of thiscontest is to foster research in general agents that can learn across differentgames and opponent types, proposing a challenge as a milestone in the directionof Artificial General Intelligence.\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "71380faf9416435b96303ad38b15910d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating outputs:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True Title: Online Guest Detection in a Smart Home using Pervasive Sensors and\n",
      "  Probabilistic Reasoning\n",
      "\n",
      "Predicted Title: Aing the Number of People in a Smart Home at Each Time Step\n",
      "\n",
      "Abstract:   Smart home environments equipped with distributed sensor networks are capableof helping people by providing services related to health, emergency detectionor daily routine management. A backbone to these systems relies often on thesystem's ability to track and detect activities performed by the users in theirhome. Despite the continuous progress in the area of activity recognition insmart homes, many systems make a strong underlying assumption that the numberof occupants in the home at any given moment of time is always known.Estimating the number of persons in a Smart Home at each time step remains achallenge nowadays. Indeed, unlike most (crowd) counting solution which arebased on computer vision techniques, the sensors considered in a Smart Home areoften very simple and do not offer individually a good overview of thesituation. The data gathered needs therefore to be fused in order to inferuseful information. This paper aims at addressing this challenge and presents aprobabilistic approach able to estimate the number of persons in theenvironment at each time step. This approach works in two steps: first, anestimate of the number of persons present in the environment is done using aConstraint Satisfaction Problem solver, based on the topology of the sensornetwork and the sensor activation pattern at this time point. Then, a HiddenMarkov Model refines this estimate by considering the uncertainty related tothe sensors. Using both simulated and real data, our method has been tested andvalidated on two smart homes of different sizes and configuration anddemonstrates the ability to accurately estimate the number of inhabitants.\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "068686802048408c9b46c93f3774948d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating outputs:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True Title: A Directional Feature with Energy based Offline Signature Verification\n",
      "  Network\n",
      "\n",
      "Predicted Title: Aofflinesignature verification system\n",
      "\n",
      "Abstract:   Signature used as a biometric is implemented in various systems as well asevery signature signed by each person is distinct at the same time. So, it isvery important to have a computerized signature verification system. In offlinesignature verification system dynamic features are not available obviously, butone can use a signature as an image and apply image processing techniques tomake an effective offline signature verification system. Author proposes aintelligent network used directional feature and energy density both as inputsto the same network and classifies the signature. Neural network is used as aclassifier for this system. The results are compared with both the very basicenergy density method and a simple directional feature method of offlinesignature verification system and this proposed new network is found veryeffective as compared to the above two methods, specially for less number oftraining samples, which can be implemented practically.\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3d1829d06c48466c95f4bd1b24ae6eac",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating outputs:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True Title: A Study of Student Learning Skills Using Fuzzy Relation Equations\n",
      "\n",
      "Predicted Title: A Fuzzy relation equations (FRE)\n",
      "\n",
      "Abstract:   Fuzzy relation equations (FRE)are associated with the composition of binaryfuzzy relations. In the present work FRE are used as a tool for studying theprocess of learning a new subject matter by a student class. A classroomapplication and other csuitable examples connected to the student learning ofthe derivative are also presented illustrating our results and usefulconclusions are obtained.\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "370397634bd64913905575333635579c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating outputs:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True Title: Configurable Markov Decision Processes\n",
      "\n",
      "Predicted Title: AConfigurableMarkov Decision Processes\n",
      "\n",
      "Abstract:   In many real-world problems, there is the possibility to configure, to alimited extent, some environmental parameters to improve the performance of alearning agent. In this paper, we propose a novel framework, ConfigurableMarkov Decision Processes (Conf-MDPs), to model this new type of interactionwith the environment. Furthermore, we provide a new learning algorithm, SafePolicy-Model Iteration (SPMI), to jointly and adaptively optimize the policyand the environment configuration. After having introduced our approach andderived some theoretical results, we present the experimental evaluation in twoexplicative problems to show the benefits of the environment configurability onthe performance of the learned policy.\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8181bf060158408f9a7f274e3330fda2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating outputs:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True Title: Shiva++: An Enhanced Graph based Ontology Matcher\n",
      "\n",
      "Predicted Title: Anology based systems for knowledge management\n",
      "\n",
      "Abstract:   With the web getting bigger and assimilating knowledge about differentconcepts and domains, it is becoming very difficult for simple database drivenapplications to capture the data for a domain. Thus developers have come outwith ontology based systems which can store large amount of information and canapply reasoning and produce timely information. Thus facilitating effectiveknowledge management. Though this approach has made our lives easier, but atthe same time has given rise to another problem. Two different ontologiesassimilating same knowledge tend to use different terms for the same concepts.This creates confusion among knowledge engineers and workers, as they do notknow which is a better term then the other. Thus we need to merge ontologiesworking on same domain so that the engineers can develop a better applicationover it. This paper shows the development of one such matcher which merges theconcepts available in two ontologies at two levels; 1) at string level and 2)at semantic level; thus producing better merged ontologies. We have used agraph matching technique which works at the core of the system. We have alsoevaluated the system and have tested its performance with its predecessor whichworks only on string matching. Thus current approach produces better results.\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "13d60b58eae44d75b220b2982792fb80",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating outputs:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True Title: SAPFOCS: a metaheuristic based approach to part family formation\n",
      "  problems in group technology\n",
      "\n",
      "Predicted Title: A Part Family Formation in GroupTechnology\n",
      "\n",
      "Abstract:   This article deals with Part family formation problem which is believed to bemoderately complicated to be solved in polynomial time in the vicinity of GroupTechnology (GT). In the past literature researchers investigated that the partfamily formation techniques are principally based on production flow analysis(PFA) which usually considers operational requirements, sequences and time.Part Coding Analysis (PCA) is merely considered in GT which is believed to bethe proficient method to identify the part families. PCA classifies parts byallotting them to different families based on their resemblances in: (1) designcharacteristics such as shape and size, and/or (2) manufacturingcharacteristics (machining requirements). A novel approach based on simulatedannealing namely SAPFOCS is adopted in this study to develop effective partfamilies exploiting the PCA technique. Thereafter Taguchi's orthogonal designmethod is employed to solve the critical issues on the subject of parametersselection for the proposed metaheuristic algorithm. The adopted technique istherefore tested on 5 different datasets of size 5 {\\times} 9 to 27 {\\times} 9and the obtained results are compared with C-Linkage clustering technique. Theexperimental results reported that the proposed metaheuristic algorithm isextremely effective in terms of the quality of the solution obtained and hasoutperformed C-Linkage algorithm in most instances.\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "678c2d1f1c47445aa753cc25ce507a69",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating outputs:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True Title: Preference-Based Monte Carlo Tree Search\n",
      "\n",
      "Predicted Title: A Monte Carlo Tree Search: A Unique and Unique Approach\n",
      "\n",
      "Abstract:   Monte Carlo tree search (MCTS) is a popular choice for solving sequentialanytime problems. However, it depends on a numeric feedback signal, which canbe difficult to define. Real-time MCTS is a variant which may only rarelyencounter states with an explicit, extrinsic reward. To deal with such cases,the experimenter has to supply an additional numeric feedback signal in theform of a heuristic, which intrinsically guides the agent. Recent work hasshown evidence that in different areas the underlying structure is ordinal andnot numerical. Hence erroneous and biased heuristics are inevitable, especiallyin such domains. In this paper, we propose a MCTS variant which only depends onqualitative feedback, and therefore opens up new applications for MCTS. We alsofind indications that translating absolute into ordinal feedback may bebeneficial. Using a puzzle domain, we show that our preference-based MCTSvariant, wich only receives qualitative feedback, is able to reach aperformance level comparable to a regular MCTS baseline, which obtainsquantitative feedback.\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "77b17fcc46bb4853bf600c986a9c995c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating outputs:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True Title: Shiva++: An Enhanced Graph based Ontology Matcher\n",
      "\n",
      "Predicted Title: Anology based systems for knowledge management\n",
      "\n",
      "Abstract:   With the web getting bigger and assimilating knowledge about differentconcepts and domains, it is becoming very difficult for simple database drivenapplications to capture the data for a domain. Thus developers have come outwith ontology based systems which can store large amount of information and canapply reasoning and produce timely information. Thus facilitating effectiveknowledge management. Though this approach has made our lives easier, but atthe same time has given rise to another problem. Two different ontologiesassimilating same knowledge tend to use different terms for the same concepts.This creates confusion among knowledge engineers and workers, as they do notknow which is a better term then the other. Thus we need to merge ontologiesworking on same domain so that the engineers can develop a better applicationover it. This paper shows the development of one such matcher which merges theconcepts available in two ontologies at two levels; 1) at string level and 2)at semantic level; thus producing better merged ontologies. We have used agraph matching technique which works at the core of the system. We have alsoevaluated the system and have tested its performance with its predecessor whichworks only on string matching. Thus current approach produces better results.\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2360a48dd03b45c0ab857ee04200a930",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating outputs:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True Title: Learning STRIPS Action Models with Classical Planning\n",
      "\n",
      "Predicted Title: Acompiling STRIPS action models from a classical planning task\n",
      "\n",
      "Abstract:   This paper presents a novel approach for learning STRIPS action models fromexamples that compiles this inductive learning task into a classical planningtask. Interestingly, the compilation approach is flexible to different amountsof available input knowledge; the learning examples can range from a set ofplans (with their corresponding initial and final states) to just a pair ofinitial and final states (no intermediate action or state is given). Moreover,the compilation accepts partially specified action models and it can be used tovalidate whether the observation of a plan execution follows a given STRIPSaction model, even if this model is not fully specified.\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "326244a90a7443d4894ddc44e41c136d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating outputs:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True Title: Preference-Based Monte Carlo Tree Search\n",
      "\n",
      "Predicted Title: A Monte Carlo Tree Search: A Unique and Unique Approach\n",
      "\n",
      "Abstract:   Monte Carlo tree search (MCTS) is a popular choice for solving sequentialanytime problems. However, it depends on a numeric feedback signal, which canbe difficult to define. Real-time MCTS is a variant which may only rarelyencounter states with an explicit, extrinsic reward. To deal with such cases,the experimenter has to supply an additional numeric feedback signal in theform of a heuristic, which intrinsically guides the agent. Recent work hasshown evidence that in different areas the underlying structure is ordinal andnot numerical. Hence erroneous and biased heuristics are inevitable, especiallyin such domains. In this paper, we propose a MCTS variant which only depends onqualitative feedback, and therefore opens up new applications for MCTS. We alsofind indications that translating absolute into ordinal feedback may bebeneficial. Using a puzzle domain, we show that our preference-based MCTSvariant, wich only receives qualitative feedback, is able to reach aperformance level comparable to a regular MCTS baseline, which obtainsquantitative feedback.\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f073488a4a2f4ef983bd8015bb9ed1dd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating outputs:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True Title: Autonomous Self-Explanation of Behavior for Interactive Reinforcement\n",
      "  Learning Agents\n",
      "\n",
      "Predicted Title: A Instruction-Based Behavior Explanation\n",
      "\n",
      "Abstract:   In cooperation, the workers must know how co-workers behave. However, anagent's policy, which is embedded in a statistical machine learning model, ishard to understand, and requires much time and knowledge to comprehend.Therefore, it is difficult for people to predict the behavior of machinelearning robots, which makes Human Robot Cooperation challenging. In thispaper, we propose Instruction-based Behavior Explanation (IBE), a method toexplain an autonomous agent's future behavior. In IBE, an agent canautonomously acquire the expressions to explain its own behavior by reusing theinstructions given by a human expert to accelerate the learning of the agent'spolicy. IBE also enables a developmental agent, whose policy may change duringthe cooperation, to explain its own behavior with sufficient time granularity.\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "96716267cc1f47cda73f1f67668ec973",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating outputs:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True Title: Autonomous Self-Explanation of Behavior for Interactive Reinforcement\n",
      "  Learning Agents\n",
      "\n",
      "Predicted Title: A Instruction-Based Behavior Explanation\n",
      "\n",
      "Abstract:   In cooperation, the workers must know how co-workers behave. However, anagent's policy, which is embedded in a statistical machine learning model, ishard to understand, and requires much time and knowledge to comprehend.Therefore, it is difficult for people to predict the behavior of machinelearning robots, which makes Human Robot Cooperation challenging. In thispaper, we propose Instruction-based Behavior Explanation (IBE), a method toexplain an autonomous agent's future behavior. In IBE, an agent canautonomously acquire the expressions to explain its own behavior by reusing theinstructions given by a human expert to accelerate the learning of the agent'spolicy. IBE also enables a developmental agent, whose policy may change duringthe cooperation, to explain its own behavior with sufficient time granularity.\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "249b2e1129004708ba2ba17c63e08968",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating outputs:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True Title: Online Guest Detection in a Smart Home using Pervasive Sensors and\n",
      "  Probabilistic Reasoning\n",
      "\n",
      "Predicted Title: Aing the Number of People in a Smart Home at Each Time Step\n",
      "\n",
      "Abstract:   Smart home environments equipped with distributed sensor networks are capableof helping people by providing services related to health, emergency detectionor daily routine management. A backbone to these systems relies often on thesystem's ability to track and detect activities performed by the users in theirhome. Despite the continuous progress in the area of activity recognition insmart homes, many systems make a strong underlying assumption that the numberof occupants in the home at any given moment of time is always known.Estimating the number of persons in a Smart Home at each time step remains achallenge nowadays. Indeed, unlike most (crowd) counting solution which arebased on computer vision techniques, the sensors considered in a Smart Home areoften very simple and do not offer individually a good overview of thesituation. The data gathered needs therefore to be fused in order to inferuseful information. This paper aims at addressing this challenge and presents aprobabilistic approach able to estimate the number of persons in theenvironment at each time step. This approach works in two steps: first, anestimate of the number of persons present in the environment is done using aConstraint Satisfaction Problem solver, based on the topology of the sensornetwork and the sensor activation pattern at this time point. Then, a HiddenMarkov Model refines this estimate by considering the uncertainty related tothe sensors. Using both simulated and real data, our method has been tested andvalidated on two smart homes of different sizes and configuration anddemonstrates the ability to accurately estimate the number of inhabitants.\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "abb1e2c952634877960c1d26f28370da",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating outputs:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True Title: A Hybrid LP-RPG Heuristic for Modelling Numeric Resource Flows in\n",
      "  Planning\n",
      "\n",
      "Predicted Title: A-Based Hybrid Heuristic for numeric planning problems\n",
      "\n",
      "Abstract:   Although the use of metric fluents is fundamental to many practical planningproblems, the study of heuristics to support fully automated planners workingwith these fluents remains relatively unexplored. The most widely usedheuristic is the relaxation of metric fluents into interval-valued variables--- an idea first proposed a decade ago. Other heuristics depend on domainencodings that supply additional information about fluents, such as capacityconstraints or other resource-related annotations. A particular challenge tothese approaches is in handling interactions between metric fluents thatrepresent exchange, such as the transformation of quantities of raw materialsinto quantities of processed goods, or trading of money for materials. Theusual relaxation of metric fluents is often very poor in these situations,since it does not recognise that resources, once spent, are no longer availableto be spent again. We present a heuristic for numeric planning problemsbuilding on the propositional relaxed planning graph, but using a mathematicalprogram for numeric reasoning. We define a class of producer--consumer planningproblems and demonstrate how the numeric constraints in these can be modelledin a mixed integer program (MIP). This MIP is then combined with a metricRelaxed Planning Graph (RPG) heuristic to produce an integrated hybridheuristic. The MIP tracks resource use more accurately than the usualrelaxation, but relaxes the ordering of actions, while the RPG captures thecausal propositional aspects of the problem. We discuss how these twocomponents interact to produce a single unified heuristic and go on to explorehow further numeric features of planning problems can be integrated into theMIP. We show that encoding a limited subset of the propositional problem toaugment the MIP can yield more accurate guidance, partly by exploitingstructure such as propositional landmarks and propositional resources. Ourresults show that the use of this heuristic enhances scalability on problemswhere numeric resource interaction is key in finding a solution.\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8689d88974fb4a3a86aaf7a84f6003c8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating outputs:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True Title: Shiva: A Framework for Graph Based Ontology Matching\n",
      "\n",
      "Predicted Title: An ontologies for data matching\n",
      "\n",
      "Abstract:   Since long, corporations are looking for knowledge sources which can providestructured description of data and can focus on meaning and sharedunderstanding. Structures which can facilitate open world assumptions and canbe flexible enough to incorporate and recognize more than one name for anentity. A source whose major purpose is to facilitate human communication andinteroperability. Clearly, databases fail to provide these features andontologies have emerged as an alternative choice, but corporations working onsame domain tend to make different ontologies. The problem occurs when theywant to share their data/knowledge. Thus we need tools to merge ontologies intoone. This task is termed as ontology matching. This is an emerging area andstill we have to go a long way in having an ideal matcher which can producegood results. In this paper we have shown a framework to matching ontologiesusing graphs.\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6e0ba53e4e75488585e462b82344694f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating outputs:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True Title: Representing and Reasoning with Qualitative Preferences for\n",
      "  Compositional Systems\n",
      "\n",
      "Predicted Title: A dominance relation for the selection of most preferred collections\n",
      "\n",
      "Abstract:   Many applications, e.g., Web service composition, complex system design, teamformation, etc., rely on methods for identifying collections of objects orentities satisfying some functional requirement. Among the collections thatsatisfy the functional requirement, it is often necessary to identify one ormore collections that are optimal with respect to user preferences over a setof attributes that describe the non-functional properties of the collection.  We develop a formalism that lets users express the relative importance amongattributes and qualitative preferences over the valuations of each attribute.We define a dominance relation that allows us to compare collections of objectsin terms of preferences over attributes of the objects that make up thecollection. We establish some key properties of the dominance relation. Inparticular, we show that the dominance relation is a strict partial order whenthe intra-attribute preference relations are strict partial orders and therelative importance preference relation is an interval order.  We provide algorithms that use this dominance relation to identify the set ofmost preferred collections. We show that under certain conditions, thealgorithms are guaranteed to return only (sound), all (complete), or at leastone (weakly complete) of the most preferred collections. We present results ofsimulation experiments comparing the proposed algorithms with respect to (a)the quality of solutions (number of most preferred solutions) produced by thealgorithms, and (b) their performance and efficiency. We also explore someinteresting conjectures suggested by the results of our experiments that relatethe properties of the user preferences, the dominance relation, and thealgorithms.\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8e034a3e3af84427abc5cfef7aa90353",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating outputs:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True Title: Preference-Based Monte Carlo Tree Search\n",
      "\n",
      "Predicted Title: A Monte Carlo Tree Search: A Unique and Unique Approach\n",
      "\n",
      "Abstract:   Monte Carlo tree search (MCTS) is a popular choice for solving sequentialanytime problems. However, it depends on a numeric feedback signal, which canbe difficult to define. Real-time MCTS is a variant which may only rarelyencounter states with an explicit, extrinsic reward. To deal with such cases,the experimenter has to supply an additional numeric feedback signal in theform of a heuristic, which intrinsically guides the agent. Recent work hasshown evidence that in different areas the underlying structure is ordinal andnot numerical. Hence erroneous and biased heuristics are inevitable, especiallyin such domains. In this paper, we propose a MCTS variant which only depends onqualitative feedback, and therefore opens up new applications for MCTS. We alsofind indications that translating absolute into ordinal feedback may bebeneficial. Using a puzzle domain, we show that our preference-based MCTSvariant, wich only receives qualitative feedback, is able to reach aperformance level comparable to a regular MCTS baseline, which obtainsquantitative feedback.\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f3b3d6d9d4524a67b936663409c55dbe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating outputs:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True Title: Deep Learning Techniques for Geospatial Data Analysis\n",
      "\n",
      "Predicted Title: A Deep Learning for Remote Sensing Data Analysis\n",
      "\n",
      "Abstract:   Consumer electronic devices such as mobile handsets, goods tagged with RFIDlabels, location and position sensors are continuously generating a vast amountof location enriched data called geospatial data. Conventionally suchgeospatial data is used for military applications. In recent times, many usefulcivilian applications have been designed and deployed around such geospatialdata. For example, a recommendation system to suggest restaurants or places ofattraction to a tourist visiting a particular locality. At the same time, civicbodies are harnessing geospatial data generated through remote sensing devicesto provide better services to citizens such as traffic monitoring, potholeidentification, and weather reporting. Typically such applications areleveraged upon non-hierarchical machine learning techniques such as Naive-BayesClassifiers, Support Vector Machines, and decision trees. Recent advances inthe field of deep-learning showed that Neural Network-based techniquesoutperform conventional techniques and provide effective solutions for manygeospatial data analysis tasks such as object recognition, imageclassification, and scene understanding. The chapter presents a survey on thecurrent state of the applications of deep learning techniques for analyzinggeospatial data.  The chapter is organized as below: (i) A brief overview of deep learningalgorithms. (ii)Geospatial Analysis: a Data Science Perspective (iii)Deep-learning techniques for Remote Sensing data analytics tasks (iv)Deep-learning techniques for GPS data analytics(iv) Deep-learning techniquesfor RFID data analytics.\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f65f3da379b84eb3b88615ce34bb701b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating outputs:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True Title: Cortex simulation system proposal using distributed computer network\n",
      "  environments\n",
      "\n",
      "Predicted Title: A neuroscience: A New Approach\n",
      "\n",
      "Abstract:   In the dawn of computer science and the eve of neuroscience we participate inrebirth of neuroscience due to new technology that allows us to deeply andprecisely explore whole new world that dwells in our brains.\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1772a0eed33e4d25b59b2b19c04f4ed2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating outputs:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True Title: The SP theory of intelligence and the representation and processing of\n",
      "  knowledge in the brain\n",
      "\n",
      "Predicted Title: A SP theory of intelligence: An overview and partial model\n",
      "\n",
      "Abstract:   The \"SP theory of intelligence\", with its realisation in the \"SP computermodel\", aims to simplify and integrate observations and concepts acrossAI-related fields, with information compression as a unifying theme. This paperdescribes how abstract structures and processes in the theory may be realisedin terms of neurons, their interconnections, and the transmission of signalsbetween neurons. This part of the SP theory -- \"SP-neural\" -- is a tentativeand partial model for the representation and processing of knowledge in thebrain. In the SP theory (apart from SP-neural), all kinds of knowledge arerepresented with \"patterns\", where a pattern is an array of atomic symbols inone or two dimensions. In SP-neural, the concept of a \"pattern\" is realised asan array of neurons called a \"pattern assembly\", similar to Hebb's concept of a\"cell assembly\" but with important differences. Central to the processing ofinformation in the SP system is the powerful concept of \"multiple alignment\",borrowed and adapted from bioinformatics. Processes such as patternrecognition, reasoning and problem solving are achieved via the building ofmultiple alignments, while unsupervised learning -- significantly differentfrom the \"Hebbian\" kinds of learning -- is achieved by creating patterns fromsensory information and also by creating patterns from multiple alignments inwhich there is a partial match between one pattern and another. Short-livedneural structures equivalent to multiple alignments will be created via aninter-play of excitatory and inhibitory neural signals. The paper discussesseveral associated issues, with relevant empirical evidence.\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ed985ed48899480cbdee219025eb55fa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating outputs:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True Title: The Multi-Agent Reinforcement Learning in Malm\\\"O (MARL\\\"O) Competition\n",
      "\n",
      "Predicted Title: A Multi-Agent Reinforcement Learning in Malm\\\"O\n",
      "\n",
      "Abstract:   Learning in multi-agent scenarios is a fruitful research direction, butcurrent approaches still show scalability problems in multiple games withgeneral reward settings and different opponent types. The Multi-AgentReinforcement Learning in Malm\\\"O (MARL\\\"O) competition is a new challenge thatproposes research in this domain using multiple 3D games. The goal of thiscontest is to foster research in general agents that can learn across differentgames and opponent types, proposing a challenge as a milestone in the directionof Artificial General Intelligence.\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ba352d6b6072405aa3d939e7b4c3274a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating outputs:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True Title: Qualitative Approximate Behavior Composition\n",
      "\n",
      "Predicted Title: Aimating behavior composition without simulation\n",
      "\n",
      "Abstract:   The behavior composition problem involves automatically building a controllerthat is able to realize a desired, but unavailable, target system (e.g., ahouse surveillance) by suitably coordinating a set of available components(e.g., video cameras, blinds, lamps, a vacuum cleaner, phones, etc.) Previouswork has almost exclusively aimed at bringing about the desired component inits totality, which is highly unsatisfactory for unsolvable problems. In thiswork, we develop an approach for approximate behavior composition withoutdeparting from the classical setting, thus making the problem applicable to amuch wider range of cases. Based on the notion of simulation, we characterizewhat a maximal controller and the \"closest\" implementable target module(optimal approximation) are, and show how these can be computed using ATL modelchecking technology for a special case. We show the uniqueness of optimalapproximations, and prove their soundness and completeness with respect totheir imported controllers.\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f531a7ed764b4f68bbee21d1033f5436",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating outputs:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True Title: Towards an Ontology based integrated Framework for Semantic Web\n",
      "\n",
      "Predicted Title: A ontologies for heterogeneous data sources\n",
      "\n",
      "Abstract:   This Ontologies are widely used as a means for solving the informationheterogeneity problems on the web because of their capability to provideexplicit meaning to the information. They become an efficient tool forknowledge representation in a structured manner. There is always more than oneontology for the same domain. Furthermore, there is no standard method forbuilding ontologies, and there are many ontology building tools using differentontology languages. Because of these reasons, interoperability between theontologies is very low. Current ontology tools mostly use functions to build,edit and inference the ontology. Methods for merging heterogeneous domainontologies are not included in most tools. This paper presents ontology mergingmethodology for building a single global ontology from heterogeneous eXtensibleMarkup Language (XML) data sources to capture and maintain all the knowledgewhich XML data sources can contain\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0092a63daecb4ce38971c085b0a00ca7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating outputs:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True Title: Learning STRIPS Action Models with Classical Planning\n",
      "\n",
      "Predicted Title: Acompiling STRIPS action models from a classical planning task\n",
      "\n",
      "Abstract:   This paper presents a novel approach for learning STRIPS action models fromexamples that compiles this inductive learning task into a classical planningtask. Interestingly, the compilation approach is flexible to different amountsof available input knowledge; the learning examples can range from a set ofplans (with their corresponding initial and final states) to just a pair ofinitial and final states (no intermediate action or state is given). Moreover,the compilation accepts partially specified action models and it can be used tovalidate whether the observation of a plan execution follows a given STRIPSaction model, even if this model is not fully specified.\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "47c50a86b6da4edeb71ec3c535c5ee25",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating outputs:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True Title: The SP theory of intelligence and the representation and processing of\n",
      "  knowledge in the brain\n",
      "\n",
      "Predicted Title: A SP theory of intelligence: An overview and partial model\n",
      "\n",
      "Abstract:   The \"SP theory of intelligence\", with its realisation in the \"SP computermodel\", aims to simplify and integrate observations and concepts acrossAI-related fields, with information compression as a unifying theme. This paperdescribes how abstract structures and processes in the theory may be realisedin terms of neurons, their interconnections, and the transmission of signalsbetween neurons. This part of the SP theory -- \"SP-neural\" -- is a tentativeand partial model for the representation and processing of knowledge in thebrain. In the SP theory (apart from SP-neural), all kinds of knowledge arerepresented with \"patterns\", where a pattern is an array of atomic symbols inone or two dimensions. In SP-neural, the concept of a \"pattern\" is realised asan array of neurons called a \"pattern assembly\", similar to Hebb's concept of a\"cell assembly\" but with important differences. Central to the processing ofinformation in the SP system is the powerful concept of \"multiple alignment\",borrowed and adapted from bioinformatics. Processes such as patternrecognition, reasoning and problem solving are achieved via the building ofmultiple alignments, while unsupervised learning -- significantly differentfrom the \"Hebbian\" kinds of learning -- is achieved by creating patterns fromsensory information and also by creating patterns from multiple alignments inwhich there is a partial match between one pattern and another. Short-livedneural structures equivalent to multiple alignments will be created via aninter-play of excitatory and inhibitory neural signals. The paper discussesseveral associated issues, with relevant empirical evidence.\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a04dd13e064b4536a17c8ddc7af321c1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating outputs:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True Title: The emergence of Explainability of Intelligent Systems: Delivering\n",
      "  Explainable and Personalised Recommendations for Energy Efficiency\n",
      "\n",
      "Predicted Title: Aation Systems for Energy Efficiency\n",
      "\n",
      "Abstract:   The recent advances in artificial intelligence namely in machine learning anddeep learning, have boosted the performance of intelligent systems in severalways. This gave rise to human expectations, but also created the need for adeeper understanding of how intelligent systems think and decide. The conceptof explainability appeared, in the extent of explaining the internal systemmechanics in human terms. Recommendation systems are intelligent systems thatsupport human decision making, and as such, they have to be explainable inorder to increase user trust and improve the acceptance of recommendations. Inthis work, we focus on a context-aware recommendation system for energyefficiency and develop a mechanism for explainable and persuasiverecommendations, which are personalized to user preferences and habits. Thepersuasive facts either emphasize on the economical saving prospects (Econ) oron a positive ecological impact (Eco) and explanations provide the reason forrecommending an energy saving action. Based on a study conducted using aTelegram bot, different scenarios have been validated with actual data andhuman feedback. Current results show a total increase of 19\\% on therecommendation acceptance ratio when both economical and ecological persuasivefacts are employed. This revolutionary approach on recommendation systems,demonstrates how intelligent recommendations can effectively encourage energysaving behavior.\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6922afcd631542a0b1f796acb21f0cc4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating outputs:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True Title: A Hybrid LP-RPG Heuristic for Modelling Numeric Resource Flows in\n",
      "  Planning\n",
      "\n",
      "Predicted Title: A-Based Hybrid Heuristic for numeric planning problems\n",
      "\n",
      "Abstract:   Although the use of metric fluents is fundamental to many practical planningproblems, the study of heuristics to support fully automated planners workingwith these fluents remains relatively unexplored. The most widely usedheuristic is the relaxation of metric fluents into interval-valued variables--- an idea first proposed a decade ago. Other heuristics depend on domainencodings that supply additional information about fluents, such as capacityconstraints or other resource-related annotations. A particular challenge tothese approaches is in handling interactions between metric fluents thatrepresent exchange, such as the transformation of quantities of raw materialsinto quantities of processed goods, or trading of money for materials. Theusual relaxation of metric fluents is often very poor in these situations,since it does not recognise that resources, once spent, are no longer availableto be spent again. We present a heuristic for numeric planning problemsbuilding on the propositional relaxed planning graph, but using a mathematicalprogram for numeric reasoning. We define a class of producer--consumer planningproblems and demonstrate how the numeric constraints in these can be modelledin a mixed integer program (MIP). This MIP is then combined with a metricRelaxed Planning Graph (RPG) heuristic to produce an integrated hybridheuristic. The MIP tracks resource use more accurately than the usualrelaxation, but relaxes the ordering of actions, while the RPG captures thecausal propositional aspects of the problem. We discuss how these twocomponents interact to produce a single unified heuristic and go on to explorehow further numeric features of planning problems can be integrated into theMIP. We show that encoding a limited subset of the propositional problem toaugment the MIP can yield more accurate guidance, partly by exploitingstructure such as propositional landmarks and propositional resources. Ourresults show that the use of this heuristic enhances scalability on problemswhere numeric resource interaction is key in finding a solution.\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3e86420e5cb04ddabbc417165e40aaa0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating outputs:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True Title: Preference-Based Monte Carlo Tree Search\n",
      "\n",
      "Predicted Title: A Monte Carlo Tree Search: A Unique and Unique Approach\n",
      "\n",
      "Abstract:   Monte Carlo tree search (MCTS) is a popular choice for solving sequentialanytime problems. However, it depends on a numeric feedback signal, which canbe difficult to define. Real-time MCTS is a variant which may only rarelyencounter states with an explicit, extrinsic reward. To deal with such cases,the experimenter has to supply an additional numeric feedback signal in theform of a heuristic, which intrinsically guides the agent. Recent work hasshown evidence that in different areas the underlying structure is ordinal andnot numerical. Hence erroneous and biased heuristics are inevitable, especiallyin such domains. In this paper, we propose a MCTS variant which only depends onqualitative feedback, and therefore opens up new applications for MCTS. We alsofind indications that translating absolute into ordinal feedback may bebeneficial. Using a puzzle domain, we show that our preference-based MCTSvariant, wich only receives qualitative feedback, is able to reach aperformance level comparable to a regular MCTS baseline, which obtainsquantitative feedback.\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3ec6aec9be2742a3b6bb86d58a24b5cc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating outputs:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True Title: A Directional Feature with Energy based Offline Signature Verification\n",
      "  Network\n",
      "\n",
      "Predicted Title: Aofflinesignature verification system\n",
      "\n",
      "Abstract:   Signature used as a biometric is implemented in various systems as well asevery signature signed by each person is distinct at the same time. So, it isvery important to have a computerized signature verification system. In offlinesignature verification system dynamic features are not available obviously, butone can use a signature as an image and apply image processing techniques tomake an effective offline signature verification system. Author proposes aintelligent network used directional feature and energy density both as inputsto the same network and classifies the signature. Neural network is used as aclassifier for this system. The results are compared with both the very basicenergy density method and a simple directional feature method of offlinesignature verification system and this proposed new network is found veryeffective as compared to the above two methods, specially for less number oftraining samples, which can be implemented practically.\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3260717766ca49c089171c9e58f04bec",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating outputs:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True Title: Autonomous Self-Explanation of Behavior for Interactive Reinforcement\n",
      "  Learning Agents\n",
      "\n",
      "Predicted Title: A Instruction-Based Behavior Explanation\n",
      "\n",
      "Abstract:   In cooperation, the workers must know how co-workers behave. However, anagent's policy, which is embedded in a statistical machine learning model, ishard to understand, and requires much time and knowledge to comprehend.Therefore, it is difficult for people to predict the behavior of machinelearning robots, which makes Human Robot Cooperation challenging. In thispaper, we propose Instruction-based Behavior Explanation (IBE), a method toexplain an autonomous agent's future behavior. In IBE, an agent canautonomously acquire the expressions to explain its own behavior by reusing theinstructions given by a human expert to accelerate the learning of the agent'spolicy. IBE also enables a developmental agent, whose policy may change duringthe cooperation, to explain its own behavior with sufficient time granularity.\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8eec8117eecb4ebebd61c2989a4d4b64",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating outputs:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True Title: A note on the complexity of the causal ordering problem\n",
      "\n",
      "Predicted Title: A causal ordering: A concise report on the complexity of the causalordering problem\n",
      "\n",
      "Abstract:   In this note we provide a concise report on the complexity of the causalordering problem, originally introduced by Simon to reason about causaldependencies implicit in systems of mathematical equations. We show thatSimon's classical algorithm to infer causal ordering is NP-Hard---anintractability previously guessed but never proven. We present then a detailedaccount based on Nayak's suggested algorithmic solution (the best available),which is dominated by computing transitive closure---bounded in time by$O(|\\mathcal V|\\cdot |\\mathcal S|)$, where $\\mathcal S(\\mathcal E, \\mathcal V)$is the input system structure composed of a set $\\mathcal E$ of equations overa set $\\mathcal V$ of variables with number of variable appearances (density)$|\\mathcal S|$. We also comment on the potential of causal ordering foremerging applications in large-scale hypothesis management and analytics.\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8b0d7137df774e89bab377fc853028e7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating outputs:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True Title: Adding Context to Concept Trees\n",
      "\n",
      "Predicted Title: A Concept Trees: A Concept Base\n",
      "\n",
      "Abstract:   A Concept Tree is a structure for storing knowledge where the trees arestored in a database called a Concept Base. It sits between the highlydistributed neural architectures and the distributed information systems, withthe intention of bringing brain-like and computer systems closer together.Concept Trees can grow from the semi-structured sources when consistentsequences of concepts are presented. Each tree ideally represents a singlecohesive concept and the trees can link with each other for navigation andsemantic purposes. The trees are therefore also a type of semantic network andwould benefit from having a consistent level of context for each node. Aconsistent build process is managed through a 'counting rule' and some otherrules that can normalise the database structure. This restricted structure canthen be complimented and enriched by the more dynamic context. It is alsosuggested to use the linking structure of the licas system [15] as a basis forthe context links, where the mathematical model is extended further to definethis. A number of tests have demonstrated the soundness of the architecture.Building the trees from text documents shows that the tree structure could beinherent in natural language. Then, two types of query language are described.Both of these can perform consistent query processes to return knowledge to theuser and even enhance the query with new knowledge. This is supported evenfurther with direct comparisons to a cognitive model, also being developed bythe author.\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d299db7ecb39497b89a5d0208af682a9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating outputs:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True Title: A Comparative Study on Parameter Estimation in Software Reliability\n",
      "  Modeling using Swarm Intelligence\n",
      "\n",
      "Predicted Title: Anfficient Swarm Optimization and Firefly Algorithm\n",
      "\n",
      "Abstract:   This work focuses on a comparison between the performances of two well-knownSwarm algorithms: Cuckoo Search (CS) and Firefly Algorithm (FA), in estimatingthe parameters of Software Reliability Growth Models. This study is furtherreinforced using Particle Swarm Optimization (PSO) and Ant Colony Optimization(ACO). All algorithms are evaluated according to real software failure data,the tests are performed and the obtained results are compared to show theperformance of each of the used algorithms. Furthermore, CS and FA are alsocompared with each other on bases of execution time and iteration number.Experimental results show that CS is more efficient in estimating theparameters of SRGMs, and it has outperformed FA in addition to PSO and ACO forthe selected Data sets and employed models.\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eb77c0ca29db467796156c2b3f35fb12",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating outputs:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True Title: Representing and Reasoning with Qualitative Preferences for\n",
      "  Compositional Systems\n",
      "\n",
      "Predicted Title: A dominance relation for the selection of most preferred collections\n",
      "\n",
      "Abstract:   Many applications, e.g., Web service composition, complex system design, teamformation, etc., rely on methods for identifying collections of objects orentities satisfying some functional requirement. Among the collections thatsatisfy the functional requirement, it is often necessary to identify one ormore collections that are optimal with respect to user preferences over a setof attributes that describe the non-functional properties of the collection.  We develop a formalism that lets users express the relative importance amongattributes and qualitative preferences over the valuations of each attribute.We define a dominance relation that allows us to compare collections of objectsin terms of preferences over attributes of the objects that make up thecollection. We establish some key properties of the dominance relation. Inparticular, we show that the dominance relation is a strict partial order whenthe intra-attribute preference relations are strict partial orders and therelative importance preference relation is an interval order.  We provide algorithms that use this dominance relation to identify the set ofmost preferred collections. We show that under certain conditions, thealgorithms are guaranteed to return only (sound), all (complete), or at leastone (weakly complete) of the most preferred collections. We present results ofsimulation experiments comparing the proposed algorithms with respect to (a)the quality of solutions (number of most preferred solutions) produced by thealgorithms, and (b) their performance and efficiency. We also explore someinteresting conjectures suggested by the results of our experiments that relatethe properties of the user preferences, the dominance relation, and thealgorithms.\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "625de57a88d5487f9cda3c998f977427",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating outputs:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True Title: The SP theory of intelligence and the representation and processing of\n",
      "  knowledge in the brain\n",
      "\n",
      "Predicted Title: A SP theory of intelligence: An overview and partial model\n",
      "\n",
      "Abstract:   The \"SP theory of intelligence\", with its realisation in the \"SP computermodel\", aims to simplify and integrate observations and concepts acrossAI-related fields, with information compression as a unifying theme. This paperdescribes how abstract structures and processes in the theory may be realisedin terms of neurons, their interconnections, and the transmission of signalsbetween neurons. This part of the SP theory -- \"SP-neural\" -- is a tentativeand partial model for the representation and processing of knowledge in thebrain. In the SP theory (apart from SP-neural), all kinds of knowledge arerepresented with \"patterns\", where a pattern is an array of atomic symbols inone or two dimensions. In SP-neural, the concept of a \"pattern\" is realised asan array of neurons called a \"pattern assembly\", similar to Hebb's concept of a\"cell assembly\" but with important differences. Central to the processing ofinformation in the SP system is the powerful concept of \"multiple alignment\",borrowed and adapted from bioinformatics. Processes such as patternrecognition, reasoning and problem solving are achieved via the building ofmultiple alignments, while unsupervised learning -- significantly differentfrom the \"Hebbian\" kinds of learning -- is achieved by creating patterns fromsensory information and also by creating patterns from multiple alignments inwhich there is a partial match between one pattern and another. Short-livedneural structures equivalent to multiple alignments will be created via aninter-play of excitatory and inhibitory neural signals. The paper discussesseveral associated issues, with relevant empirical evidence.\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a3784257aaed4a858a994a059ee904a0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating outputs:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True Title: Modeling Variations of First-Order Horn Abduction in Answer Set\n",
      "  Programming\n",
      "\n",
      "Predicted Title: A Answer Set Programming: An Approach to Abduction in First Order Horn Logic\n",
      "\n",
      "Abstract:   We study abduction in First Order Horn logic theories where all atoms can beabduced and we are looking for preferred solutions with respect to threeobjective functions: cardinality minimality, coherence, and weighted abduction.We represent this reasoning problem in Answer Set Programming (ASP), in orderto obtain a flexible framework for experimenting with global constraints andobjective functions, and to test the boundaries of what is possible with ASP.Realizing this problem in ASP is challenging as it requires value invention andequivalence between certain constants, because the Unique Names Assumption doesnot hold in general. To permit reasoning in cyclic theories, we formallydescribe fine-grained variations of limiting Skolemization. We identify termequivalence as a main instantiation bottleneck, and improve the efficiency ofour approach with on-demand constraints that were used to eliminate the samebottleneck in state-of-the-art solvers. We evaluate our approach experimentallyon the ACCEL benchmark for plan recognition in Natural Language Understanding.Our encodings are publicly available, modular, and our approach is moreefficient than state-of-the-art solvers on the ACCEL benchmark.\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a31e7f928adf453dbf8c179cdcb3386a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating outputs:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True Title: Contextual Symmetries in Probabilistic Graphical Models\n",
      "\n",
      "Predicted Title: A Contextual symmetries\n",
      "\n",
      "Abstract:   An important approach for efficient inference in probabilistic graphicalmodels exploits symmetries among objects in the domain. Symmetric variables(states) are collapsed into meta-variables (meta-states) and inferencealgorithms are run over the lifted graphical model instead of the flat one. Ourpaper extends existing definitions of symmetry by introducing the novel notionof contextual symmetry. Two states that are not globally symmetric, can becontextually symmetric under some specific assignment to a subset of variables,referred to as the context variables. Contextual symmetry subsumes previoussymmetry definitions and can rep resent a large class of symmetries notrepresentable earlier. We show how to compute contextual symmetries by reducingit to the problem of graph isomorphism. We extend previous work on exploitingsymmetries in the MCMC framework to the case of contextual symmetries. Ourexperiments on several domains of interest demonstrate that exploitingcontextual symmetries can result in significant computational gains.\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a2a3eae4a02f4a5f9d539194556392b9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating outputs:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True Title: Modeling Variations of First-Order Horn Abduction in Answer Set\n",
      "  Programming\n",
      "\n",
      "Predicted Title: A Answer Set Programming: An Approach to Abduction in First Order Horn Logic\n",
      "\n",
      "Abstract:   We study abduction in First Order Horn logic theories where all atoms can beabduced and we are looking for preferred solutions with respect to threeobjective functions: cardinality minimality, coherence, and weighted abduction.We represent this reasoning problem in Answer Set Programming (ASP), in orderto obtain a flexible framework for experimenting with global constraints andobjective functions, and to test the boundaries of what is possible with ASP.Realizing this problem in ASP is challenging as it requires value invention andequivalence between certain constants, because the Unique Names Assumption doesnot hold in general. To permit reasoning in cyclic theories, we formallydescribe fine-grained variations of limiting Skolemization. We identify termequivalence as a main instantiation bottleneck, and improve the efficiency ofour approach with on-demand constraints that were used to eliminate the samebottleneck in state-of-the-art solvers. We evaluate our approach experimentallyon the ACCEL benchmark for plan recognition in Natural Language Understanding.Our encodings are publicly available, modular, and our approach is moreefficient than state-of-the-art solvers on the ACCEL benchmark.\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2d1a6bd3163d4091b52a9594bdbfbfac",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating outputs:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True Title: Qualitative Approximate Behavior Composition\n",
      "\n",
      "Predicted Title: Aimating behavior composition without simulation\n",
      "\n",
      "Abstract:   The behavior composition problem involves automatically building a controllerthat is able to realize a desired, but unavailable, target system (e.g., ahouse surveillance) by suitably coordinating a set of available components(e.g., video cameras, blinds, lamps, a vacuum cleaner, phones, etc.) Previouswork has almost exclusively aimed at bringing about the desired component inits totality, which is highly unsatisfactory for unsolvable problems. In thiswork, we develop an approach for approximate behavior composition withoutdeparting from the classical setting, thus making the problem applicable to amuch wider range of cases. Based on the notion of simulation, we characterizewhat a maximal controller and the \"closest\" implementable target module(optimal approximation) are, and show how these can be computed using ATL modelchecking technology for a special case. We show the uniqueness of optimalapproximations, and prove their soundness and completeness with respect totheir imported controllers.\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cd3c6f52f936463bb983c9be6b595c77",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating outputs:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True Title: Representing and Reasoning with Qualitative Preferences for\n",
      "  Compositional Systems\n",
      "\n",
      "Predicted Title: A dominance relation for the selection of most preferred collections\n",
      "\n",
      "Abstract:   Many applications, e.g., Web service composition, complex system design, teamformation, etc., rely on methods for identifying collections of objects orentities satisfying some functional requirement. Among the collections thatsatisfy the functional requirement, it is often necessary to identify one ormore collections that are optimal with respect to user preferences over a setof attributes that describe the non-functional properties of the collection.  We develop a formalism that lets users express the relative importance amongattributes and qualitative preferences over the valuations of each attribute.We define a dominance relation that allows us to compare collections of objectsin terms of preferences over attributes of the objects that make up thecollection. We establish some key properties of the dominance relation. Inparticular, we show that the dominance relation is a strict partial order whenthe intra-attribute preference relations are strict partial orders and therelative importance preference relation is an interval order.  We provide algorithms that use this dominance relation to identify the set ofmost preferred collections. We show that under certain conditions, thealgorithms are guaranteed to return only (sound), all (complete), or at leastone (weakly complete) of the most preferred collections. We present results ofsimulation experiments comparing the proposed algorithms with respect to (a)the quality of solutions (number of most preferred solutions) produced by thealgorithms, and (b) their performance and efficiency. We also explore someinteresting conjectures suggested by the results of our experiments that relatethe properties of the user preferences, the dominance relation, and thealgorithms.\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d22b11952b6c410da13151fcd0eb5e67",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating outputs:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True Title: A Modified Vortex Search Algorithm for Numerical Function Optimization\n",
      "\n",
      "Predicted Title: Avortex Search: A Modification of Vortex Search\n",
      "\n",
      "Abstract:   The Vortex Search (VS) algorithm is one of the recently proposedmetaheuristic algorithms which was inspired from the vortical flow of thestirred fluids. Although the VS algorithm is shown to be a good candidate forthe solution of certain optimization problems, it also has some drawbacks. Inthe VS algorithm, candidate solutions are generated around the current bestsolution by using a Gaussian distribution at each iteration pass. This providessimplicity to the algorithm but it also leads to some problems along.Especially, for the functions those have a number of local minimum points, toselect a single point to generate candidate solutions leads the algorithm tobeing trapped into a local minimum point. Due to the adaptive step-sizeadjustment scheme used in the VS algorithm, the locality of the createdcandidate solutions is increased at each iteration pass. Therefore, if thealgorithm cannot escape a local point as quickly as possible, it becomes muchmore difficult for the algorithm to escape from that point in the latteriterations. In this study, a modified Vortex Search algorithm (MVS) is proposedto overcome above mentioned drawback of the existing VS algorithm. In the MVSalgorithm, the candidate solutions are generated around a number of points ateach iteration pass. Computational results showed that with the help of thismodification the global search ability of the existing VS algorithm is improvedand the MVS algorithm outperformed the existing VS algorithm, PSO2011 and ABCalgorithms for the benchmark numerical function set.\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b09d55c5ef644ed5a3eb79b8813710e7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating outputs:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True Title: Shiva: A Framework for Graph Based Ontology Matching\n",
      "\n",
      "Predicted Title: An ontologies for data matching\n",
      "\n",
      "Abstract:   Since long, corporations are looking for knowledge sources which can providestructured description of data and can focus on meaning and sharedunderstanding. Structures which can facilitate open world assumptions and canbe flexible enough to incorporate and recognize more than one name for anentity. A source whose major purpose is to facilitate human communication andinteroperability. Clearly, databases fail to provide these features andontologies have emerged as an alternative choice, but corporations working onsame domain tend to make different ontologies. The problem occurs when theywant to share their data/knowledge. Thus we need tools to merge ontologies intoone. This task is termed as ontology matching. This is an emerging area andstill we have to go a long way in having an ideal matcher which can producegood results. In this paper we have shown a framework to matching ontologiesusing graphs.\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f3fcabef7c53465990c5ffaeb6ac57c1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating outputs:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True Title: A note on the complexity of the causal ordering problem\n",
      "\n",
      "Predicted Title: A causal ordering: A concise report on the complexity of the causalordering problem\n",
      "\n",
      "Abstract:   In this note we provide a concise report on the complexity of the causalordering problem, originally introduced by Simon to reason about causaldependencies implicit in systems of mathematical equations. We show thatSimon's classical algorithm to infer causal ordering is NP-Hard---anintractability previously guessed but never proven. We present then a detailedaccount based on Nayak's suggested algorithmic solution (the best available),which is dominated by computing transitive closure---bounded in time by$O(|\\mathcal V|\\cdot |\\mathcal S|)$, where $\\mathcal S(\\mathcal E, \\mathcal V)$is the input system structure composed of a set $\\mathcal E$ of equations overa set $\\mathcal V$ of variables with number of variable appearances (density)$|\\mathcal S|$. We also comment on the potential of causal ordering foremerging applications in large-scale hypothesis management and analytics.\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "069ef99dcffc4dbdb7b2089de40d4749",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating outputs:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True Title: Online Guest Detection in a Smart Home using Pervasive Sensors and\n",
      "  Probabilistic Reasoning\n",
      "\n",
      "Predicted Title: Aing the Number of People in a Smart Home at Each Time Step\n",
      "\n",
      "Abstract:   Smart home environments equipped with distributed sensor networks are capableof helping people by providing services related to health, emergency detectionor daily routine management. A backbone to these systems relies often on thesystem's ability to track and detect activities performed by the users in theirhome. Despite the continuous progress in the area of activity recognition insmart homes, many systems make a strong underlying assumption that the numberof occupants in the home at any given moment of time is always known.Estimating the number of persons in a Smart Home at each time step remains achallenge nowadays. Indeed, unlike most (crowd) counting solution which arebased on computer vision techniques, the sensors considered in a Smart Home areoften very simple and do not offer individually a good overview of thesituation. The data gathered needs therefore to be fused in order to inferuseful information. This paper aims at addressing this challenge and presents aprobabilistic approach able to estimate the number of persons in theenvironment at each time step. This approach works in two steps: first, anestimate of the number of persons present in the environment is done using aConstraint Satisfaction Problem solver, based on the topology of the sensornetwork and the sensor activation pattern at this time point. Then, a HiddenMarkov Model refines this estimate by considering the uncertainty related tothe sensors. Using both simulated and real data, our method has been tested andvalidated on two smart homes of different sizes and configuration anddemonstrates the ability to accurately estimate the number of inhabitants.\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "03f9bf41ff694a7497eb5e44e34188e5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating outputs:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True Title: Evaluating Go Game Records for Prediction of Player Attributes\n",
      "\n",
      "Predicted Title: A Evaluation of Go Game Data\n",
      "\n",
      "Abstract:   We propose a way of extracting and aggregating per-move evaluations from setsof Go game records. The evaluations capture different aspects of the games suchas played patterns or statistic of sente/gote sequences. Using machine learningalgorithms, the evaluations can be utilized to predict different relevanttarget variables. We apply this methodology to predict the strength and playingstyle of the player (e.g. territoriality or aggressivity) with good accuracy.We propose a number of possible applications including aiding in Go study,seeding real-work ranks of internet players or tuning of Go-playing programs.\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ef0c7acebc4040de8676b800823e5f54",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating outputs:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True Title: The Multi-Agent Reinforcement Learning in Malm\\\"O (MARL\\\"O) Competition\n",
      "\n",
      "Predicted Title: A Multi-Agent Reinforcement Learning in Malm\\\"O\n",
      "\n",
      "Abstract:   Learning in multi-agent scenarios is a fruitful research direction, butcurrent approaches still show scalability problems in multiple games withgeneral reward settings and different opponent types. The Multi-AgentReinforcement Learning in Malm\\\"O (MARL\\\"O) competition is a new challenge thatproposes research in this domain using multiple 3D games. The goal of thiscontest is to foster research in general agents that can learn across differentgames and opponent types, proposing a challenge as a milestone in the directionof Artificial General Intelligence.\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f649509b3ef04f6189c05a307bfa7233",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating outputs:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True Title: Self-Organizing Maps for Storage and Transfer of Knowledge in\n",
      "  Reinforcement Learning\n",
      "\n",
      "Predicted Title: A-based Map for Reusing and Transfer Knowledge from Previously Learned Task Knowledge\n",
      "\n",
      "Abstract:   The idea of reusing or transferring information from previously learned tasks(source tasks) for the learning of new tasks (target tasks) has the potentialto significantly improve the sample efficiency of a reinforcement learningagent. In this work, we describe a novel approach for reusing previouslyacquired knowledge by using it to guide the exploration of an agent while itlearns new tasks. In order to do so, we employ a variant of the growingself-organizing map algorithm, which is trained using a measure of similaritythat is defined directly in the space of the vectorized representations of thevalue functions. In addition to enabling transfer across tasks, the resultingmap is simultaneously used to enable the efficient storage of previouslyacquired task knowledge in an adaptive and scalable manner. We empiricallyvalidate our approach in a simulated navigation environment, and alsodemonstrate its utility through simple experiments using a mobilemicro-robotics platform. In addition, we demonstrate the scalability of thisapproach, and analytically examine its relation to the proposed network growthmechanism. Further, we briefly discuss some of the possible improvements andextensions to this approach, as well as its relevance to real world scenariosin the context of continual learning.\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d80e09284de440ae88c1d9993721e83a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating outputs:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True Title: Shiva++: An Enhanced Graph based Ontology Matcher\n",
      "\n",
      "Predicted Title: Anology based systems for knowledge management\n",
      "\n",
      "Abstract:   With the web getting bigger and assimilating knowledge about differentconcepts and domains, it is becoming very difficult for simple database drivenapplications to capture the data for a domain. Thus developers have come outwith ontology based systems which can store large amount of information and canapply reasoning and produce timely information. Thus facilitating effectiveknowledge management. Though this approach has made our lives easier, but atthe same time has given rise to another problem. Two different ontologiesassimilating same knowledge tend to use different terms for the same concepts.This creates confusion among knowledge engineers and workers, as they do notknow which is a better term then the other. Thus we need to merge ontologiesworking on same domain so that the engineers can develop a better applicationover it. This paper shows the development of one such matcher which merges theconcepts available in two ontologies at two levels; 1) at string level and 2)at semantic level; thus producing better merged ontologies. We have used agraph matching technique which works at the core of the system. We have alsoevaluated the system and have tested its performance with its predecessor whichworks only on string matching. Thus current approach produces better results.\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ae609c23ef5d43ae9525d5313a7a9a97",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating outputs:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True Title: Contextual Symmetries in Probabilistic Graphical Models\n",
      "\n",
      "Predicted Title: A Contextual symmetries\n",
      "\n",
      "Abstract:   An important approach for efficient inference in probabilistic graphicalmodels exploits symmetries among objects in the domain. Symmetric variables(states) are collapsed into meta-variables (meta-states) and inferencealgorithms are run over the lifted graphical model instead of the flat one. Ourpaper extends existing definitions of symmetry by introducing the novel notionof contextual symmetry. Two states that are not globally symmetric, can becontextually symmetric under some specific assignment to a subset of variables,referred to as the context variables. Contextual symmetry subsumes previoussymmetry definitions and can rep resent a large class of symmetries notrepresentable earlier. We show how to compute contextual symmetries by reducingit to the problem of graph isomorphism. We extend previous work on exploitingsymmetries in the MCMC framework to the case of contextual symmetries. Ourexperiments on several domains of interest demonstrate that exploitingcontextual symmetries can result in significant computational gains.\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5086124900a24cb79f11196fada2c57f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating outputs:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True Title: A Hybrid LP-RPG Heuristic for Modelling Numeric Resource Flows in\n",
      "  Planning\n",
      "\n",
      "Predicted Title: A-Based Hybrid Heuristic for numeric planning problems\n",
      "\n",
      "Abstract:   Although the use of metric fluents is fundamental to many practical planningproblems, the study of heuristics to support fully automated planners workingwith these fluents remains relatively unexplored. The most widely usedheuristic is the relaxation of metric fluents into interval-valued variables--- an idea first proposed a decade ago. Other heuristics depend on domainencodings that supply additional information about fluents, such as capacityconstraints or other resource-related annotations. A particular challenge tothese approaches is in handling interactions between metric fluents thatrepresent exchange, such as the transformation of quantities of raw materialsinto quantities of processed goods, or trading of money for materials. Theusual relaxation of metric fluents is often very poor in these situations,since it does not recognise that resources, once spent, are no longer availableto be spent again. We present a heuristic for numeric planning problemsbuilding on the propositional relaxed planning graph, but using a mathematicalprogram for numeric reasoning. We define a class of producer--consumer planningproblems and demonstrate how the numeric constraints in these can be modelledin a mixed integer program (MIP). This MIP is then combined with a metricRelaxed Planning Graph (RPG) heuristic to produce an integrated hybridheuristic. The MIP tracks resource use more accurately than the usualrelaxation, but relaxes the ordering of actions, while the RPG captures thecausal propositional aspects of the problem. We discuss how these twocomponents interact to produce a single unified heuristic and go on to explorehow further numeric features of planning problems can be integrated into theMIP. We show that encoding a limited subset of the propositional problem toaugment the MIP can yield more accurate guidance, partly by exploitingstructure such as propositional landmarks and propositional resources. Ourresults show that the use of this heuristic enhances scalability on problemswhere numeric resource interaction is key in finding a solution.\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e7667cd94da6431bbbdf7cb568f08120",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating outputs:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True Title: A Fuzzy AHP Approach for Supplier Selection Problem: A Case Study in a\n",
      "  Gear Motor Company\n",
      "\n",
      "Predicted Title: A Fuzzy AHP for Suuplier Selection\n",
      "\n",
      "Abstract:   Suuplier selection is one of the most important functions of a purchasingdepartment. Since by deciding the best supplier, companies can save materialcosts and increase competitive advantage.However this decision becomescompilcated in case of multiple suppliers, multiple conflicting criteria, andimprecise parameters. In addition the uncertainty and vagueness of the experts'opinion is the prominent characteristic of the problem. therefore anextensively used multi criteria decision making tool Fuzzy AHP can be utilizedas an approach for supplier selection problem. This paper reveals theapplication of Fuzzy AHP in a gear motor company determining the best supplierwith respect to selected criteria. the contribution of this study is not onlythe application of the Fuzzy AHP methodology for supplier selection problem,but also releasing a comprehensive literature review of multi criteria decisionmaking problems. In addition by stating the steps of Fuzzy AHP clearly andnumerically, this study can be a guide of the methodology to be implemented toother multiple criteria decision making problems.\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "99cc1f54f7bd4d9eab79d13560b99f84",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating outputs:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True Title: SAPFOCS: a metaheuristic based approach to part family formation\n",
      "  problems in group technology\n",
      "\n",
      "Predicted Title: A Part Family Formation in GroupTechnology\n",
      "\n",
      "Abstract:   This article deals with Part family formation problem which is believed to bemoderately complicated to be solved in polynomial time in the vicinity of GroupTechnology (GT). In the past literature researchers investigated that the partfamily formation techniques are principally based on production flow analysis(PFA) which usually considers operational requirements, sequences and time.Part Coding Analysis (PCA) is merely considered in GT which is believed to bethe proficient method to identify the part families. PCA classifies parts byallotting them to different families based on their resemblances in: (1) designcharacteristics such as shape and size, and/or (2) manufacturingcharacteristics (machining requirements). A novel approach based on simulatedannealing namely SAPFOCS is adopted in this study to develop effective partfamilies exploiting the PCA technique. Thereafter Taguchi's orthogonal designmethod is employed to solve the critical issues on the subject of parametersselection for the proposed metaheuristic algorithm. The adopted technique istherefore tested on 5 different datasets of size 5 {\\times} 9 to 27 {\\times} 9and the obtained results are compared with C-Linkage clustering technique. Theexperimental results reported that the proposed metaheuristic algorithm isextremely effective in terms of the quality of the solution obtained and hasoutperformed C-Linkage algorithm in most instances.\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2ddd17b44ef349dbbb4b3a3f0edf6c5b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating outputs:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True Title: Modeling Variations of First-Order Horn Abduction in Answer Set\n",
      "  Programming\n",
      "\n",
      "Predicted Title: A Answer Set Programming: An Approach to Abduction in First Order Horn Logic\n",
      "\n",
      "Abstract:   We study abduction in First Order Horn logic theories where all atoms can beabduced and we are looking for preferred solutions with respect to threeobjective functions: cardinality minimality, coherence, and weighted abduction.We represent this reasoning problem in Answer Set Programming (ASP), in orderto obtain a flexible framework for experimenting with global constraints andobjective functions, and to test the boundaries of what is possible with ASP.Realizing this problem in ASP is challenging as it requires value invention andequivalence between certain constants, because the Unique Names Assumption doesnot hold in general. To permit reasoning in cyclic theories, we formallydescribe fine-grained variations of limiting Skolemization. We identify termequivalence as a main instantiation bottleneck, and improve the efficiency ofour approach with on-demand constraints that were used to eliminate the samebottleneck in state-of-the-art solvers. We evaluate our approach experimentallyon the ACCEL benchmark for plan recognition in Natural Language Understanding.Our encodings are publicly available, modular, and our approach is moreefficient than state-of-the-art solvers on the ACCEL benchmark.\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "208904f95b2843f09799bbfe2a4938ca",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating outputs:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True Title: A Survey of Multi-Objective Sequential Decision-Making\n",
      "\n",
      "Predicted Title: A Multi-Objective Methods for Planning and Learning\n",
      "\n",
      "Abstract:   Sequential decision-making problems with multiple objectives arise naturallyin practice and pose unique challenges for research in decision-theoreticplanning and learning, which has largely focused on single-objective settings.This article surveys algorithms designed for sequential decision-makingproblems with multiple objectives. Though there is a growing body of literatureon this subject, little of it makes explicit under what circumstances specialmethods are needed to solve multi-objective problems. Therefore, we identifythree distinct scenarios in which converting such a problem to asingle-objective one is impossible, infeasible, or undesirable. Furthermore, wepropose a taxonomy that classifies multi-objective methods according to theapplicable scenario, the nature of the scalarization function (which projectsmulti-objective values to scalar ones), and the type of policies considered. Weshow how these factors determine the nature of an optimal solution, which canbe a single policy, a convex hull, or a Pareto front. Using this taxonomy, wesurvey the literature on multi-objective methods for planning and learning.Finally, we discuss key applications of such methods and outline opportunitiesfor future work.\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c777bbc63503411d9ba4ba1ec5739744",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating outputs:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True Title: Self-Organizing Maps for Storage and Transfer of Knowledge in\n",
      "  Reinforcement Learning\n",
      "\n",
      "Predicted Title: A-based Map for Reusing and Transfer Knowledge from Previously Learned Task Knowledge\n",
      "\n",
      "Abstract:   The idea of reusing or transferring information from previously learned tasks(source tasks) for the learning of new tasks (target tasks) has the potentialto significantly improve the sample efficiency of a reinforcement learningagent. In this work, we describe a novel approach for reusing previouslyacquired knowledge by using it to guide the exploration of an agent while itlearns new tasks. In order to do so, we employ a variant of the growingself-organizing map algorithm, which is trained using a measure of similaritythat is defined directly in the space of the vectorized representations of thevalue functions. In addition to enabling transfer across tasks, the resultingmap is simultaneously used to enable the efficient storage of previouslyacquired task knowledge in an adaptive and scalable manner. We empiricallyvalidate our approach in a simulated navigation environment, and alsodemonstrate its utility through simple experiments using a mobilemicro-robotics platform. In addition, we demonstrate the scalability of thisapproach, and analytically examine its relation to the proposed network growthmechanism. Further, we briefly discuss some of the possible improvements andextensions to this approach, as well as its relevance to real world scenariosin the context of continual learning.\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e526f02c30c54fefb2e94ddf8e414827",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating outputs:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True Title: Preference-Based Monte Carlo Tree Search\n",
      "\n",
      "Predicted Title: A Monte Carlo Tree Search: A Unique and Unique Approach\n",
      "\n",
      "Abstract:   Monte Carlo tree search (MCTS) is a popular choice for solving sequentialanytime problems. However, it depends on a numeric feedback signal, which canbe difficult to define. Real-time MCTS is a variant which may only rarelyencounter states with an explicit, extrinsic reward. To deal with such cases,the experimenter has to supply an additional numeric feedback signal in theform of a heuristic, which intrinsically guides the agent. Recent work hasshown evidence that in different areas the underlying structure is ordinal andnot numerical. Hence erroneous and biased heuristics are inevitable, especiallyin such domains. In this paper, we propose a MCTS variant which only depends onqualitative feedback, and therefore opens up new applications for MCTS. We alsofind indications that translating absolute into ordinal feedback may bebeneficial. Using a puzzle domain, we show that our preference-based MCTSvariant, wich only receives qualitative feedback, is able to reach aperformance level comparable to a regular MCTS baseline, which obtainsquantitative feedback.\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8a6cff265fd047799ca87a051c421706",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating outputs:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True Title: Towards an Ontology based integrated Framework for Semantic Web\n",
      "\n",
      "Predicted Title: A ontologies for heterogeneous data sources\n",
      "\n",
      "Abstract:   This Ontologies are widely used as a means for solving the informationheterogeneity problems on the web because of their capability to provideexplicit meaning to the information. They become an efficient tool forknowledge representation in a structured manner. There is always more than oneontology for the same domain. Furthermore, there is no standard method forbuilding ontologies, and there are many ontology building tools using differentontology languages. Because of these reasons, interoperability between theontologies is very low. Current ontology tools mostly use functions to build,edit and inference the ontology. Methods for merging heterogeneous domainontologies are not included in most tools. This paper presents ontology mergingmethodology for building a single global ontology from heterogeneous eXtensibleMarkup Language (XML) data sources to capture and maintain all the knowledgewhich XML data sources can contain\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f3eed4db5ab04d3a8b25ea0dca85f92b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating outputs:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True Title: A Survey of Multi-Objective Sequential Decision-Making\n",
      "\n",
      "Predicted Title: A Multi-Objective Methods for Planning and Learning\n",
      "\n",
      "Abstract:   Sequential decision-making problems with multiple objectives arise naturallyin practice and pose unique challenges for research in decision-theoreticplanning and learning, which has largely focused on single-objective settings.This article surveys algorithms designed for sequential decision-makingproblems with multiple objectives. Though there is a growing body of literatureon this subject, little of it makes explicit under what circumstances specialmethods are needed to solve multi-objective problems. Therefore, we identifythree distinct scenarios in which converting such a problem to asingle-objective one is impossible, infeasible, or undesirable. Furthermore, wepropose a taxonomy that classifies multi-objective methods according to theapplicable scenario, the nature of the scalarization function (which projectsmulti-objective values to scalar ones), and the type of policies considered. Weshow how these factors determine the nature of an optimal solution, which canbe a single policy, a convex hull, or a Pareto front. Using this taxonomy, wesurvey the literature on multi-objective methods for planning and learning.Finally, we discuss key applications of such methods and outline opportunitiesfor future work.\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dc9856eb48b645b78a518e739def6142",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating outputs:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True Title: Online Guest Detection in a Smart Home using Pervasive Sensors and\n",
      "  Probabilistic Reasoning\n",
      "\n",
      "Predicted Title: Aing the Number of People in a Smart Home at Each Time Step\n",
      "\n",
      "Abstract:   Smart home environments equipped with distributed sensor networks are capableof helping people by providing services related to health, emergency detectionor daily routine management. A backbone to these systems relies often on thesystem's ability to track and detect activities performed by the users in theirhome. Despite the continuous progress in the area of activity recognition insmart homes, many systems make a strong underlying assumption that the numberof occupants in the home at any given moment of time is always known.Estimating the number of persons in a Smart Home at each time step remains achallenge nowadays. Indeed, unlike most (crowd) counting solution which arebased on computer vision techniques, the sensors considered in a Smart Home areoften very simple and do not offer individually a good overview of thesituation. The data gathered needs therefore to be fused in order to inferuseful information. This paper aims at addressing this challenge and presents aprobabilistic approach able to estimate the number of persons in theenvironment at each time step. This approach works in two steps: first, anestimate of the number of persons present in the environment is done using aConstraint Satisfaction Problem solver, based on the topology of the sensornetwork and the sensor activation pattern at this time point. Then, a HiddenMarkov Model refines this estimate by considering the uncertainty related tothe sensors. Using both simulated and real data, our method has been tested andvalidated on two smart homes of different sizes and configuration anddemonstrates the ability to accurately estimate the number of inhabitants.\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5be06a1cf74c4f2db9f5d63aa9c18bd2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating outputs:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True Title: SAPFOCS: a metaheuristic based approach to part family formation\n",
      "  problems in group technology\n",
      "\n",
      "Predicted Title: A Part Family Formation in GroupTechnology\n",
      "\n",
      "Abstract:   This article deals with Part family formation problem which is believed to bemoderately complicated to be solved in polynomial time in the vicinity of GroupTechnology (GT). In the past literature researchers investigated that the partfamily formation techniques are principally based on production flow analysis(PFA) which usually considers operational requirements, sequences and time.Part Coding Analysis (PCA) is merely considered in GT which is believed to bethe proficient method to identify the part families. PCA classifies parts byallotting them to different families based on their resemblances in: (1) designcharacteristics such as shape and size, and/or (2) manufacturingcharacteristics (machining requirements). A novel approach based on simulatedannealing namely SAPFOCS is adopted in this study to develop effective partfamilies exploiting the PCA technique. Thereafter Taguchi's orthogonal designmethod is employed to solve the critical issues on the subject of parametersselection for the proposed metaheuristic algorithm. The adopted technique istherefore tested on 5 different datasets of size 5 {\\times} 9 to 27 {\\times} 9and the obtained results are compared with C-Linkage clustering technique. Theexperimental results reported that the proposed metaheuristic algorithm isextremely effective in terms of the quality of the solution obtained and hasoutperformed C-Linkage algorithm in most instances.\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b61dfaa97088448b94f1030c62a82bc0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating outputs:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True Title: SAPFOCS: a metaheuristic based approach to part family formation\n",
      "  problems in group technology\n",
      "\n",
      "Predicted Title: A Part Family Formation in GroupTechnology\n",
      "\n",
      "Abstract:   This article deals with Part family formation problem which is believed to bemoderately complicated to be solved in polynomial time in the vicinity of GroupTechnology (GT). In the past literature researchers investigated that the partfamily formation techniques are principally based on production flow analysis(PFA) which usually considers operational requirements, sequences and time.Part Coding Analysis (PCA) is merely considered in GT which is believed to bethe proficient method to identify the part families. PCA classifies parts byallotting them to different families based on their resemblances in: (1) designcharacteristics such as shape and size, and/or (2) manufacturingcharacteristics (machining requirements). A novel approach based on simulatedannealing namely SAPFOCS is adopted in this study to develop effective partfamilies exploiting the PCA technique. Thereafter Taguchi's orthogonal designmethod is employed to solve the critical issues on the subject of parametersselection for the proposed metaheuristic algorithm. The adopted technique istherefore tested on 5 different datasets of size 5 {\\times} 9 to 27 {\\times} 9and the obtained results are compared with C-Linkage clustering technique. Theexperimental results reported that the proposed metaheuristic algorithm isextremely effective in terms of the quality of the solution obtained and hasoutperformed C-Linkage algorithm in most instances.\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c15033d7e29747eba9e1ef4cd16c2b7c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating outputs:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True Title: Qualitative Approximate Behavior Composition\n",
      "\n",
      "Predicted Title: Aimating behavior composition without simulation\n",
      "\n",
      "Abstract:   The behavior composition problem involves automatically building a controllerthat is able to realize a desired, but unavailable, target system (e.g., ahouse surveillance) by suitably coordinating a set of available components(e.g., video cameras, blinds, lamps, a vacuum cleaner, phones, etc.) Previouswork has almost exclusively aimed at bringing about the desired component inits totality, which is highly unsatisfactory for unsolvable problems. In thiswork, we develop an approach for approximate behavior composition withoutdeparting from the classical setting, thus making the problem applicable to amuch wider range of cases. Based on the notion of simulation, we characterizewhat a maximal controller and the \"closest\" implementable target module(optimal approximation) are, and show how these can be computed using ATL modelchecking technology for a special case. We show the uniqueness of optimalapproximations, and prove their soundness and completeness with respect totheir imported controllers.\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6a2e00462be544278999cad9701323d1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating outputs:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True Title: Shiva++: An Enhanced Graph based Ontology Matcher\n",
      "\n",
      "Predicted Title: Anology based systems for knowledge management\n",
      "\n",
      "Abstract:   With the web getting bigger and assimilating knowledge about differentconcepts and domains, it is becoming very difficult for simple database drivenapplications to capture the data for a domain. Thus developers have come outwith ontology based systems which can store large amount of information and canapply reasoning and produce timely information. Thus facilitating effectiveknowledge management. Though this approach has made our lives easier, but atthe same time has given rise to another problem. Two different ontologiesassimilating same knowledge tend to use different terms for the same concepts.This creates confusion among knowledge engineers and workers, as they do notknow which is a better term then the other. Thus we need to merge ontologiesworking on same domain so that the engineers can develop a better applicationover it. This paper shows the development of one such matcher which merges theconcepts available in two ontologies at two levels; 1) at string level and 2)at semantic level; thus producing better merged ontologies. We have used agraph matching technique which works at the core of the system. We have alsoevaluated the system and have tested its performance with its predecessor whichworks only on string matching. Thus current approach produces better results.\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8eb36f53a14e4d3195a7a1aaf3e13ab1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating outputs:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True Title: Self-Organizing Maps for Storage and Transfer of Knowledge in\n",
      "  Reinforcement Learning\n",
      "\n",
      "Predicted Title: A-based Map for Reusing and Transfer Knowledge from Previously Learned Task Knowledge\n",
      "\n",
      "Abstract:   The idea of reusing or transferring information from previously learned tasks(source tasks) for the learning of new tasks (target tasks) has the potentialto significantly improve the sample efficiency of a reinforcement learningagent. In this work, we describe a novel approach for reusing previouslyacquired knowledge by using it to guide the exploration of an agent while itlearns new tasks. In order to do so, we employ a variant of the growingself-organizing map algorithm, which is trained using a measure of similaritythat is defined directly in the space of the vectorized representations of thevalue functions. In addition to enabling transfer across tasks, the resultingmap is simultaneously used to enable the efficient storage of previouslyacquired task knowledge in an adaptive and scalable manner. We empiricallyvalidate our approach in a simulated navigation environment, and alsodemonstrate its utility through simple experiments using a mobilemicro-robotics platform. In addition, we demonstrate the scalability of thisapproach, and analytically examine its relation to the proposed network growthmechanism. Further, we briefly discuss some of the possible improvements andextensions to this approach, as well as its relevance to real world scenariosin the context of continual learning.\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "70fdefc7427a4cddba09efa09254e4cd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating outputs:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True Title: A note on the complexity of the causal ordering problem\n",
      "\n",
      "Predicted Title: A causal ordering: A concise report on the complexity of the causalordering problem\n",
      "\n",
      "Abstract:   In this note we provide a concise report on the complexity of the causalordering problem, originally introduced by Simon to reason about causaldependencies implicit in systems of mathematical equations. We show thatSimon's classical algorithm to infer causal ordering is NP-Hard---anintractability previously guessed but never proven. We present then a detailedaccount based on Nayak's suggested algorithmic solution (the best available),which is dominated by computing transitive closure---bounded in time by$O(|\\mathcal V|\\cdot |\\mathcal S|)$, where $\\mathcal S(\\mathcal E, \\mathcal V)$is the input system structure composed of a set $\\mathcal E$ of equations overa set $\\mathcal V$ of variables with number of variable appearances (density)$|\\mathcal S|$. We also comment on the potential of causal ordering foremerging applications in large-scale hypothesis management and analytics.\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7609fee69bbf429ab1900cec39e78069",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating outputs:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True Title: Autonomous Self-Explanation of Behavior for Interactive Reinforcement\n",
      "  Learning Agents\n",
      "\n",
      "Predicted Title: A Instruction-Based Behavior Explanation\n",
      "\n",
      "Abstract:   In cooperation, the workers must know how co-workers behave. However, anagent's policy, which is embedded in a statistical machine learning model, ishard to understand, and requires much time and knowledge to comprehend.Therefore, it is difficult for people to predict the behavior of machinelearning robots, which makes Human Robot Cooperation challenging. In thispaper, we propose Instruction-based Behavior Explanation (IBE), a method toexplain an autonomous agent's future behavior. In IBE, an agent canautonomously acquire the expressions to explain its own behavior by reusing theinstructions given by a human expert to accelerate the learning of the agent'spolicy. IBE also enables a developmental agent, whose policy may change duringthe cooperation, to explain its own behavior with sufficient time granularity.\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "56ed35444305493ba02099b97c6f2bd7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating outputs:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True Title: Autonomous Self-Explanation of Behavior for Interactive Reinforcement\n",
      "  Learning Agents\n",
      "\n",
      "Predicted Title: A Instruction-Based Behavior Explanation\n",
      "\n",
      "Abstract:   In cooperation, the workers must know how co-workers behave. However, anagent's policy, which is embedded in a statistical machine learning model, ishard to understand, and requires much time and knowledge to comprehend.Therefore, it is difficult for people to predict the behavior of machinelearning robots, which makes Human Robot Cooperation challenging. In thispaper, we propose Instruction-based Behavior Explanation (IBE), a method toexplain an autonomous agent's future behavior. In IBE, an agent canautonomously acquire the expressions to explain its own behavior by reusing theinstructions given by a human expert to accelerate the learning of the agent'spolicy. IBE also enables a developmental agent, whose policy may change duringthe cooperation, to explain its own behavior with sufficient time granularity.\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6654923c9551438bbb94ab4ecc53beb6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating outputs:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True Title: A Modified Vortex Search Algorithm for Numerical Function Optimization\n",
      "\n",
      "Predicted Title: Avortex Search: A Modification of Vortex Search\n",
      "\n",
      "Abstract:   The Vortex Search (VS) algorithm is one of the recently proposedmetaheuristic algorithms which was inspired from the vortical flow of thestirred fluids. Although the VS algorithm is shown to be a good candidate forthe solution of certain optimization problems, it also has some drawbacks. Inthe VS algorithm, candidate solutions are generated around the current bestsolution by using a Gaussian distribution at each iteration pass. This providessimplicity to the algorithm but it also leads to some problems along.Especially, for the functions those have a number of local minimum points, toselect a single point to generate candidate solutions leads the algorithm tobeing trapped into a local minimum point. Due to the adaptive step-sizeadjustment scheme used in the VS algorithm, the locality of the createdcandidate solutions is increased at each iteration pass. Therefore, if thealgorithm cannot escape a local point as quickly as possible, it becomes muchmore difficult for the algorithm to escape from that point in the latteriterations. In this study, a modified Vortex Search algorithm (MVS) is proposedto overcome above mentioned drawback of the existing VS algorithm. In the MVSalgorithm, the candidate solutions are generated around a number of points ateach iteration pass. Computational results showed that with the help of thismodification the global search ability of the existing VS algorithm is improvedand the MVS algorithm outperformed the existing VS algorithm, PSO2011 and ABCalgorithms for the benchmark numerical function set.\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "82c73045c2e8440a9ef91d7ad028f57c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating outputs:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True Title: The SP theory of intelligence and the representation and processing of\n",
      "  knowledge in the brain\n",
      "\n",
      "Predicted Title: A SP theory of intelligence: An overview and partial model\n",
      "\n",
      "Abstract:   The \"SP theory of intelligence\", with its realisation in the \"SP computermodel\", aims to simplify and integrate observations and concepts acrossAI-related fields, with information compression as a unifying theme. This paperdescribes how abstract structures and processes in the theory may be realisedin terms of neurons, their interconnections, and the transmission of signalsbetween neurons. This part of the SP theory -- \"SP-neural\" -- is a tentativeand partial model for the representation and processing of knowledge in thebrain. In the SP theory (apart from SP-neural), all kinds of knowledge arerepresented with \"patterns\", where a pattern is an array of atomic symbols inone or two dimensions. In SP-neural, the concept of a \"pattern\" is realised asan array of neurons called a \"pattern assembly\", similar to Hebb's concept of a\"cell assembly\" but with important differences. Central to the processing ofinformation in the SP system is the powerful concept of \"multiple alignment\",borrowed and adapted from bioinformatics. Processes such as patternrecognition, reasoning and problem solving are achieved via the building ofmultiple alignments, while unsupervised learning -- significantly differentfrom the \"Hebbian\" kinds of learning -- is achieved by creating patterns fromsensory information and also by creating patterns from multiple alignments inwhich there is a partial match between one pattern and another. Short-livedneural structures equivalent to multiple alignments will be created via aninter-play of excitatory and inhibitory neural signals. The paper discussesseveral associated issues, with relevant empirical evidence.\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e5d86873e97642e480861c5fe6de43e3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating outputs:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True Title: A Study of Student Learning Skills Using Fuzzy Relation Equations\n",
      "\n",
      "Predicted Title: A Fuzzy relation equations (FRE)\n",
      "\n",
      "Abstract:   Fuzzy relation equations (FRE)are associated with the composition of binaryfuzzy relations. In the present work FRE are used as a tool for studying theprocess of learning a new subject matter by a student class. A classroomapplication and other csuitable examples connected to the student learning ofthe derivative are also presented illustrating our results and usefulconclusions are obtained.\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6c10667f2abf4352a2e5fe3e06a767cc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating outputs:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True Title: Learning STRIPS Action Models with Classical Planning\n",
      "\n",
      "Predicted Title: Acompiling STRIPS action models from a classical planning task\n",
      "\n",
      "Abstract:   This paper presents a novel approach for learning STRIPS action models fromexamples that compiles this inductive learning task into a classical planningtask. Interestingly, the compilation approach is flexible to different amountsof available input knowledge; the learning examples can range from a set ofplans (with their corresponding initial and final states) to just a pair ofinitial and final states (no intermediate action or state is given). Moreover,the compilation accepts partially specified action models and it can be used tovalidate whether the observation of a plan execution follows a given STRIPSaction model, even if this model is not fully specified.\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "26eb530605c5465580d1d5d058774964",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating outputs:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True Title: A Directional Feature with Energy based Offline Signature Verification\n",
      "  Network\n",
      "\n",
      "Predicted Title: Aofflinesignature verification system\n",
      "\n",
      "Abstract:   Signature used as a biometric is implemented in various systems as well asevery signature signed by each person is distinct at the same time. So, it isvery important to have a computerized signature verification system. In offlinesignature verification system dynamic features are not available obviously, butone can use a signature as an image and apply image processing techniques tomake an effective offline signature verification system. Author proposes aintelligent network used directional feature and energy density both as inputsto the same network and classifies the signature. Neural network is used as aclassifier for this system. The results are compared with both the very basicenergy density method and a simple directional feature method of offlinesignature verification system and this proposed new network is found veryeffective as compared to the above two methods, specially for less number oftraining samples, which can be implemented practically.\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fd622176e1424e7cae8194818308cd3b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating outputs:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True Title: A Modified Vortex Search Algorithm for Numerical Function Optimization\n",
      "\n",
      "Predicted Title: Avortex Search: A Modification of Vortex Search\n",
      "\n",
      "Abstract:   The Vortex Search (VS) algorithm is one of the recently proposedmetaheuristic algorithms which was inspired from the vortical flow of thestirred fluids. Although the VS algorithm is shown to be a good candidate forthe solution of certain optimization problems, it also has some drawbacks. Inthe VS algorithm, candidate solutions are generated around the current bestsolution by using a Gaussian distribution at each iteration pass. This providessimplicity to the algorithm but it also leads to some problems along.Especially, for the functions those have a number of local minimum points, toselect a single point to generate candidate solutions leads the algorithm tobeing trapped into a local minimum point. Due to the adaptive step-sizeadjustment scheme used in the VS algorithm, the locality of the createdcandidate solutions is increased at each iteration pass. Therefore, if thealgorithm cannot escape a local point as quickly as possible, it becomes muchmore difficult for the algorithm to escape from that point in the latteriterations. In this study, a modified Vortex Search algorithm (MVS) is proposedto overcome above mentioned drawback of the existing VS algorithm. In the MVSalgorithm, the candidate solutions are generated around a number of points ateach iteration pass. Computational results showed that with the help of thismodification the global search ability of the existing VS algorithm is improvedand the MVS algorithm outperformed the existing VS algorithm, PSO2011 and ABCalgorithms for the benchmark numerical function set.\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d2bd9fe9432345b1a7c0aa6293f49f85",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating outputs:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True Title: Preference-Based Monte Carlo Tree Search\n",
      "\n",
      "Predicted Title: A Monte Carlo Tree Search: A Unique and Unique Approach\n",
      "\n",
      "Abstract:   Monte Carlo tree search (MCTS) is a popular choice for solving sequentialanytime problems. However, it depends on a numeric feedback signal, which canbe difficult to define. Real-time MCTS is a variant which may only rarelyencounter states with an explicit, extrinsic reward. To deal with such cases,the experimenter has to supply an additional numeric feedback signal in theform of a heuristic, which intrinsically guides the agent. Recent work hasshown evidence that in different areas the underlying structure is ordinal andnot numerical. Hence erroneous and biased heuristics are inevitable, especiallyin such domains. In this paper, we propose a MCTS variant which only depends onqualitative feedback, and therefore opens up new applications for MCTS. We alsofind indications that translating absolute into ordinal feedback may bebeneficial. Using a puzzle domain, we show that our preference-based MCTSvariant, wich only receives qualitative feedback, is able to reach aperformance level comparable to a regular MCTS baseline, which obtainsquantitative feedback.\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4e9a17166f0b4f3281763926834c3ab9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating outputs:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True Title: A Study of Student Learning Skills Using Fuzzy Relation Equations\n",
      "\n",
      "Predicted Title: A Fuzzy relation equations (FRE)\n",
      "\n",
      "Abstract:   Fuzzy relation equations (FRE)are associated with the composition of binaryfuzzy relations. In the present work FRE are used as a tool for studying theprocess of learning a new subject matter by a student class. A classroomapplication and other csuitable examples connected to the student learning ofthe derivative are also presented illustrating our results and usefulconclusions are obtained.\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "30fd60e84e154a709c2115a692eafd09",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating outputs:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True Title: Contextual Symmetries in Probabilistic Graphical Models\n",
      "\n",
      "Predicted Title: A Contextual symmetries\n",
      "\n",
      "Abstract:   An important approach for efficient inference in probabilistic graphicalmodels exploits symmetries among objects in the domain. Symmetric variables(states) are collapsed into meta-variables (meta-states) and inferencealgorithms are run over the lifted graphical model instead of the flat one. Ourpaper extends existing definitions of symmetry by introducing the novel notionof contextual symmetry. Two states that are not globally symmetric, can becontextually symmetric under some specific assignment to a subset of variables,referred to as the context variables. Contextual symmetry subsumes previoussymmetry definitions and can rep resent a large class of symmetries notrepresentable earlier. We show how to compute contextual symmetries by reducingit to the problem of graph isomorphism. We extend previous work on exploitingsymmetries in the MCMC framework to the case of contextual symmetries. Ourexperiments on several domains of interest demonstrate that exploitingcontextual symmetries can result in significant computational gains.\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9a5a315f05774c2fa58404f713c6b05a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating outputs:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True Title: Preference-Based Monte Carlo Tree Search\n",
      "\n",
      "Predicted Title: A Monte Carlo Tree Search: A Unique and Unique Approach\n",
      "\n",
      "Abstract:   Monte Carlo tree search (MCTS) is a popular choice for solving sequentialanytime problems. However, it depends on a numeric feedback signal, which canbe difficult to define. Real-time MCTS is a variant which may only rarelyencounter states with an explicit, extrinsic reward. To deal with such cases,the experimenter has to supply an additional numeric feedback signal in theform of a heuristic, which intrinsically guides the agent. Recent work hasshown evidence that in different areas the underlying structure is ordinal andnot numerical. Hence erroneous and biased heuristics are inevitable, especiallyin such domains. In this paper, we propose a MCTS variant which only depends onqualitative feedback, and therefore opens up new applications for MCTS. We alsofind indications that translating absolute into ordinal feedback may bebeneficial. Using a puzzle domain, we show that our preference-based MCTSvariant, wich only receives qualitative feedback, is able to reach aperformance level comparable to a regular MCTS baseline, which obtainsquantitative feedback.\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9483935ddac242d093567f7108a21d4d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating outputs:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True Title: Towards an Ontology based integrated Framework for Semantic Web\n",
      "\n",
      "Predicted Title: A ontologies for heterogeneous data sources\n",
      "\n",
      "Abstract:   This Ontologies are widely used as a means for solving the informationheterogeneity problems on the web because of their capability to provideexplicit meaning to the information. They become an efficient tool forknowledge representation in a structured manner. There is always more than oneontology for the same domain. Furthermore, there is no standard method forbuilding ontologies, and there are many ontology building tools using differentontology languages. Because of these reasons, interoperability between theontologies is very low. Current ontology tools mostly use functions to build,edit and inference the ontology. Methods for merging heterogeneous domainontologies are not included in most tools. This paper presents ontology mergingmethodology for building a single global ontology from heterogeneous eXtensibleMarkup Language (XML) data sources to capture and maintain all the knowledgewhich XML data sources can contain\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e6cedf7ffa0a4385aa911c25bd79c7a3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating outputs:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True Title: A Modified Vortex Search Algorithm for Numerical Function Optimization\n",
      "\n",
      "Predicted Title: Avortex Search: A Modification of Vortex Search\n",
      "\n",
      "Abstract:   The Vortex Search (VS) algorithm is one of the recently proposedmetaheuristic algorithms which was inspired from the vortical flow of thestirred fluids. Although the VS algorithm is shown to be a good candidate forthe solution of certain optimization problems, it also has some drawbacks. Inthe VS algorithm, candidate solutions are generated around the current bestsolution by using a Gaussian distribution at each iteration pass. This providessimplicity to the algorithm but it also leads to some problems along.Especially, for the functions those have a number of local minimum points, toselect a single point to generate candidate solutions leads the algorithm tobeing trapped into a local minimum point. Due to the adaptive step-sizeadjustment scheme used in the VS algorithm, the locality of the createdcandidate solutions is increased at each iteration pass. Therefore, if thealgorithm cannot escape a local point as quickly as possible, it becomes muchmore difficult for the algorithm to escape from that point in the latteriterations. In this study, a modified Vortex Search algorithm (MVS) is proposedto overcome above mentioned drawback of the existing VS algorithm. In the MVSalgorithm, the candidate solutions are generated around a number of points ateach iteration pass. Computational results showed that with the help of thismodification the global search ability of the existing VS algorithm is improvedand the MVS algorithm outperformed the existing VS algorithm, PSO2011 and ABCalgorithms for the benchmark numerical function set.\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b93339a3d97448afa2970d431f87ce63",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating outputs:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True Title: Representing and Reasoning with Qualitative Preferences for\n",
      "  Compositional Systems\n",
      "\n",
      "Predicted Title: A dominance relation for the selection of most preferred collections\n",
      "\n",
      "Abstract:   Many applications, e.g., Web service composition, complex system design, teamformation, etc., rely on methods for identifying collections of objects orentities satisfying some functional requirement. Among the collections thatsatisfy the functional requirement, it is often necessary to identify one ormore collections that are optimal with respect to user preferences over a setof attributes that describe the non-functional properties of the collection.  We develop a formalism that lets users express the relative importance amongattributes and qualitative preferences over the valuations of each attribute.We define a dominance relation that allows us to compare collections of objectsin terms of preferences over attributes of the objects that make up thecollection. We establish some key properties of the dominance relation. Inparticular, we show that the dominance relation is a strict partial order whenthe intra-attribute preference relations are strict partial orders and therelative importance preference relation is an interval order.  We provide algorithms that use this dominance relation to identify the set ofmost preferred collections. We show that under certain conditions, thealgorithms are guaranteed to return only (sound), all (complete), or at leastone (weakly complete) of the most preferred collections. We present results ofsimulation experiments comparing the proposed algorithms with respect to (a)the quality of solutions (number of most preferred solutions) produced by thealgorithms, and (b) their performance and efficiency. We also explore someinteresting conjectures suggested by the results of our experiments that relatethe properties of the user preferences, the dominance relation, and thealgorithms.\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9f2240cdd1e74dc6a52dc3eecbe8b7a4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating outputs:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True Title: Shiva++: An Enhanced Graph based Ontology Matcher\n",
      "\n",
      "Predicted Title: Anology based systems for knowledge management\n",
      "\n",
      "Abstract:   With the web getting bigger and assimilating knowledge about differentconcepts and domains, it is becoming very difficult for simple database drivenapplications to capture the data for a domain. Thus developers have come outwith ontology based systems which can store large amount of information and canapply reasoning and produce timely information. Thus facilitating effectiveknowledge management. Though this approach has made our lives easier, but atthe same time has given rise to another problem. Two different ontologiesassimilating same knowledge tend to use different terms for the same concepts.This creates confusion among knowledge engineers and workers, as they do notknow which is a better term then the other. Thus we need to merge ontologiesworking on same domain so that the engineers can develop a better applicationover it. This paper shows the development of one such matcher which merges theconcepts available in two ontologies at two levels; 1) at string level and 2)at semantic level; thus producing better merged ontologies. We have used agraph matching technique which works at the core of the system. We have alsoevaluated the system and have tested its performance with its predecessor whichworks only on string matching. Thus current approach produces better results.\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a9b4c2bd8cd54336b986d0424d49c8da",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating outputs:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True Title: Shiva++: An Enhanced Graph based Ontology Matcher\n",
      "\n",
      "Predicted Title: Anology based systems for knowledge management\n",
      "\n",
      "Abstract:   With the web getting bigger and assimilating knowledge about differentconcepts and domains, it is becoming very difficult for simple database drivenapplications to capture the data for a domain. Thus developers have come outwith ontology based systems which can store large amount of information and canapply reasoning and produce timely information. Thus facilitating effectiveknowledge management. Though this approach has made our lives easier, but atthe same time has given rise to another problem. Two different ontologiesassimilating same knowledge tend to use different terms for the same concepts.This creates confusion among knowledge engineers and workers, as they do notknow which is a better term then the other. Thus we need to merge ontologiesworking on same domain so that the engineers can develop a better applicationover it. This paper shows the development of one such matcher which merges theconcepts available in two ontologies at two levels; 1) at string level and 2)at semantic level; thus producing better merged ontologies. We have used agraph matching technique which works at the core of the system. We have alsoevaluated the system and have tested its performance with its predecessor whichworks only on string matching. Thus current approach produces better results.\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b8302747565e4fbfb5f33bd0526f5641",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating outputs:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True Title: Modeling Variations of First-Order Horn Abduction in Answer Set\n",
      "  Programming\n",
      "\n",
      "Predicted Title: A Answer Set Programming: An Approach to Abduction in First Order Horn Logic\n",
      "\n",
      "Abstract:   We study abduction in First Order Horn logic theories where all atoms can beabduced and we are looking for preferred solutions with respect to threeobjective functions: cardinality minimality, coherence, and weighted abduction.We represent this reasoning problem in Answer Set Programming (ASP), in orderto obtain a flexible framework for experimenting with global constraints andobjective functions, and to test the boundaries of what is possible with ASP.Realizing this problem in ASP is challenging as it requires value invention andequivalence between certain constants, because the Unique Names Assumption doesnot hold in general. To permit reasoning in cyclic theories, we formallydescribe fine-grained variations of limiting Skolemization. We identify termequivalence as a main instantiation bottleneck, and improve the efficiency ofour approach with on-demand constraints that were used to eliminate the samebottleneck in state-of-the-art solvers. We evaluate our approach experimentallyon the ACCEL benchmark for plan recognition in Natural Language Understanding.Our encodings are publicly available, modular, and our approach is moreefficient than state-of-the-art solvers on the ACCEL benchmark.\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b202a128d83a4174a3c886bbeb1bb474",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating outputs:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True Title: Contextual Symmetries in Probabilistic Graphical Models\n",
      "\n",
      "Predicted Title: A Contextual symmetries\n",
      "\n",
      "Abstract:   An important approach for efficient inference in probabilistic graphicalmodels exploits symmetries among objects in the domain. Symmetric variables(states) are collapsed into meta-variables (meta-states) and inferencealgorithms are run over the lifted graphical model instead of the flat one. Ourpaper extends existing definitions of symmetry by introducing the novel notionof contextual symmetry. Two states that are not globally symmetric, can becontextually symmetric under some specific assignment to a subset of variables,referred to as the context variables. Contextual symmetry subsumes previoussymmetry definitions and can rep resent a large class of symmetries notrepresentable earlier. We show how to compute contextual symmetries by reducingit to the problem of graph isomorphism. We extend previous work on exploitingsymmetries in the MCMC framework to the case of contextual symmetries. Ourexperiments on several domains of interest demonstrate that exploitingcontextual symmetries can result in significant computational gains.\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d3cf416c304a43009c43ac1945bd6e80",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating outputs:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True Title: A Fuzzy AHP Approach for Supplier Selection Problem: A Case Study in a\n",
      "  Gear Motor Company\n",
      "\n",
      "Predicted Title: A Fuzzy AHP for Suuplier Selection\n",
      "\n",
      "Abstract:   Suuplier selection is one of the most important functions of a purchasingdepartment. Since by deciding the best supplier, companies can save materialcosts and increase competitive advantage.However this decision becomescompilcated in case of multiple suppliers, multiple conflicting criteria, andimprecise parameters. In addition the uncertainty and vagueness of the experts'opinion is the prominent characteristic of the problem. therefore anextensively used multi criteria decision making tool Fuzzy AHP can be utilizedas an approach for supplier selection problem. This paper reveals theapplication of Fuzzy AHP in a gear motor company determining the best supplierwith respect to selected criteria. the contribution of this study is not onlythe application of the Fuzzy AHP methodology for supplier selection problem,but also releasing a comprehensive literature review of multi criteria decisionmaking problems. In addition by stating the steps of Fuzzy AHP clearly andnumerically, this study can be a guide of the methodology to be implemented toother multiple criteria decision making problems.\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b0bd0470d0cf43089132a9eb53a51ddb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating outputs:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True Title: A Fuzzy AHP Approach for Supplier Selection Problem: A Case Study in a\n",
      "  Gear Motor Company\n",
      "\n",
      "Predicted Title: A Fuzzy AHP for Suuplier Selection\n",
      "\n",
      "Abstract:   Suuplier selection is one of the most important functions of a purchasingdepartment. Since by deciding the best supplier, companies can save materialcosts and increase competitive advantage.However this decision becomescompilcated in case of multiple suppliers, multiple conflicting criteria, andimprecise parameters. In addition the uncertainty and vagueness of the experts'opinion is the prominent characteristic of the problem. therefore anextensively used multi criteria decision making tool Fuzzy AHP can be utilizedas an approach for supplier selection problem. This paper reveals theapplication of Fuzzy AHP in a gear motor company determining the best supplierwith respect to selected criteria. the contribution of this study is not onlythe application of the Fuzzy AHP methodology for supplier selection problem,but also releasing a comprehensive literature review of multi criteria decisionmaking problems. In addition by stating the steps of Fuzzy AHP clearly andnumerically, this study can be a guide of the methodology to be implemented toother multiple criteria decision making problems.\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "56edea5b6d384447b0233dc1152ddc75",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating outputs:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True Title: Learning STRIPS Action Models with Classical Planning\n",
      "\n",
      "Predicted Title: Acompiling STRIPS action models from a classical planning task\n",
      "\n",
      "Abstract:   This paper presents a novel approach for learning STRIPS action models fromexamples that compiles this inductive learning task into a classical planningtask. Interestingly, the compilation approach is flexible to different amountsof available input knowledge; the learning examples can range from a set ofplans (with their corresponding initial and final states) to just a pair ofinitial and final states (no intermediate action or state is given). Moreover,the compilation accepts partially specified action models and it can be used tovalidate whether the observation of a plan execution follows a given STRIPSaction model, even if this model is not fully specified.\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "600f23dde8384f14857215ea5140ae2d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating outputs:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True Title: A Survey of Multi-Objective Sequential Decision-Making\n",
      "\n",
      "Predicted Title: A Multi-Objective Methods for Planning and Learning\n",
      "\n",
      "Abstract:   Sequential decision-making problems with multiple objectives arise naturallyin practice and pose unique challenges for research in decision-theoreticplanning and learning, which has largely focused on single-objective settings.This article surveys algorithms designed for sequential decision-makingproblems with multiple objectives. Though there is a growing body of literatureon this subject, little of it makes explicit under what circumstances specialmethods are needed to solve multi-objective problems. Therefore, we identifythree distinct scenarios in which converting such a problem to asingle-objective one is impossible, infeasible, or undesirable. Furthermore, wepropose a taxonomy that classifies multi-objective methods according to theapplicable scenario, the nature of the scalarization function (which projectsmulti-objective values to scalar ones), and the type of policies considered. Weshow how these factors determine the nature of an optimal solution, which canbe a single policy, a convex hull, or a Pareto front. Using this taxonomy, wesurvey the literature on multi-objective methods for planning and learning.Finally, we discuss key applications of such methods and outline opportunitiesfor future work.\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e11bfa718108407aa25fdc3d4b943f40",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating outputs:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True Title: A note on the complexity of the causal ordering problem\n",
      "\n",
      "Predicted Title: A causal ordering: A concise report on the complexity of the causalordering problem\n",
      "\n",
      "Abstract:   In this note we provide a concise report on the complexity of the causalordering problem, originally introduced by Simon to reason about causaldependencies implicit in systems of mathematical equations. We show thatSimon's classical algorithm to infer causal ordering is NP-Hard---anintractability previously guessed but never proven. We present then a detailedaccount based on Nayak's suggested algorithmic solution (the best available),which is dominated by computing transitive closure---bounded in time by$O(|\\mathcal V|\\cdot |\\mathcal S|)$, where $\\mathcal S(\\mathcal E, \\mathcal V)$is the input system structure composed of a set $\\mathcal E$ of equations overa set $\\mathcal V$ of variables with number of variable appearances (density)$|\\mathcal S|$. We also comment on the potential of causal ordering foremerging applications in large-scale hypothesis management and analytics.\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8fd711d90dda4a57a46a47673be4e755",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating outputs:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True Title: The Multi-Agent Reinforcement Learning in Malm\\\"O (MARL\\\"O) Competition\n",
      "\n",
      "Predicted Title: A Multi-Agent Reinforcement Learning in Malm\\\"O\n",
      "\n",
      "Abstract:   Learning in multi-agent scenarios is a fruitful research direction, butcurrent approaches still show scalability problems in multiple games withgeneral reward settings and different opponent types. The Multi-AgentReinforcement Learning in Malm\\\"O (MARL\\\"O) competition is a new challenge thatproposes research in this domain using multiple 3D games. The goal of thiscontest is to foster research in general agents that can learn across differentgames and opponent types, proposing a challenge as a milestone in the directionof Artificial General Intelligence.\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f5858cf4c67e41438a71563f30766239",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating outputs:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True Title: The emergence of Explainability of Intelligent Systems: Delivering\n",
      "  Explainable and Personalised Recommendations for Energy Efficiency\n",
      "\n",
      "Predicted Title: Aation Systems for Energy Efficiency\n",
      "\n",
      "Abstract:   The recent advances in artificial intelligence namely in machine learning anddeep learning, have boosted the performance of intelligent systems in severalways. This gave rise to human expectations, but also created the need for adeeper understanding of how intelligent systems think and decide. The conceptof explainability appeared, in the extent of explaining the internal systemmechanics in human terms. Recommendation systems are intelligent systems thatsupport human decision making, and as such, they have to be explainable inorder to increase user trust and improve the acceptance of recommendations. Inthis work, we focus on a context-aware recommendation system for energyefficiency and develop a mechanism for explainable and persuasiverecommendations, which are personalized to user preferences and habits. Thepersuasive facts either emphasize on the economical saving prospects (Econ) oron a positive ecological impact (Eco) and explanations provide the reason forrecommending an energy saving action. Based on a study conducted using aTelegram bot, different scenarios have been validated with actual data andhuman feedback. Current results show a total increase of 19\\% on therecommendation acceptance ratio when both economical and ecological persuasivefacts are employed. This revolutionary approach on recommendation systems,demonstrates how intelligent recommendations can effectively encourage energysaving behavior.\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9e4b7c8adac14b60ba9f797a63460515",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating outputs:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True Title: Shiva++: An Enhanced Graph based Ontology Matcher\n",
      "\n",
      "Predicted Title: Anology based systems for knowledge management\n",
      "\n",
      "Abstract:   With the web getting bigger and assimilating knowledge about differentconcepts and domains, it is becoming very difficult for simple database drivenapplications to capture the data for a domain. Thus developers have come outwith ontology based systems which can store large amount of information and canapply reasoning and produce timely information. Thus facilitating effectiveknowledge management. Though this approach has made our lives easier, but atthe same time has given rise to another problem. Two different ontologiesassimilating same knowledge tend to use different terms for the same concepts.This creates confusion among knowledge engineers and workers, as they do notknow which is a better term then the other. Thus we need to merge ontologiesworking on same domain so that the engineers can develop a better applicationover it. This paper shows the development of one such matcher which merges theconcepts available in two ontologies at two levels; 1) at string level and 2)at semantic level; thus producing better merged ontologies. We have used agraph matching technique which works at the core of the system. We have alsoevaluated the system and have tested its performance with its predecessor whichworks only on string matching. Thus current approach produces better results.\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1cd9aed9e06e4f9c8474187f15598b94",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating outputs:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True Title: A Fuzzy AHP Approach for Supplier Selection Problem: A Case Study in a\n",
      "  Gear Motor Company\n",
      "\n",
      "Predicted Title: A Fuzzy AHP for Suuplier Selection\n",
      "\n",
      "Abstract:   Suuplier selection is one of the most important functions of a purchasingdepartment. Since by deciding the best supplier, companies can save materialcosts and increase competitive advantage.However this decision becomescompilcated in case of multiple suppliers, multiple conflicting criteria, andimprecise parameters. In addition the uncertainty and vagueness of the experts'opinion is the prominent characteristic of the problem. therefore anextensively used multi criteria decision making tool Fuzzy AHP can be utilizedas an approach for supplier selection problem. This paper reveals theapplication of Fuzzy AHP in a gear motor company determining the best supplierwith respect to selected criteria. the contribution of this study is not onlythe application of the Fuzzy AHP methodology for supplier selection problem,but also releasing a comprehensive literature review of multi criteria decisionmaking problems. In addition by stating the steps of Fuzzy AHP clearly andnumerically, this study can be a guide of the methodology to be implemented toother multiple criteria decision making problems.\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "27a10bf8ead843408b7c5d8383b1bfb2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating outputs:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True Title: Learning STRIPS Action Models with Classical Planning\n",
      "\n",
      "Predicted Title: Acompiling STRIPS action models from a classical planning task\n",
      "\n",
      "Abstract:   This paper presents a novel approach for learning STRIPS action models fromexamples that compiles this inductive learning task into a classical planningtask. Interestingly, the compilation approach is flexible to different amountsof available input knowledge; the learning examples can range from a set ofplans (with their corresponding initial and final states) to just a pair ofinitial and final states (no intermediate action or state is given). Moreover,the compilation accepts partially specified action models and it can be used tovalidate whether the observation of a plan execution follows a given STRIPSaction model, even if this model is not fully specified.\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "937ff8deb29a4a179c013c8a3d85fa1e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating outputs:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True Title: A Study of Student Learning Skills Using Fuzzy Relation Equations\n",
      "\n",
      "Predicted Title: A Fuzzy relation equations (FRE)\n",
      "\n",
      "Abstract:   Fuzzy relation equations (FRE)are associated with the composition of binaryfuzzy relations. In the present work FRE are used as a tool for studying theprocess of learning a new subject matter by a student class. A classroomapplication and other csuitable examples connected to the student learning ofthe derivative are also presented illustrating our results and usefulconclusions are obtained.\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5d3b31193cec4033ae6dcb8cea3c554d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating outputs:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True Title: The SP theory of intelligence and the representation and processing of\n",
      "  knowledge in the brain\n",
      "\n",
      "Predicted Title: A SP theory of intelligence: An overview and partial model\n",
      "\n",
      "Abstract:   The \"SP theory of intelligence\", with its realisation in the \"SP computermodel\", aims to simplify and integrate observations and concepts acrossAI-related fields, with information compression as a unifying theme. This paperdescribes how abstract structures and processes in the theory may be realisedin terms of neurons, their interconnections, and the transmission of signalsbetween neurons. This part of the SP theory -- \"SP-neural\" -- is a tentativeand partial model for the representation and processing of knowledge in thebrain. In the SP theory (apart from SP-neural), all kinds of knowledge arerepresented with \"patterns\", where a pattern is an array of atomic symbols inone or two dimensions. In SP-neural, the concept of a \"pattern\" is realised asan array of neurons called a \"pattern assembly\", similar to Hebb's concept of a\"cell assembly\" but with important differences. Central to the processing ofinformation in the SP system is the powerful concept of \"multiple alignment\",borrowed and adapted from bioinformatics. Processes such as patternrecognition, reasoning and problem solving are achieved via the building ofmultiple alignments, while unsupervised learning -- significantly differentfrom the \"Hebbian\" kinds of learning -- is achieved by creating patterns fromsensory information and also by creating patterns from multiple alignments inwhich there is a partial match between one pattern and another. Short-livedneural structures equivalent to multiple alignments will be created via aninter-play of excitatory and inhibitory neural signals. The paper discussesseveral associated issues, with relevant empirical evidence.\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c476a56a828c4fb091277b92cb9685c6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating outputs:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True Title: Adding Context to Concept Trees\n",
      "\n",
      "Predicted Title: A Concept Trees: A Concept Base\n",
      "\n",
      "Abstract:   A Concept Tree is a structure for storing knowledge where the trees arestored in a database called a Concept Base. It sits between the highlydistributed neural architectures and the distributed information systems, withthe intention of bringing brain-like and computer systems closer together.Concept Trees can grow from the semi-structured sources when consistentsequences of concepts are presented. Each tree ideally represents a singlecohesive concept and the trees can link with each other for navigation andsemantic purposes. The trees are therefore also a type of semantic network andwould benefit from having a consistent level of context for each node. Aconsistent build process is managed through a 'counting rule' and some otherrules that can normalise the database structure. This restricted structure canthen be complimented and enriched by the more dynamic context. It is alsosuggested to use the linking structure of the licas system [15] as a basis forthe context links, where the mathematical model is extended further to definethis. A number of tests have demonstrated the soundness of the architecture.Building the trees from text documents shows that the tree structure could beinherent in natural language. Then, two types of query language are described.Both of these can perform consistent query processes to return knowledge to theuser and even enhance the query with new knowledge. This is supported evenfurther with direct comparisons to a cognitive model, also being developed bythe author.\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "07772e655c6a41979a992acac5bbaf17",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating outputs:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True Title: Modeling Variations of First-Order Horn Abduction in Answer Set\n",
      "  Programming\n",
      "\n",
      "Predicted Title: A Answer Set Programming: An Approach to Abduction in First Order Horn Logic\n",
      "\n",
      "Abstract:   We study abduction in First Order Horn logic theories where all atoms can beabduced and we are looking for preferred solutions with respect to threeobjective functions: cardinality minimality, coherence, and weighted abduction.We represent this reasoning problem in Answer Set Programming (ASP), in orderto obtain a flexible framework for experimenting with global constraints andobjective functions, and to test the boundaries of what is possible with ASP.Realizing this problem in ASP is challenging as it requires value invention andequivalence between certain constants, because the Unique Names Assumption doesnot hold in general. To permit reasoning in cyclic theories, we formallydescribe fine-grained variations of limiting Skolemization. We identify termequivalence as a main instantiation bottleneck, and improve the efficiency ofour approach with on-demand constraints that were used to eliminate the samebottleneck in state-of-the-art solvers. We evaluate our approach experimentallyon the ACCEL benchmark for plan recognition in Natural Language Understanding.Our encodings are publicly available, modular, and our approach is moreefficient than state-of-the-art solvers on the ACCEL benchmark.\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3357e48c2a0e4f798ea0aafbb21e298b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating outputs:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True Title: Learning STRIPS Action Models with Classical Planning\n",
      "\n",
      "Predicted Title: Acompiling STRIPS action models from a classical planning task\n",
      "\n",
      "Abstract:   This paper presents a novel approach for learning STRIPS action models fromexamples that compiles this inductive learning task into a classical planningtask. Interestingly, the compilation approach is flexible to different amountsof available input knowledge; the learning examples can range from a set ofplans (with their corresponding initial and final states) to just a pair ofinitial and final states (no intermediate action or state is given). Moreover,the compilation accepts partially specified action models and it can be used tovalidate whether the observation of a plan execution follows a given STRIPSaction model, even if this model is not fully specified.\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c24b14efb2a54846a645a14accbdf971",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating outputs:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True Title: Evaluating Go Game Records for Prediction of Player Attributes\n",
      "\n",
      "Predicted Title: A Evaluation of Go Game Data\n",
      "\n",
      "Abstract:   We propose a way of extracting and aggregating per-move evaluations from setsof Go game records. The evaluations capture different aspects of the games suchas played patterns or statistic of sente/gote sequences. Using machine learningalgorithms, the evaluations can be utilized to predict different relevanttarget variables. We apply this methodology to predict the strength and playingstyle of the player (e.g. territoriality or aggressivity) with good accuracy.We propose a number of possible applications including aiding in Go study,seeding real-work ranks of internet players or tuning of Go-playing programs.\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4e413df45d6646969102869d1f0158a9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating outputs:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True Title: A Survey of Multi-Objective Sequential Decision-Making\n",
      "\n",
      "Predicted Title: A Multi-Objective Methods for Planning and Learning\n",
      "\n",
      "Abstract:   Sequential decision-making problems with multiple objectives arise naturallyin practice and pose unique challenges for research in decision-theoreticplanning and learning, which has largely focused on single-objective settings.This article surveys algorithms designed for sequential decision-makingproblems with multiple objectives. Though there is a growing body of literatureon this subject, little of it makes explicit under what circumstances specialmethods are needed to solve multi-objective problems. Therefore, we identifythree distinct scenarios in which converting such a problem to asingle-objective one is impossible, infeasible, or undesirable. Furthermore, wepropose a taxonomy that classifies multi-objective methods according to theapplicable scenario, the nature of the scalarization function (which projectsmulti-objective values to scalar ones), and the type of policies considered. Weshow how these factors determine the nature of an optimal solution, which canbe a single policy, a convex hull, or a Pareto front. Using this taxonomy, wesurvey the literature on multi-objective methods for planning and learning.Finally, we discuss key applications of such methods and outline opportunitiesfor future work.\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6051f384456244dabf2d8db90bfb6062",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating outputs:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True Title: Online Guest Detection in a Smart Home using Pervasive Sensors and\n",
      "  Probabilistic Reasoning\n",
      "\n",
      "Predicted Title: Aing the Number of People in a Smart Home at Each Time Step\n",
      "\n",
      "Abstract:   Smart home environments equipped with distributed sensor networks are capableof helping people by providing services related to health, emergency detectionor daily routine management. A backbone to these systems relies often on thesystem's ability to track and detect activities performed by the users in theirhome. Despite the continuous progress in the area of activity recognition insmart homes, many systems make a strong underlying assumption that the numberof occupants in the home at any given moment of time is always known.Estimating the number of persons in a Smart Home at each time step remains achallenge nowadays. Indeed, unlike most (crowd) counting solution which arebased on computer vision techniques, the sensors considered in a Smart Home areoften very simple and do not offer individually a good overview of thesituation. The data gathered needs therefore to be fused in order to inferuseful information. This paper aims at addressing this challenge and presents aprobabilistic approach able to estimate the number of persons in theenvironment at each time step. This approach works in two steps: first, anestimate of the number of persons present in the environment is done using aConstraint Satisfaction Problem solver, based on the topology of the sensornetwork and the sensor activation pattern at this time point. Then, a HiddenMarkov Model refines this estimate by considering the uncertainty related tothe sensors. Using both simulated and real data, our method has been tested andvalidated on two smart homes of different sizes and configuration anddemonstrates the ability to accurately estimate the number of inhabitants.\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5d26ce05d72e4ac99751a0b120729a8d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating outputs:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True Title: A Directional Feature with Energy based Offline Signature Verification\n",
      "  Network\n",
      "\n",
      "Predicted Title: Aofflinesignature verification system\n",
      "\n",
      "Abstract:   Signature used as a biometric is implemented in various systems as well asevery signature signed by each person is distinct at the same time. So, it isvery important to have a computerized signature verification system. In offlinesignature verification system dynamic features are not available obviously, butone can use a signature as an image and apply image processing techniques tomake an effective offline signature verification system. Author proposes aintelligent network used directional feature and energy density both as inputsto the same network and classifies the signature. Neural network is used as aclassifier for this system. The results are compared with both the very basicenergy density method and a simple directional feature method of offlinesignature verification system and this proposed new network is found veryeffective as compared to the above two methods, specially for less number oftraining samples, which can be implemented practically.\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "040e20b49fe44ca69cd4b525c2f4c2e6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating outputs:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True Title: Adding Context to Concept Trees\n",
      "\n",
      "Predicted Title: A Concept Trees: A Concept Base\n",
      "\n",
      "Abstract:   A Concept Tree is a structure for storing knowledge where the trees arestored in a database called a Concept Base. It sits between the highlydistributed neural architectures and the distributed information systems, withthe intention of bringing brain-like and computer systems closer together.Concept Trees can grow from the semi-structured sources when consistentsequences of concepts are presented. Each tree ideally represents a singlecohesive concept and the trees can link with each other for navigation andsemantic purposes. The trees are therefore also a type of semantic network andwould benefit from having a consistent level of context for each node. Aconsistent build process is managed through a 'counting rule' and some otherrules that can normalise the database structure. This restricted structure canthen be complimented and enriched by the more dynamic context. It is alsosuggested to use the linking structure of the licas system [15] as a basis forthe context links, where the mathematical model is extended further to definethis. A number of tests have demonstrated the soundness of the architecture.Building the trees from text documents shows that the tree structure could beinherent in natural language. Then, two types of query language are described.Both of these can perform consistent query processes to return knowledge to theuser and even enhance the query with new knowledge. This is supported evenfurther with direct comparisons to a cognitive model, also being developed bythe author.\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6f9067faa7bb4f63a3d4c95e977eb63c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating outputs:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True Title: Deep Learning Techniques for Geospatial Data Analysis\n",
      "\n",
      "Predicted Title: A Deep Learning for Remote Sensing Data Analysis\n",
      "\n",
      "Abstract:   Consumer electronic devices such as mobile handsets, goods tagged with RFIDlabels, location and position sensors are continuously generating a vast amountof location enriched data called geospatial data. Conventionally suchgeospatial data is used for military applications. In recent times, many usefulcivilian applications have been designed and deployed around such geospatialdata. For example, a recommendation system to suggest restaurants or places ofattraction to a tourist visiting a particular locality. At the same time, civicbodies are harnessing geospatial data generated through remote sensing devicesto provide better services to citizens such as traffic monitoring, potholeidentification, and weather reporting. Typically such applications areleveraged upon non-hierarchical machine learning techniques such as Naive-BayesClassifiers, Support Vector Machines, and decision trees. Recent advances inthe field of deep-learning showed that Neural Network-based techniquesoutperform conventional techniques and provide effective solutions for manygeospatial data analysis tasks such as object recognition, imageclassification, and scene understanding. The chapter presents a survey on thecurrent state of the applications of deep learning techniques for analyzinggeospatial data.  The chapter is organized as below: (i) A brief overview of deep learningalgorithms. (ii)Geospatial Analysis: a Data Science Perspective (iii)Deep-learning techniques for Remote Sensing data analytics tasks (iv)Deep-learning techniques for GPS data analytics(iv) Deep-learning techniquesfor RFID data analytics.\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "02319d5b25c14181a058e95754dfaaec",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating outputs:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True Title: A Comparative Study on Parameter Estimation in Software Reliability\n",
      "  Modeling using Swarm Intelligence\n",
      "\n",
      "Predicted Title: Anfficient Swarm Optimization and Firefly Algorithm\n",
      "\n",
      "Abstract:   This work focuses on a comparison between the performances of two well-knownSwarm algorithms: Cuckoo Search (CS) and Firefly Algorithm (FA), in estimatingthe parameters of Software Reliability Growth Models. This study is furtherreinforced using Particle Swarm Optimization (PSO) and Ant Colony Optimization(ACO). All algorithms are evaluated according to real software failure data,the tests are performed and the obtained results are compared to show theperformance of each of the used algorithms. Furthermore, CS and FA are alsocompared with each other on bases of execution time and iteration number.Experimental results show that CS is more efficient in estimating theparameters of SRGMs, and it has outperformed FA in addition to PSO and ACO forthe selected Data sets and employed models.\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f378078867a34800952e23017def31c9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating outputs:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True Title: Autonomous Self-Explanation of Behavior for Interactive Reinforcement\n",
      "  Learning Agents\n",
      "\n",
      "Predicted Title: A Instruction-Based Behavior Explanation\n",
      "\n",
      "Abstract:   In cooperation, the workers must know how co-workers behave. However, anagent's policy, which is embedded in a statistical machine learning model, ishard to understand, and requires much time and knowledge to comprehend.Therefore, it is difficult for people to predict the behavior of machinelearning robots, which makes Human Robot Cooperation challenging. In thispaper, we propose Instruction-based Behavior Explanation (IBE), a method toexplain an autonomous agent's future behavior. In IBE, an agent canautonomously acquire the expressions to explain its own behavior by reusing theinstructions given by a human expert to accelerate the learning of the agent'spolicy. IBE also enables a developmental agent, whose policy may change duringthe cooperation, to explain its own behavior with sufficient time granularity.\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "480077f4f90f45e38c1eec40f33c7306",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating outputs:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True Title: Autonomous Self-Explanation of Behavior for Interactive Reinforcement\n",
      "  Learning Agents\n",
      "\n",
      "Predicted Title: A Instruction-Based Behavior Explanation\n",
      "\n",
      "Abstract:   In cooperation, the workers must know how co-workers behave. However, anagent's policy, which is embedded in a statistical machine learning model, ishard to understand, and requires much time and knowledge to comprehend.Therefore, it is difficult for people to predict the behavior of machinelearning robots, which makes Human Robot Cooperation challenging. In thispaper, we propose Instruction-based Behavior Explanation (IBE), a method toexplain an autonomous agent's future behavior. In IBE, an agent canautonomously acquire the expressions to explain its own behavior by reusing theinstructions given by a human expert to accelerate the learning of the agent'spolicy. IBE also enables a developmental agent, whose policy may change duringthe cooperation, to explain its own behavior with sufficient time granularity.\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c1a9122cbe4f4352b733bc35f5991d41",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating outputs:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True Title: A Survey of Multi-Objective Sequential Decision-Making\n",
      "\n",
      "Predicted Title: A Multi-Objective Methods for Planning and Learning\n",
      "\n",
      "Abstract:   Sequential decision-making problems with multiple objectives arise naturallyin practice and pose unique challenges for research in decision-theoreticplanning and learning, which has largely focused on single-objective settings.This article surveys algorithms designed for sequential decision-makingproblems with multiple objectives. Though there is a growing body of literatureon this subject, little of it makes explicit under what circumstances specialmethods are needed to solve multi-objective problems. Therefore, we identifythree distinct scenarios in which converting such a problem to asingle-objective one is impossible, infeasible, or undesirable. Furthermore, wepropose a taxonomy that classifies multi-objective methods according to theapplicable scenario, the nature of the scalarization function (which projectsmulti-objective values to scalar ones), and the type of policies considered. Weshow how these factors determine the nature of an optimal solution, which canbe a single policy, a convex hull, or a Pareto front. Using this taxonomy, wesurvey the literature on multi-objective methods for planning and learning.Finally, we discuss key applications of such methods and outline opportunitiesfor future work.\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for _ in range(250):\n",
    "\n",
    "    random_idx = random.randint(0, len(eval_df)-1)\n",
    "\n",
    "    abstract = eval_df.iloc[random_idx]['input_text']\n",
    "    true_title = eval_df.iloc[random_idx]['target_text']\n",
    "\n",
    "    # Predict with trained BART model\n",
    "    predicted_title = model.predict([abstract])[0]\n",
    "\n",
    "    print(f'True Title: {true_title}\\n')\n",
    "    print(f'Predicted Title: {predicted_title}\\n')\n",
    "    print(f'Abstract: {abstract}\\n\\n\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f0cd40f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
